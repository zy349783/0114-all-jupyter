{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    url = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    client = pymongo.MongoClient(url, maxPoolSize=None)\n",
    "    db = client[db_name]\n",
    "    return db\n",
    "\n",
    "def build_query(start_date=None, end_date=None, index_id=None):\n",
    "    query = {}\n",
    "\n",
    "    def parse_date(x):\n",
    "        if type(x) == int:\n",
    "            return x\n",
    "        elif type(x) == str:\n",
    "            if len(x) != 8:\n",
    "                raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "            return int(x)\n",
    "        elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "            return x.strftime(\"%Y%m%d\").astype(int)\n",
    "        else:\n",
    "            raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "    if start_date is not None or end_date is not None:\n",
    "        query['date'] = {}\n",
    "        if start_date is not None:\n",
    "            query['date']['$gte'] = parse_date(start_date)\n",
    "        if end_date is not None:\n",
    "            query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "    def parse_symbol(x):\n",
    "        if type(x) == int:\n",
    "            return x\n",
    "        else:\n",
    "            return int(x)\n",
    "\n",
    "    if index_id:\n",
    "        if type(index_id) == list or type(index_id) == tuple:\n",
    "            query['index_id'] = {'$in': [parse_symbol(x) for x in index_id]}\n",
    "        else:\n",
    "            query['index_id'] = parse_symbol(index_id)\n",
    "    \n",
    "    return query\n",
    "\n",
    "# def write_data(db, name, df):\n",
    "#     collection = db[name]\n",
    "#     # drop all records with same index_id and same time\n",
    "#     for symbol in df['index_id'].unique():\n",
    "#         if symbol in collection.distinct('index_id'):\n",
    "#             start_date = df[df['index_id'] == symbol]['date'].min()\n",
    "#             end_date = df[df['index_id'] == symbol]['date'].max()\n",
    "#             query = build_query(start_date, end_date, index_id=symbol)\n",
    "#             collection.delete_many(query)\n",
    "#     df = df.to_dict('records')\n",
    "#     collection.insert_many(df) \n",
    "\n",
    "def write_data(db, name, df):\n",
    "    collection = db[name]\n",
    "    # drop all records with same index_id and same time\n",
    "    for symbol in df['index_id'].unique():\n",
    "        if symbol in collection.distinct('index_id'):\n",
    "            m_ax = pd.DataFrame.from_records(collection.find({'index_id':{'$in':[symbol]}}).sort([('date',-1)]).skip(0).limit(1))['max_value'].values[0]\n",
    "#             m_ax = pd.DataFrame.from_records(collection.aggregate([{\"$group\":{'_id': 'max','max_value':{\"$max\":\"$date\"}}}]))['max_value'].values[0]\n",
    "            df = df[(df['index_id'] == symbol) & (df['date'] > m_ax)]\n",
    "    df = df.to_dict('records')\n",
    "    collection.insert_many(df) \n",
    "    \n",
    "def write_weight_data(db, name, df, index_id):\n",
    "    collection = db[name]\n",
    "    df = df[df['index_id'] == index_id]\n",
    "    for (date, skey), sub_df in df.groupby(['date', 'skey']):\n",
    "        date = int(date)\n",
    "        skey = int(skey)\n",
    "        weight = sub_df['weight'].values[0]\n",
    "        print(weight)\n",
    "        collection.update({'skey': skey, 'date':date, 'index_id':index_id}, {'$set':{'weight':float(weight)}})\n",
    "\n",
    "def delete_data(db, name, start_date=None, end_date=None, index_id=None):\n",
    "    collection = db[name]\n",
    "    query = build_query(start_date, end_date, index_id)\n",
    "    if not query:\n",
    "        print('cannot delete the whole table')\n",
    "        return None\n",
    "    collection.delete_many(query)    \n",
    " \n",
    "def read_daily(db, name, start_date=None, end_date=None, skey=None, index_id=None, interval=None, col=None, return_sdi=True):\n",
    "    collection = db[name]\n",
    "    # Build projection\n",
    "    prj = {'_id': 0}\n",
    "    if col is not None:\n",
    "        if return_sdi:\n",
    "            col = ['skey', 'date', 'interval'] + col\n",
    "        for col_name in col:\n",
    "            prj[col_name] = 1\n",
    "\n",
    "    # Build query\n",
    "    query = {}\n",
    "    if skey is not None:\n",
    "        query['skey'] = {'$in': skey}\n",
    "    if index_id is not None:\n",
    "        query['index_id'] = {'$in': index_id}\n",
    "    if interval is not None:\n",
    "        query['interval'] = {'$in': interval}\n",
    "    if start_date is not None:\n",
    "        if end_date is not None:\n",
    "            query['date'] = {'$gte': start_date, '$lte': end_date}\n",
    "        else:\n",
    "            query['date'] = {'$gte': start_date}\n",
    "    elif end_date is not None:\n",
    "        query['date'] = {'$lte': end_date}\n",
    "\n",
    "    # Load data\n",
    "    cur = collection.find(query, prj)\n",
    "    df = pd.DataFrame.from_records(cur)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        df = df.sort_values(by=['date', 'index_id', 'skey'])\n",
    "    return df    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "\n",
    "pd.set_option('max_columns', 200)\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "# 1. read data\n",
    "kk = read_daily(db1, 'index_memb', 20200813, 20200813)\n",
    "# kk = read_daily(db1, 'index_memb', 20170502, 20170503, index_id=[1000300])\n",
    "\n",
    "# 2. uploda data\n",
    "# write_data(db1, 'index_memb', IF[IF['date'].isin([20170502])])\n",
    "\n",
    "# 3. delete data\n",
    "# delete_data(db1, 'index_memb', start_date=20170502, end_date=20170502)\n",
    "\n",
    "# 4. update weight column\n",
    "# write_weight_data(db1, 'index_memb', test, 1000300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_data(db1, 'index_memb', start_date=20170502, end_date=20200827, index_id=[1000300, 1000905])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IF = pd.read_pickle(r'A:\\index\\weight_table_IF.pkl')\n",
    "IC = pd.read_pickle(r'A:\\index\\weight_table_IC.pkl')\n",
    "IF['date'] = IF['date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "IF['ID'] = np.where(IF['ID'].str[:2] =='SZ', IF['ID'].str[2:].astype(int) + 2000000, IF['ID'].str[2:].astype(int) + 1000000)\n",
    "IC['date'] = IC['date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "IC['ID'] = np.where(IC['ID'].str[:2] =='SZ', IC['ID'].str[2:].astype(int) + 2000000, IC['ID'].str[2:].astype(int) + 1000000)\n",
    "IC = IC.rename(columns={'ID':'skey'})\n",
    "IF = IF.rename(columns={'ID':'skey'})\n",
    "IC['index_id'] = 1000905\n",
    "IF['index_id'] = 1000300\n",
    "IC['index_name'] = 'IC'\n",
    "IF['index_name'] = 'IF'\n",
    "IC = IC.sort_values(by=['date', 'skey'])\n",
    "IF = IF.sort_values(by=['date', 'skey'])\n",
    "write_data(db1, 'index_memb', IC)\n",
    "write_data(db1, 'index_memb', IF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 AMAC CSIH30053\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intdate</th>\n",
       "      <th>minute</th>\n",
       "      <th>morning</th>\n",
       "      <th>time</th>\n",
       "      <th>close</th>\n",
       "      <th>industry_open</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "      <th>ID</th>\n",
       "      <th>ordering</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>43833</td>\n",
       "      <td>541</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-03 09:00:01</td>\n",
       "      <td>2079.6519</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>0</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>43833</td>\n",
       "      <td>566</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-03 09:25:06</td>\n",
       "      <td>2081.2721</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>1.555597e+07</td>\n",
       "      <td>1.779575e+04</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>1</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>43833</td>\n",
       "      <td>566</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-03 09:25:11</td>\n",
       "      <td>2082.8919</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>2.086762e+07</td>\n",
       "      <td>2.445689e+04</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>2</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>43833</td>\n",
       "      <td>566</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-03 09:25:21</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>2.114212e+07</td>\n",
       "      <td>2.464953e+04</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>3</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>43833</td>\n",
       "      <td>571</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-03 09:30:01</td>\n",
       "      <td>2082.8321</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>2.155072e+07</td>\n",
       "      <td>2.497494e+04</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>4</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2851</td>\n",
       "      <td>43833</td>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-03 14:57:05</td>\n",
       "      <td>2090.5493</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>3.122167e+09</td>\n",
       "      <td>3.088311e+06</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>2851</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2852</td>\n",
       "      <td>43833</td>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-03 14:57:10</td>\n",
       "      <td>2090.6369</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>3.122317e+09</td>\n",
       "      <td>3.088402e+06</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>2852</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2853</td>\n",
       "      <td>43833</td>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-03 15:00:05</td>\n",
       "      <td>2090.7631</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>3.148050e+09</td>\n",
       "      <td>3.113167e+06</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>2853</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2854</td>\n",
       "      <td>43833</td>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-03 15:00:10</td>\n",
       "      <td>2091.4399</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>3.165474e+09</td>\n",
       "      <td>3.126518e+06</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>2854</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2855</td>\n",
       "      <td>43833</td>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-03 15:00:25</td>\n",
       "      <td>2091.4399</td>\n",
       "      <td>2082.8988</td>\n",
       "      <td>3.166596e+09</td>\n",
       "      <td>3.127462e+06</td>\n",
       "      <td>CSIH30053</td>\n",
       "      <td>2855</td>\n",
       "      <td>202001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2856 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      intdate  minute  morning                time      close  industry_open  \\\n",
       "0       43833     541        1 2020-01-03 09:00:01  2079.6519      2082.8988   \n",
       "1       43833     566        1 2020-01-03 09:25:06  2081.2721      2082.8988   \n",
       "2       43833     566        1 2020-01-03 09:25:11  2082.8919      2082.8988   \n",
       "3       43833     566        1 2020-01-03 09:25:21  2082.8988      2082.8988   \n",
       "4       43833     571        1 2020-01-03 09:30:01  2082.8321      2082.8988   \n",
       "...       ...     ...      ...                 ...        ...            ...   \n",
       "2851    43833     898        0 2020-01-03 14:57:05  2090.5493      2082.8988   \n",
       "2852    43833     898        0 2020-01-03 14:57:10  2090.6369      2082.8988   \n",
       "2853    43833     901        0 2020-01-03 15:00:05  2090.7631      2082.8988   \n",
       "2854    43833     901        0 2020-01-03 15:00:10  2091.4399      2082.8988   \n",
       "2855    43833     901        0 2020-01-03 15:00:25  2091.4399      2082.8988   \n",
       "\n",
       "        cum_volume    cum_amount         ID  ordering   month  \n",
       "0     0.000000e+00  0.000000e+00  CSIH30053         0  202001  \n",
       "1     1.555597e+07  1.779575e+04  CSIH30053         1  202001  \n",
       "2     2.086762e+07  2.445689e+04  CSIH30053         2  202001  \n",
       "3     2.114212e+07  2.464953e+04  CSIH30053         3  202001  \n",
       "4     2.155072e+07  2.497494e+04  CSIH30053         4  202001  \n",
       "...            ...           ...        ...       ...     ...  \n",
       "2851  3.122167e+09  3.088311e+06  CSIH30053      2851  202001  \n",
       "2852  3.122317e+09  3.088402e+06  CSIH30053      2852  202001  \n",
       "2853  3.148050e+09  3.113167e+06  CSIH30053      2853  202001  \n",
       "2854  3.165474e+09  3.126518e+06  CSIH30053      2854  202001  \n",
       "2855  3.166596e+09  3.127462e+06  CSIH30053      2855  202001  \n",
       "\n",
       "[2856 rows x 11 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AMAC tick data (update weekly in the future)：\n",
    "import TSLPy3\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def updateAMAC(updateList, startDate, endDate):\n",
    "    for num in range(len(updateList)):\n",
    "       stock=updateList[num]\n",
    "       tickname = 'Tick_'+ stock\n",
    "       if num%10 == 0: print('Processing ' + str(num)+' AMAC '+stock)\n",
    "       tsstr=\"\"\"\n",
    "               BegT :=%s;\n",
    "               EndT :=%s + 0.99;\n",
    "               setSysParam(pn_stock(),'%s');\n",
    "               returnData := select ['date'],['close'],['sectional_open'],['sectional_vol'],['sectional_amount']\n",
    "                             from tradetable datekey BegT to EndT of DefaultStockID() end;\n",
    "               return returnData;\n",
    "               \"\"\"%(startDate,endDate,stock)\n",
    "       Tick_Stock = pd.DataFrame(TSLPy3.RemoteExecute(tsstr,{})[1])\n",
    "       Tick_Stock.columns = list(pd.Series(Tick_Stock.columns).str.decode('GBK'))\n",
    "       Tick_Stock['intdate'] = Tick_Stock.date.astype(int)\n",
    "       Tick_Stock['time'] = Tick_Stock.date.map(lambda x: datetime.datetime.utcfromtimestamp(round((x - 25569) * 86400.0)))\n",
    "       Tick_Stock['adjTime'] = Tick_Stock.date.map(lambda x: datetime.datetime.utcfromtimestamp(round((x - 25569) * 86400.0) - 1))\n",
    "       Tick_Stock['minute'] = Tick_Stock.adjTime.map(lambda x: (x.hour*60 + x.minute + 1))\n",
    "       assert (Tick_Stock.minute.max() >= 900) & (Tick_Stock.minute.min() <= 570)\n",
    "       Tick_Stock['morning'] = np.where(Tick_Stock.minute <= 690, 1, 0)          \n",
    "       Tick_Stock.rename(columns = {'sectional_open':'industry_open','sectional_vol':'cum_volume','sectional_amount':'cum_amount'}, inplace=True)            \n",
    "       Tick_Stock = Tick_Stock[['intdate','minute','morning','time','close','industry_open','cum_volume','cum_amount']].reset_index(drop = True)\n",
    "       Tick_Stock['ID'] = stock\n",
    "       ## ordering per day per stock\n",
    "       for intD in Tick_Stock.intdate.unique():\n",
    "           Tick_Stock.loc[Tick_Stock.intdate == intD, 'ordering'] = range(0, len(Tick_Stock.loc[Tick_Stock.intdate == intD, 'ID']))\n",
    "       Tick_Stock['month'] = Tick_Stock.time.dt.month + Tick_Stock.time.dt.year * 100\n",
    "       return Tick_Stock\n",
    "#        assert (datetime.datetime(1899,12,30) + datetime.timedelta(int(Tick_Stock.intdate.max()))).strftime('%Y%m%d') == UPDATEDATE\n",
    "data = updateAMAC(['CSIH30053'], '20200103T', '20200103T')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>weight</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>6.608433</td>\n",
       "      <td>SH601318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2.972627</td>\n",
       "      <td>SH600519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2.567800</td>\n",
       "      <td>SH600036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2.152648</td>\n",
       "      <td>SZ000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>1.825902</td>\n",
       "      <td>SZ000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4195</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>0.032009</td>\n",
       "      <td>SH600390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4196</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>0.031682</td>\n",
       "      <td>SH603858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4197</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>0.023580</td>\n",
       "      <td>SZ002839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4198</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>SZ002841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4199</td>\n",
       "      <td>2018-01-19</td>\n",
       "      <td>0.020838</td>\n",
       "      <td>SZ002831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date    weight        ID\n",
       "0    2018-01-02  6.608433  SH601318\n",
       "1    2018-01-02  2.972627  SH600519\n",
       "2    2018-01-02  2.567800  SH600036\n",
       "3    2018-01-02  2.152648  SZ000333\n",
       "4    2018-01-02  1.825902  SZ000651\n",
       "...         ...       ...       ...\n",
       "4195 2018-01-19  0.032009  SH600390\n",
       "4196 2018-01-19  0.031682  SH603858\n",
       "4197 2018-01-19  0.023580  SZ002839\n",
       "4198 2018-01-19  0.021894  SZ002841\n",
       "4199 2018-01-19  0.020838  SZ002831\n",
       "\n",
       "[4200 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index weight:\n",
    "indexCode = 'SH000300'\n",
    "startDate = '20180101'\n",
    "endDate = '20180120'\n",
    "tsstr = \"\"\"\n",
    "           indexTicker:= '{}';\n",
    "           BegT:= {};\n",
    "           EndT:= {} + 0.99;\n",
    "           dateArr:=MarketTradeDayQk(BegT,EndT);\n",
    "           r:=array();\n",
    "           for nI:=0 to length(dateArr)-1 do\n",
    "           begin\n",
    "             GetBKWeightByDate(indexTicker,dateArr[nI],t);\n",
    "             t := t[:,array(\"截止日\",\"代码\",\"比例(%)\")]; \n",
    "             r:=r union t;\n",
    "           end;\n",
    "           return r;  \n",
    "        \"\"\".format(indexCode, startDate + 'T', endDate + 'T')\n",
    "weight_table = pd.DataFrame(TSLPy3.RemoteExecute(tsstr,[],{})[1])\n",
    "weight_table.columns=['date','weight','ID']\n",
    "weight_table['ID'] = weight_table['ID'].str.decode('GBK')\n",
    "weight_table['date'] = pd.to_datetime(weight_table.date.astype(str))\n",
    "weight_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in weight_table.date.unique():\n",
    "    #day = '2019-07-28'\n",
    "    IC_stock = list(IC_weight[IC_weight.date == day].ID.unique())\n",
    "    IF_stock = list(IF_weight[IF_weight.date == day].ID.unique())\n",
    "    CSI1000_stock = list(CSI1000_weight[CSI1000_weight.date == day].ID.unique())\n",
    "    ex_stock = list(set(IC_stock + IF_stock + CSI1000_stock))\n",
    "    assert len(ex_stock) == 1800\n",
    "    CSIRest_weight_day = weight_table[(weight_table.date == day) & (~weight_table.ID.isin(ex_stock))]\n",
    "    CSIRest_weight = CSIRest_weight.append(CSIRest_weight_day, ignore_index = True)\n",
    "sumWeightToday = CSIRest_weight.groupby('date')['weight'].sum().reset_index()\n",
    "sumWeightToday.rename(columns = {'weight':'sumWeightDay'}, inplace = True)\n",
    "weight_table = CSIRest_weight.merge(sumWeightToday, on = 'date', how = 'left')\n",
    "weight_table['weight'] = weight_table['weight'] / weight_table['sumWeightDay'] * 100\n",
    "weight_table = weight_table.drop(columns = {'sumWeightDay'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "sys.path.append(\"D:/program files/Tinysoft/Analyse.NET\")# \n",
    "import TSLPy3 #导入模块 \n",
    "import datetime\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.options.mode.chained_assignment = None\n",
    "from pandas.tseries.offsets import BDay\n",
    "import numpy as np\n",
    "perc = [.01, .05, .1, .25, .5, .75, .9, .95, .99]\n",
    "\n",
    "TodayDate = '20200903'\n",
    "\n",
    "AMACPath = r'D:\\shareWithBenny\\AMAC Industry Weight'\n",
    "YesterdayDate = (datetime.datetime.now() - BDay(5)).strftime('%Y%m%d')\n",
    "tomorrow = (datetime.datetime.now() + BDay(2)).strftime('%Y-%m-%d')\n",
    "#!!!ATTTENTION: temp change here\n",
    "#tomorrow = '2019-06-10'\n",
    "#YesterdayDate = '20200618'\n",
    "assert datetime.datetime.now().strftime('%Y%m%d') == TodayDate\n",
    "\n",
    "\n",
    "## load today AMAC weight table\n",
    "todayAMACRaw = pd.read_excel(AMACPath + '\\\\AMACIndustryWeight_' + TodayDate + '.xls')\n",
    "todayAMAC = todayAMACRaw[['日期\\nDate', '指数代码\\nIndex Code','成分券代码\\nConstituent Code','交易所\\nExchange', '权重(%)\\nWeight(%)']]\n",
    "todayAMAC.rename(columns = {'日期\\nDate':'date', '指数代码\\nIndex Code':'AMACCode','成分券代码\\nConstituent Code':'StockID',\\\n",
    "                            '交易所\\nExchange':'exchange', '权重(%)\\nWeight(%)':'weight'}, inplace = True)\n",
    "assert todayAMAC.date.unique()[0] == datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "todayAMAC['StockID'] = todayAMAC['StockID'].map(lambda x:str(int(x)).zfill(6))\n",
    "todayAMAC['exchange'] = np.where(todayAMAC.exchange == 'Shenzhen', 'SZ', np.where(todayAMAC.exchange == 'Shanghai', 'SH', np.nan))\n",
    "assert len(todayAMAC['exchange'].unique()) == 2\n",
    "todayAMAC['ID'] = todayAMAC['exchange'] + todayAMAC['StockID']\n",
    "assert len(todayAMAC['ID'].unique()) == todayAMAC.shape[0]\n",
    "\n",
    "assert abs(todayAMAC.groupby('AMACCode')['weight'].sum() - 100).max() < 0.2\n",
    "sumWeightToday = todayAMAC.groupby('AMACCode')['weight'].sum().reset_index()\n",
    "sumWeightToday.rename(columns = {'weight':'sumWeightIndex'}, inplace = True)\n",
    "todayAMAC = todayAMAC.merge(sumWeightToday, on = 'AMACCode', how = 'left')\n",
    "todayAMAC['weightNormalized'] = todayAMAC['weight'] / todayAMAC['sumWeightIndex']\n",
    "todayAMAC[['date','AMACCode','ID','weight','weightNormalized']].to_csv(AMACPath + '\\\\AMACNormalizedWeight_' + TodayDate + '.csv', index = False)\n",
    "\n",
    "## load yesterday AMAC weight table\n",
    "yesterdayAMACRaw = pd.read_excel(AMACPath + '\\\\AMACIndustryWeight_' + YesterdayDate + '.xls')\n",
    "yesterdayAMAC = yesterdayAMACRaw[['日期\\nDate', '指数代码\\nIndex Code','成分券代码\\nConstituent Code','交易所\\nExchange', '权重(%)\\nWeight(%)']]\n",
    "yesterdayAMAC.rename(columns = {'日期\\nDate':'date', '指数代码\\nIndex Code':'AMACCode','成分券代码\\nConstituent Code':'StockID',\\\n",
    "                            '交易所\\nExchange':'exchange', '权重(%)\\nWeight(%)':'weight'}, inplace = True)\n",
    "yesterdayAMAC['StockID'] = yesterdayAMAC['StockID'].map(lambda x:str(int(x)).zfill(6))\n",
    "yesterdayAMAC['exchange'] = np.where(yesterdayAMAC.exchange == 'Shenzhen', 'SZ', np.where(yesterdayAMAC.exchange == 'Shanghai', 'SH', np.nan))\n",
    "assert len(yesterdayAMAC['exchange'].unique()) == 2\n",
    "yesterdayAMAC['ID'] = yesterdayAMAC['exchange'] + yesterdayAMAC['StockID']\n",
    "assert abs(yesterdayAMAC.groupby('AMACCode')['weight'].sum() - 100).max() < 0.2\n",
    "\n",
    "## make sure two days have same number of AMAC industry\n",
    "assert todayAMAC.AMACCode.nunique() == yesterdayAMAC.AMACCode.nunique() \n",
    "assert todayAMAC.AMACCode.nunique() == 43\n",
    "## produce changes list\n",
    "changeList = []\n",
    "for industry in todayAMAC.AMACCode.unique():\n",
    "    induToday = todayAMAC[todayAMAC.AMACCode == industry]\n",
    "    induYesterday = yesterdayAMAC[yesterdayAMAC.AMACCode == industry]\n",
    "    weightChangePerStock = induToday[['ID','weight']].merge(induYesterday[['ID','weight']], on = 'ID', how = 'inner')\n",
    "    weightChangePerStock['weightDiff'] = (weightChangePerStock['weight_x'] / weightChangePerStock['weight_y'] - 1).abs()\n",
    "    weightChangePerStock['absWeightDiff'] = (weightChangePerStock['weight_x'] - weightChangePerStock['weight_y']).abs()\n",
    "    ## make the sure the change is less than 15% when the weight is bigger than 0.06\n",
    "    if industry not in []: \n",
    "       assert (weightChangePerStock['weightDiff'].max() < 0.75) | (weightChangePerStock['weight_y'].min() < 0.05)\n",
    "    todayAddList = list(set(induToday.ID.unique()).difference(induYesterday.ID.unique()))\n",
    "    todayDelList = list(set(induYesterday.ID.unique()).difference(induToday.ID.unique()))\n",
    "    todayAddListPerc = round((len(todayAddList) / len(induYesterday.ID.unique())),4)\n",
    "    todayDelListPerc = round((len(todayDelList) / len(induYesterday.ID.unique())),4)\n",
    "    todayAddListWeights = (induToday[induToday.ID.isin(todayAddList)].weight.sum() / 100).round(4)\n",
    "    todayDelListWeights = (induYesterday[induYesterday.ID.isin(todayDelList)].weight.sum() / 100).round(4)\n",
    "    \n",
    "    induChange = [TodayDate, industry, len(induToday.ID.unique()), len(induYesterday.ID.unique()), weightChangePerStock['absWeightDiff'].sum() / 100, todayAddListPerc, todayDelListPerc, todayAddListWeights, todayDelListWeights]\n",
    "    changeList += [induChange]\n",
    "changeTable = pd.DataFrame(changeList, columns = ['date','AMACCode','todayN','yesterdayN','totalAbsWeightChange','addPerc','delPerc','addWeights','delWeights'])   \n",
    "## sum of weight is 1.\n",
    "print('absolute value of the total change in weight: ',round(changeTable['totalAbsWeightChange'].mean(),4))\n",
    "print('    sum of the weight for deleted members: ',round(changeTable.loc[changeTable['delWeights'] > 0,'delWeights'].mean(),4))\n",
    "print('     sum of the weight for added members: ',round(changeTable.loc[changeTable['addWeights'] > 0,'addWeights'].mean(),4))\n",
    "print('number of indices which has deleted members: ',changeTable[(changeTable.delPerc > 0)].shape[0])\n",
    "print(' number of indices which has added members: ',changeTable[(changeTable.addPerc > 0)].shape[0])\n",
    "changeTable_L1 =  pd.read_csv(AMACPath + '\\\\memberChangeInfo_' + YesterdayDate + '.csv')\n",
    "changeTable = changeTable.append(changeTable_L1, ignore_index = True)\n",
    "changeTable.to_csv(AMACPath + '\\\\memberChangeInfo_' + TodayDate + '.csv', index = False)    \n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "    \n",
    "# ============================================================================= \n",
    "# =============================================================================   \n",
    "## STEP 1: check if AMAC changes for all historical members\n",
    "memberList = [] \n",
    "for bench in ['IC','IF','CSI1000','CSIRest']:\n",
    "    indexPath = r'D:\\\\shareWithBenny\\\\indexInfo\\\\' + bench\n",
    "    weightDf = pd.read_pickle(indexPath + '\\\\weight_table_' + bench + '.pkl')\n",
    "#    newStockTrade = sorted(list(pd.read_csv(indexPath + '\\\\curStockList_' + bench + '.csv')['StockID']))\n",
    "#    memberList += sorted(list(set(list(weightDf[weightDf.date >= '2017-09-01'].ID.unique()) + newStockTrade)))\n",
    "    memberList += list(weightDf[weightDf.date >= '2017-09-01'].ID.unique())\n",
    "memberList = sorted(list(set(memberList)))\n",
    "tsstr = \"\"\"\n",
    "            StockArr := array(%s);\n",
    "            r := array();\n",
    "            for nI:=0 to length(StockArr)-1 do begin\n",
    "                    setSysParam(pn_stock(),StockArr[nI]);\n",
    "                    r[nI]['StockID'] := StockArr[nI];\n",
    "                    r[nI]['ZJH2'] := base(10039);\n",
    "            end;\n",
    "            return r;\"\"\"%(str(memberList)[1:-1])\n",
    "curAMACDict = pd.DataFrame(TSLPy3.RemoteExecute(tsstr,[],{})[1])  \n",
    "curAMACDict.columns = ['ID','ZJH2']\n",
    "curAMACDict['ID'] = curAMACDict['ID'].str.decode('GBK')\n",
    "curAMACDict['ZJH2'] = curAMACDict['ZJH2'].str.decode('GBK')\n",
    "curAMACDict['ZJH2'] = curAMACDict['ZJH2'].astype(int)\n",
    "curAMACDict['AMACCode'] = np.where(curAMACDict.ZJH2 <= 5, 'CSIH11030',np.where(curAMACDict.ZJH2 <= 12, 'CSIH11031', \\\n",
    "                   np.where(curAMACDict.ZJH2 == 13, 'CSIH30041',np.where(curAMACDict.ZJH2 == 14, 'CSIH30042', \\\n",
    "                   np.where(curAMACDict.ZJH2 == 15, 'CSIH30043',np.where(curAMACDict.ZJH2 == 17, 'CSIH30044',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 18, 'CSIH30045',np.where(curAMACDict.ZJH2 == 19, 'CSIH30046',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 20, 'CSIH30047',np.where(curAMACDict.ZJH2 == 21, 'CSIH30048',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 22, 'CSIH30049',np.where(curAMACDict.ZJH2 == 23, 'CSIH30050',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 24, 'CSIH30051',np.where(curAMACDict.ZJH2 == 25, 'CSIH30052',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 26, 'CSIH30053',np.where(curAMACDict.ZJH2 == 27, 'CSIH30054',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 28, 'CSIH30055',np.where(curAMACDict.ZJH2 == 29, 'CSIH30056',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 30, 'CSIH30057',np.where(curAMACDict.ZJH2 == 31, 'CSIH30058',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 32, 'CSIH30059',np.where(curAMACDict.ZJH2 == 33, 'CSIH30060',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 34, 'CSIH30061',np.where(curAMACDict.ZJH2 == 35, 'CSIH30062',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 36, 'CSIH30063',np.where(curAMACDict.ZJH2 == 37, 'CSIH30064',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 38, 'CSIH30065',np.where(curAMACDict.ZJH2 == 39, 'CSIH30066',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 40, 'CSIH30067',np.where(curAMACDict.ZJH2 <= 43, 'CSIH11050',\\\n",
    "                   np.where(curAMACDict.ZJH2 <= 46, 'CSIH11041',np.where(curAMACDict.ZJH2 <= 50, 'CSIH11042',\\\n",
    "                   np.where(curAMACDict.ZJH2 <= 52, 'CSIH11045',np.where(curAMACDict.ZJH2 <= 60, 'CSIH11043',\\\n",
    "                   np.where(curAMACDict.ZJH2 <= 62, 'CSIH30036',np.where(curAMACDict.ZJH2 <= 65, 'CSIH11044',\\\n",
    "                   np.where(curAMACDict.ZJH2 <= 69, 'CSIH11046',np.where(curAMACDict.ZJH2 == 70, 'CSIH11047',\\\n",
    "                   np.where(curAMACDict.ZJH2 <= 72, 'CSIH30037',np.where(curAMACDict.ZJH2 <= 75, 'CSIH30038',\\\n",
    "                   np.where(curAMACDict.ZJH2 <= 78, 'CSIH30039',np.where(curAMACDict.ZJH2 <= 81, 'CSIH30040',\\\n",
    "                   np.where(curAMACDict.ZJH2 == 82, 'CSIH11049',np.where(curAMACDict.ZJH2 <= 84, 'CSIH30040',\\\n",
    "                   np.where(curAMACDict.ZJH2 <= 89, 'CSIH11049',np.where(curAMACDict.ZJH2 == 90, 'CSIH11050',np.nan))))))))))))))))))))))))))))))))))))))))))))))\n",
    "oldAMACDict = pd.read_pickle(r'D:\\\\shareWithBenny\\\\indexInfo\\\\AMAC\\\\cur_AMAC_industry.pkl')\n",
    "oldAMACDict.to_pickle(r'D:\\\\shareWithBenny\\\\indexInfo\\\\AMAC\\\\cur_AMAC_industry_old.pkl')\n",
    "compAMACDict = oldAMACDict.merge(curAMACDict, on = ['ID'], how = 'outer', validate = 'one_to_one')\n",
    "assert (pd.notnull(compAMACDict.AMACCode_x).all() & (pd.notnull(compAMACDict.AMACCode_y).all()))\n",
    "assert (compAMACDict.AMACCode_x == compAMACDict.AMACCode_y).all()\n",
    "curAMACDict.to_pickle(r'D:\\\\shareWithBenny\\\\indexInfo\\\\AMAC\\\\cur_AMAC_industry.pkl')\n",
    "\n",
    "## STEP 2: update weekly AMAC table for all stocks\n",
    "histDf = pd.read_pickle(r'D:\\\\shareWithBenny\\\\indexInfo\\\\AMAC\\\\weekly_AMAC_table.pkl')\n",
    "histDf.to_pickle(r'D:\\\\shareWithBenny\\\\indexInfo\\\\AMAC\\\\weekly_AMAC_table_old.pkl')\n",
    "curDf = todayAMAC[['date','ID','AMACCode']]\n",
    "curDf['date'] = pd.to_datetime(curDf['date'])\n",
    "curDf['AMACCode'] = curDf['AMACCode'].map(lambda x : 'CSI' + str(x))\n",
    "curDf['intdate'] = curDf['date'].apply(lambda x: (x-datetime.datetime(1899, 12, 30)).days)\n",
    "curDf = curDf[['intdate','AMACCode','ID']]\n",
    "#memberList = [] \n",
    "#for bench in ['IC','IF','CSI1000','CSIRest']:\n",
    "#    indexPath = r'D:\\\\shareWithBenny\\\\indexInfo\\\\' + bench\n",
    "#    weightDf = pd.read_pickle(indexPath + '\\\\weight_table_' + bench + '.pkl')\n",
    "#    memberList += list(weightDf[weightDf.date >= '2017-09-01'].ID.unique())\n",
    "#memberList = sorted(list(set(memberList)))\n",
    "addList = sorted(list(set(memberList).difference(curDf.ID.unique())))\n",
    "addDf = []\n",
    "for addS in addList:\n",
    "#    print('ADDING ', addS)\n",
    "    addSL = [curDf.intdate.max(), curAMACDict[curAMACDict.ID == addS].AMACCode.values[0], addS]\n",
    "    addDf += [addSL]\n",
    "addDf = pd.DataFrame(addDf, columns = ['intdate','AMACCode','ID'])\n",
    "curDf = curDf.append(addDf).sort_values(['intdate','AMACCode']).reset_index(drop = True)\n",
    "curDf = histDf.append(curDf).sort_values(['intdate','AMACCode']).reset_index(drop = True)\n",
    "curDf.to_pickle(r'D:\\\\shareWithBenny\\\\indexInfo\\\\AMAC\\\\weekly_AMAC_table.pkl')\n",
    "#curDf.groupby('intdate')['ID'].count().tail()\n",
    "\n",
    "## STEP 3: update daily AMAC table for all stocks  \n",
    "curDf = pd.read_pickle(r'D:\\\\shareWithBenny\\\\indexInfo\\\\AMAC\\\\weekly_AMAC_table.pkl')\n",
    "todayIntdate = curDf.intdate.max()\n",
    "assert curDf.groupby(['intdate','ID'])['AMACCode'].count().max() == 1\n",
    "curDf['date'] = curDf['intdate'].apply(lambda x: datetime.datetime(1899, 12, 30) + datetime.timedelta(x))\n",
    "curDf['prevWeek'] = curDf['date'].apply(lambda x: x.isocalendar()[0]*100 + x.isocalendar()[1])\n",
    "curDf = curDf[['prevWeek', 'AMACCode', 'ID']]\n",
    "### each stock each week should only have one value\n",
    "assert((curDf.groupby(['prevWeek', 'ID'])['AMACCode'].count() == 1).all())\n",
    "\n",
    "icData = pd.read_pickle(os.path.join(r'\\\\192.168.10.217\\shareWithBenny\\indexInfo\\IC', 'Day_Index_IC.pkl'))\n",
    "assert (datetime.datetime(1899, 12, 30) + datetime.timedelta(int(icData.intdate.max()))).strftime('%Y%m%d') == TodayDate\n",
    "icData['intdate'] = icData.intdate.astype('int64')\n",
    "icData = icData[icData.intdate <= todayIntdate]\n",
    "icData = icData[['intdate']]\n",
    "icData['date'] = icData['intdate'].apply(lambda x: datetime.datetime(1899, 12, 30) + datetime.timedelta(x))\n",
    "icData['week'] = icData['date'].apply(lambda x: x.isocalendar()[0]*100 + x.isocalendar()[1])\n",
    "icData['hasThu'] = np.where(icData.date.dt.weekday == 3, 1, 0)\n",
    "prevWeek = icData.groupby(['week'])['hasThu'].max().reset_index()\n",
    "prevWeek['prevHasThu'] = prevWeek['hasThu'].shift(1)\n",
    "prevWeek['prevWeek'] = prevWeek['week'].shift(1)\n",
    "prevWeek['prevWeek'] = np.where(prevWeek['prevHasThu'] == 1, prevWeek['prevWeek'], np.nan)\n",
    "prevWeek['prevWeek'] = prevWeek['prevWeek'].ffill()\n",
    "prevWeek = prevWeek[['week', 'prevWeek']]\n",
    "\n",
    "icData = icData[['intdate', 'week']]\n",
    "fullData = icData.append([icData]*(curDf['ID'].nunique() - 1)).sort_values(by=['intdate'])\n",
    "fullData['ID'] = curDf['ID'].unique().tolist()*fullData['intdate'].nunique()\n",
    "fullData = pd.merge(fullData, prevWeek, how='left', on=['week'], validate='many_to_one')\n",
    "fullData = pd.merge(fullData, curDf, how='left', on=['prevWeek', 'ID'], validate='many_to_one')\n",
    "fullData['AMACCode'] = fullData.groupby(['ID'])['AMACCode'].ffill()\n",
    "fullData['AMACCode'] = fullData.groupby(['ID'])['AMACCode'].bfill()\n",
    "## add new coming\n",
    "for miss in list(set(curDf.ID.unique()).difference(fullData[pd.notnull(fullData['AMACCode'])].ID.unique())):\n",
    "    print('missing stock in daily AMAC', miss)\n",
    "    miss_temp = fullData[fullData.ID == 'SH600000'].reset_index(drop = True)\n",
    "    miss_temp['ID'] = miss\n",
    "    miss_temp['AMACCode'] = curDf[curDf.ID == miss].AMACCode.unique()[-1]\n",
    "    fullData = fullData.append(miss_temp)\n",
    "# keep dates after 20170901 (intdate 42979)\n",
    "fullData = fullData.loc[(fullData['intdate'] >= 42979) & (~fullData['AMACCode'].isnull()),\n",
    "                        ['intdate', 'AMACCode', 'ID']].sort_values(by=['intdate', 'AMACCode', 'ID']).reset_index(drop=True)\n",
    "savePath = r'\\\\192.168.10.217\\shareWithBenny\\indexInfo\\AMAC'\n",
    "fullData.to_pickle(os.path.join(savePath, 'daily_AMAC_table.pkl'))  \n",
    "mysqlPath = r'G:\\data\\rch\\raw\\secData_TR\\AMAC_member'\n",
    "fullData.to_csv(os.path.join(mysqlPath, 'daily_AMAC_table.csv.gz'), index = False, compression = 'gzip')  \n",
    "\n",
    "# =============================================================================    \n",
    "# =============================================================================    \n",
    "AMACList = list(todayAMAC.AMACCode.unique())\n",
    "File=open('D:\\shareWithBenny\\generateCPPCode\\\\industryIndexWeights_' + TodayDate +'.txt',\"w\")\n",
    "#File=open('D:\\Dropbox (study-int)\\Andy Yin\\generateCPPCode\\\\test_industryIndexWeights__' + saveDate +'.txt',\"w\")\n",
    "File.write(\"    // as of productionDate \" + tomorrow + \"\\n\")\n",
    "File.write('\\n')\n",
    "for industry in AMACList:\n",
    "    #industry = AMACList[0]\n",
    "    secidAMAC = '30' + industry[1:]\n",
    "    if AMACList.index(industry) == 0: \n",
    "           File.write(\"    if (_indexCode == \" + str(secidAMAC) + ')' + \"\\n\")\n",
    "    else: File.write(\"    else if (_indexCode == \" + str(secidAMAC) + ')' + \"\\n\")\n",
    "    File.write(\"    {\" + \"\\n\")\n",
    "    \n",
    "    stockList = list(todayAMAC[todayAMAC.AMACCode == industry].ID.unique())\n",
    "    for stock in stockList:\n",
    "        #stock = stockList[0]\n",
    "        print ('Processing ', industry, stock)\n",
    "        if stock[:2] == 'SH': secidStock = '1' + stock[2:]\n",
    "        else:  secidStock = '2' + stock[2:]\n",
    "        W = round(todayAMAC.loc[(todayAMAC.AMACCode == industry) & (todayAMAC.ID == stock),'weightNormalized'].values[0], 6)\n",
    "        \n",
    "        File.write(\"        this->indexMembers\" + \"[\" + secidStock + \"] = \"  + 'new IndexMember(' + secidStock + ', ' + str(W) + ', this' + ');' + \"\\n\")        \n",
    "    File.write(\"    }\" + \"\\n\")\n",
    "\n",
    "File.close()\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "##\n",
    "### ============================================================================= \n",
    "#for bench in ['IC','IF','CSI1000','CSIRest']:\n",
    "#    indexPath = r'D:\\\\shareWithBenny\\\\indexInfo\\\\' + bench\n",
    "#    AMACTable = pd.read_pickle(indexPath +'\\\\' + bench + '_AMAC_industry.pkl')    \n",
    "#    AMACTable['indexCat'] = bench\n",
    "#    AMAC_comp = todayAMAC[['date','ID','AMACCode']].merge(AMACTable[['ID','industry','indexCat']], on = 'ID', how = 'right')\n",
    "#    AMAC_comp.rename(columns = {'AMACCode':'AMAC_CSI','industry':'AMAC_TR'}, inplace = True)\n",
    "#    AMAC_comp['AMAC_CSI'] = AMAC_comp['AMAC_CSI'].map(lambda x : 'CSI' + str(x))\n",
    "#    \n",
    "#    AMAC_comp = AMAC_comp[pd.notnull(AMAC_comp.date)]\n",
    "#    assert (AMAC_comp.AMAC_CSI == AMAC_comp.AMAC_TR).all()\n",
    "#    \n",
    "## =============================================================================    \n",
    "#\n",
    "#\n",
    "#\n",
    "### STEP 3: update daily AMAC table for all stocks    \n",
    "#curDf = pd.read_pickle(r'D:\\\\shareWithBenny\\\\indexInfo\\\\AMAC\\\\weekly_AMAC_table.pkl')\n",
    "#assert curDf.groupby(['intdate','ID'])['AMACCode'].count().max() == 1\n",
    "#icData = pd.read_pickle(os.path.join(r'\\\\192.168.10.217\\shareWithBenny\\indexInfo\\IC', 'Day_Index_IC.pkl'))\n",
    "##icData = icData[icData.intdate <= 43748]\n",
    "#icData['intdate'] = icData.intdate.astype('int64')\n",
    "#icData = icData[['intdate']]\n",
    "#fullData = icData.append([icData]*(curDf['ID'].nunique() - 1)).sort_values(by=['intdate'])\n",
    "#fullData['ID'] = curDf['ID'].unique().tolist()*fullData['intdate'].nunique()\n",
    "#fullData = pd.merge(fullData, curDf, how='left', on=['intdate', 'ID'], validate='one_to_one')\n",
    "#fullData['AMACCode'] = fullData.groupby(['ID'])['AMACCode'].ffill()\n",
    "#fullData['AMACCode'] = fullData.groupby(['ID'])['AMACCode'].bfill()\n",
    "#fullData = fullData.sort_values(by=['intdate', 'AMACCode', 'ID'])\n",
    "## keep dates after 20170901 (intdate 42979)\n",
    "#fullData = fullData[(fullData['intdate'] >= 42979) & (~fullData['AMACCode'].isnull())].reset_index(drop=True)\n",
    "#fullData = fullData[['intdate', 'AMACCode', 'ID']]\n",
    "##savePath = r'\\\\192.168.10.217\\shareWithBenny\\indexInfo\\AMAC'\n",
    "##fullData.to_pickle(os.path.join(savePath, 'daily_AMAC_table_20191107v0.pkl'))  \n",
    "#\n",
    "#\n",
    "#['SH603815', 'SZ300795', 'SZ300793']\n",
    "#histDf = pd.read_pickle(r'D:\\\\shareWithBenny\\\\indexInfo\\\\AMAC\\\\daily_AMAC_table.pkl')\n",
    "#com_Df = fullData.merge(histDf, on = ['intdate', 'ID'], how = 'left', validate='one_to_one')\n",
    "#temp1 = com_Df[pd.isnull(com_Df.AMACCode_x) & pd.notnull(com_Df.AMACCode_y)]\n",
    "#assert len(temp1) == 0\n",
    "#temp2 = com_Df[pd.isnull(com_Df.AMACCode_y) & pd.notnull(com_Df.AMACCode_x)]\n",
    "#newList = temp2.ID.unique()\n",
    "#temp3 = com_Df[(com_Df.AMACCode_y != com_Df.AMACCode_x) & (~com_Df.ID.isin(newList))].sort_values('intdate').reset_index(drop = True)\n",
    "#temp3['date'] = temp3['intdate'].apply(lambda x: datetime.datetime(1899, 12, 30) + datetime.timedelta(x))\n",
    "#temp3['weekday'] = temp3['date'].dt.weekday\n",
    "#temp3['chgCount'] = temp3.groupby('ID')['date'].transform('count')\n",
    "#temp3 = temp3.sort_values(by = ['chgCount','ID'], ascending = [False,True]).reset_index(drop=True)\n",
    "##temp3.shape\n",
    "##Out[87]: (326, 7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
