{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'M:\\\\tickData_AMAC\\\\tick_stock_202001\\\\Tick_CSIH11041.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f05b9db6c195>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'M:\\tickData_AMAC\\tick_stock_202001\\Tick_CSIH11041.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(path, compression)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m    144\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;31m# 1) try standard libary Pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'M:\\\\tickData_AMAC\\\\tick_stock_202001\\\\Tick_CSIH11041.pkl'"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "import numpy as np\n",
    "\n",
    "test = pd.read_pickle(r'M:\\tickData_AMAC\\tick_stock_202001\\Tick_CSIH11041.pkl')\n",
    "test['date'] = test['time'].astype(str).apply(lambda x: int(x.split(' ')[0].replace(\"-\", \"\")))\n",
    "test['time'] = test['time'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(\":\", \"\")))\n",
    "test['datetime'] = test['date'] * 1000000 + test['time']\n",
    "test[\"clockAtArrival\"] = test[\"datetime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "test['datetime'] = test[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "test['time'] = test['time'].astype('int64') * 1000000\n",
    "test['skey'] = test['ID'].str[4:].astype(int) + 3000000\n",
    "test = test.rename(columns={'industry_open':\"open\"})\n",
    "test['open1'] = test.groupby(['skey', 'date'])['open'].transform('max')\n",
    "test['close1'] = test['close'].shift(1)\n",
    "# 每天只有一条tick cum_volume==0, 且当时open==today's open, close==yesterday's close\n",
    "assert(sum(test[test['cum_volume'] == 0].groupby('date')['ordering'].size() != 1) == 0)\n",
    "assert((test[test['cum_volume'] == 0]['open'].min() > 0) & (test[test['open'] != test['open1']].shape[0] == 0))\n",
    "assert(sum(test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close'] != \n",
    "    test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close1']) == 0)\n",
    "test = test[test['cum_volume'] != 0]\n",
    "test = test.sort_values(by=['date', 'ordering'])\n",
    "test = test[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"cum_volume\", \"cum_amount\", \n",
    "             \"open\", \"close\"]]\n",
    "# change to second level tick data\n",
    "k1 = test.groupby(['date', 'skey'])['datetime'].min().reset_index()\n",
    "k1 = k1.rename(columns={'datetime':'min'})\n",
    "k2 = test.groupby(['date', 'skey'])['datetime'].max().reset_index()\n",
    "k2 = k2.rename(columns={'datetime':'max'})\n",
    "k = pd.merge(k1, k2, on=['date', 'skey'])\n",
    "k['diff'] = (k['max']-k['min']).apply(lambda x: x.seconds)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i in np.arange(k.shape[0]):\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['datetime1'] = [k.loc[i, 'min'] + datetime.timedelta(seconds=int(x)) for x in np.arange(0, k.loc[i, 'diff'] + 1)]\n",
    "    df1['skey'] = k.loc[i, 'skey']\n",
    "    df1['date'] = k.loc[i, 'date']\n",
    "    assert(df1['datetime1'].min() == k.loc[i, 'min'])\n",
    "    assert(df1['datetime1'].max() == k.loc[i, 'max'])\n",
    "    df = pd.concat([df, df1])\n",
    "test = pd.merge(test, df, left_on=['skey', 'datetime', 'date'], right_on=['skey', 'datetime1', 'date'], how='outer').sort_values(by=['skey', 'date', 'datetime1']).reset_index(drop=True)\n",
    "assert(test[test['datetime1'].isnull()].shape[0] == 0)\n",
    "for cols in ['cum_volume', 'cum_amount', 'open', 'close']:\n",
    "    test[cols] = test.groupby('skey')[cols].ffill()\n",
    "test.drop([\"datetime\"],axis=1,inplace=True)\n",
    "test = test.rename(columns={'datetime1':'datetime'})\n",
    "test['skey'] = test['skey'].astype('int32')\n",
    "test[\"time\"] = test['datetime'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(':', \"\"))).astype(np.int64)\n",
    "test['SendingTime'] = test['date'] * 1000000 + test['time']\n",
    "test[\"clockAtArrival\"] = test[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "test.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "test['time'] = test['time'] * 1000000\n",
    "\n",
    "assert(sum(test[test[\"open\"] != 0].groupby([\"skey\", 'date'])[\"open\"].nunique() != 1) == 0)\n",
    "test[\"open\"] = np.where(test[\"cum_volume\"] > 0, test.groupby([\"skey\", 'date'])[\"open\"].transform(\"max\"), test[\"open\"])\n",
    "assert(sum(test[test[\"open\"] != 0].groupby([\"skey\", 'date'])[\"open\"].nunique() != 1) == 0)\n",
    "assert(test[test[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "\n",
    "test['date'] = test['date'].astype('int32')\n",
    "test['cum_volume'] = test['cum_volume'].astype('int64')\n",
    "test = test.sort_values(by=['date', 'skey', 'time'])\n",
    "test[\"ordering\"] = test.groupby([\"skey\", 'date']).cumcount() + 1\n",
    "test['ordering'] = test['ordering'].astype('int32')\n",
    "\n",
    "assert(test['cum_amount'].astype(str).apply(lambda x: len(x.split('.')[1])).max() == 4)\n",
    "assert(test['open'].astype(str).apply(lambda x: len(x.split('.')[1])).max() == 4)\n",
    "assert(test['close'].astype(str).apply(lambda x: len(x.split('.')[1])).max() == 4)\n",
    "\n",
    "test = test[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"cum_volume\", \"cum_amount\", \n",
    "         \"open\", \"close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'TSLPy3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1d3d5437b2bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/progrma files/Tinysoft/Analyse.NET\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mTSLPy3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTSLPy3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRemoteExecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return 1;\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'TSLPy3'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/progrma files/Tinysoft/Analyse.NET\")\n",
    "import TSLPy3\n",
    "data = TSLPy3.RemoteExecute(\"return 1;\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skey</th>\n",
       "      <th>time</th>\n",
       "      <th>clockAtArrival</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20200102</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130005000000</td>\n",
       "      <td>1577941205000000</td>\n",
       "      <td>2020-01-02 13:00:05</td>\n",
       "      <td>776537904.0</td>\n",
       "      <td>426913.1022</td>\n",
       "      <td>1735.0651</td>\n",
       "      <td>1741.3676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200103</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130005000000</td>\n",
       "      <td>1578027605000000</td>\n",
       "      <td>2020-01-03 13:00:05</td>\n",
       "      <td>631666243.0</td>\n",
       "      <td>381000.5140</td>\n",
       "      <td>1742.8987</td>\n",
       "      <td>1743.6417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200106</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130004000000</td>\n",
       "      <td>1578286804000000</td>\n",
       "      <td>2020-01-06 13:00:04</td>\n",
       "      <td>808174846.0</td>\n",
       "      <td>488619.4116</td>\n",
       "      <td>1741.6024</td>\n",
       "      <td>1742.3595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200107</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130001000000</td>\n",
       "      <td>1578373201000000</td>\n",
       "      <td>2020-01-07 13:00:01</td>\n",
       "      <td>879092634.0</td>\n",
       "      <td>547724.7280</td>\n",
       "      <td>1740.8944</td>\n",
       "      <td>1744.4468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200108</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130002000000</td>\n",
       "      <td>1578459602000000</td>\n",
       "      <td>2020-01-08 13:00:02</td>\n",
       "      <td>780720214.0</td>\n",
       "      <td>470607.7068</td>\n",
       "      <td>1745.7644</td>\n",
       "      <td>1740.6562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200109</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130001000000</td>\n",
       "      <td>1578546001000000</td>\n",
       "      <td>2020-01-09 13:00:01</td>\n",
       "      <td>714341777.0</td>\n",
       "      <td>408561.9773</td>\n",
       "      <td>1737.7325</td>\n",
       "      <td>1733.3469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200110</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130001000000</td>\n",
       "      <td>1578632401000000</td>\n",
       "      <td>2020-01-10 13:00:01</td>\n",
       "      <td>614852604.0</td>\n",
       "      <td>363957.5615</td>\n",
       "      <td>1740.6900</td>\n",
       "      <td>1728.7751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200113</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130002000000</td>\n",
       "      <td>1578891602000000</td>\n",
       "      <td>2020-01-13 13:00:02</td>\n",
       "      <td>592580069.0</td>\n",
       "      <td>331362.5415</td>\n",
       "      <td>1727.9399</td>\n",
       "      <td>1728.4251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200114</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130000000000</td>\n",
       "      <td>1578978000000000</td>\n",
       "      <td>2020-01-14 13:00:00</td>\n",
       "      <td>758181621.0</td>\n",
       "      <td>415746.3226</td>\n",
       "      <td>1737.1173</td>\n",
       "      <td>1739.7512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200115</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130003000000</td>\n",
       "      <td>1579064403000000</td>\n",
       "      <td>2020-01-15 13:00:03</td>\n",
       "      <td>645670581.0</td>\n",
       "      <td>362149.5136</td>\n",
       "      <td>1739.2733</td>\n",
       "      <td>1729.4077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200116</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130005000000</td>\n",
       "      <td>1579150805000000</td>\n",
       "      <td>2020-01-16 13:00:05</td>\n",
       "      <td>492241019.0</td>\n",
       "      <td>297941.9875</td>\n",
       "      <td>1728.6409</td>\n",
       "      <td>1722.6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200117</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130004000000</td>\n",
       "      <td>1579237204000000</td>\n",
       "      <td>2020-01-17 13:00:04</td>\n",
       "      <td>427567213.0</td>\n",
       "      <td>290425.0290</td>\n",
       "      <td>1715.5208</td>\n",
       "      <td>1705.0013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200120</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130004000000</td>\n",
       "      <td>1579496404000000</td>\n",
       "      <td>2020-01-20 13:00:04</td>\n",
       "      <td>506813157.0</td>\n",
       "      <td>320519.3783</td>\n",
       "      <td>1704.1873</td>\n",
       "      <td>1705.1445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200121</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130004000000</td>\n",
       "      <td>1579582804000000</td>\n",
       "      <td>2020-01-21 13:00:04</td>\n",
       "      <td>610982794.0</td>\n",
       "      <td>344703.4730</td>\n",
       "      <td>1704.9506</td>\n",
       "      <td>1695.0889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200122</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130003000000</td>\n",
       "      <td>1579669203000000</td>\n",
       "      <td>2020-01-22 13:00:03</td>\n",
       "      <td>525873750.0</td>\n",
       "      <td>298149.7045</td>\n",
       "      <td>1688.2535</td>\n",
       "      <td>1683.8723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200123</td>\n",
       "      <td>3011041</td>\n",
       "      <td>130003000000</td>\n",
       "      <td>1579755603000000</td>\n",
       "      <td>2020-01-23 13:00:03</td>\n",
       "      <td>534387940.0</td>\n",
       "      <td>289794.8060</td>\n",
       "      <td>1676.0461</td>\n",
       "      <td>1663.7475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             skey          time    clockAtArrival            datetime  \\\n",
       "date                                                                    \n",
       "20200102  3011041  130005000000  1577941205000000 2020-01-02 13:00:05   \n",
       "20200103  3011041  130005000000  1578027605000000 2020-01-03 13:00:05   \n",
       "20200106  3011041  130004000000  1578286804000000 2020-01-06 13:00:04   \n",
       "20200107  3011041  130001000000  1578373201000000 2020-01-07 13:00:01   \n",
       "20200108  3011041  130002000000  1578459602000000 2020-01-08 13:00:02   \n",
       "20200109  3011041  130001000000  1578546001000000 2020-01-09 13:00:01   \n",
       "20200110  3011041  130001000000  1578632401000000 2020-01-10 13:00:01   \n",
       "20200113  3011041  130002000000  1578891602000000 2020-01-13 13:00:02   \n",
       "20200114  3011041  130000000000  1578978000000000 2020-01-14 13:00:00   \n",
       "20200115  3011041  130003000000  1579064403000000 2020-01-15 13:00:03   \n",
       "20200116  3011041  130005000000  1579150805000000 2020-01-16 13:00:05   \n",
       "20200117  3011041  130004000000  1579237204000000 2020-01-17 13:00:04   \n",
       "20200120  3011041  130004000000  1579496404000000 2020-01-20 13:00:04   \n",
       "20200121  3011041  130004000000  1579582804000000 2020-01-21 13:00:04   \n",
       "20200122  3011041  130003000000  1579669203000000 2020-01-22 13:00:03   \n",
       "20200123  3011041  130003000000  1579755603000000 2020-01-23 13:00:03   \n",
       "\n",
       "           cum_volume   cum_amount       open      close  \n",
       "date                                                      \n",
       "20200102  776537904.0  426913.1022  1735.0651  1741.3676  \n",
       "20200103  631666243.0  381000.5140  1742.8987  1743.6417  \n",
       "20200106  808174846.0  488619.4116  1741.6024  1742.3595  \n",
       "20200107  879092634.0  547724.7280  1740.8944  1744.4468  \n",
       "20200108  780720214.0  470607.7068  1745.7644  1740.6562  \n",
       "20200109  714341777.0  408561.9773  1737.7325  1733.3469  \n",
       "20200110  614852604.0  363957.5615  1740.6900  1728.7751  \n",
       "20200113  592580069.0  331362.5415  1727.9399  1728.4251  \n",
       "20200114  758181621.0  415746.3226  1737.1173  1739.7512  \n",
       "20200115  645670581.0  362149.5136  1739.2733  1729.4077  \n",
       "20200116  492241019.0  297941.9875  1728.6409  1722.6023  \n",
       "20200117  427567213.0  290425.0290  1715.5208  1705.0013  \n",
       "20200120  506813157.0  320519.3783  1704.1873  1705.1445  \n",
       "20200121  610982794.0  344703.4730  1704.9506  1695.0889  \n",
       "20200122  525873750.0  298149.7045  1688.2535  1683.8723  \n",
       "20200123  534387940.0  289794.8060  1676.0461  1663.7475  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['time'] >= 130000000000].groupby('date').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skey</th>\n",
       "      <th>time</th>\n",
       "      <th>clockAtArrival</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20200102</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113010000000</td>\n",
       "      <td>1577935810000000</td>\n",
       "      <td>2020-01-02 11:30:10</td>\n",
       "      <td>772485160.0</td>\n",
       "      <td>424815.7186</td>\n",
       "      <td>1735.0651</td>\n",
       "      <td>1741.5859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200103</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113005000000</td>\n",
       "      <td>1578022205000000</td>\n",
       "      <td>2020-01-03 11:30:05</td>\n",
       "      <td>628413339.0</td>\n",
       "      <td>379335.8051</td>\n",
       "      <td>1742.8987</td>\n",
       "      <td>1743.9610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200106</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113014000000</td>\n",
       "      <td>1578281414000000</td>\n",
       "      <td>2020-01-06 11:30:14</td>\n",
       "      <td>805051811.0</td>\n",
       "      <td>487146.1411</td>\n",
       "      <td>1741.6024</td>\n",
       "      <td>1742.3351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200107</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113006000000</td>\n",
       "      <td>1578367806000000</td>\n",
       "      <td>2020-01-07 11:30:06</td>\n",
       "      <td>878877134.0</td>\n",
       "      <td>547650.8115</td>\n",
       "      <td>1740.8944</td>\n",
       "      <td>1744.4468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200108</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113012000000</td>\n",
       "      <td>1578454212000000</td>\n",
       "      <td>2020-01-08 11:30:12</td>\n",
       "      <td>780714014.0</td>\n",
       "      <td>470604.1963</td>\n",
       "      <td>1745.7644</td>\n",
       "      <td>1740.6562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200109</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113011000000</td>\n",
       "      <td>1578540611000000</td>\n",
       "      <td>2020-01-09 11:30:11</td>\n",
       "      <td>714193889.0</td>\n",
       "      <td>408481.9900</td>\n",
       "      <td>1737.7325</td>\n",
       "      <td>1733.4006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200110</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113011000000</td>\n",
       "      <td>1578627011000000</td>\n",
       "      <td>2020-01-10 11:30:11</td>\n",
       "      <td>614834304.0</td>\n",
       "      <td>363937.6582</td>\n",
       "      <td>1740.6900</td>\n",
       "      <td>1728.7751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200113</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113012000000</td>\n",
       "      <td>1578886212000000</td>\n",
       "      <td>2020-01-13 11:30:12</td>\n",
       "      <td>592575169.0</td>\n",
       "      <td>331360.4456</td>\n",
       "      <td>1727.9399</td>\n",
       "      <td>1728.5299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200114</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113010000000</td>\n",
       "      <td>1578972610000000</td>\n",
       "      <td>2020-01-14 11:30:10</td>\n",
       "      <td>758172921.0</td>\n",
       "      <td>415742.9979</td>\n",
       "      <td>1737.1173</td>\n",
       "      <td>1739.7512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200115</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113008000000</td>\n",
       "      <td>1579059008000000</td>\n",
       "      <td>2020-01-15 11:30:08</td>\n",
       "      <td>645663581.0</td>\n",
       "      <td>362140.7356</td>\n",
       "      <td>1739.2733</td>\n",
       "      <td>1729.4077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200116</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113010000000</td>\n",
       "      <td>1579145410000000</td>\n",
       "      <td>2020-01-16 11:30:10</td>\n",
       "      <td>490353735.0</td>\n",
       "      <td>297014.7339</td>\n",
       "      <td>1728.6409</td>\n",
       "      <td>1722.5577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200117</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113014000000</td>\n",
       "      <td>1579231814000000</td>\n",
       "      <td>2020-01-17 11:30:14</td>\n",
       "      <td>425623788.0</td>\n",
       "      <td>289523.8326</td>\n",
       "      <td>1715.5208</td>\n",
       "      <td>1704.9415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200120</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113009000000</td>\n",
       "      <td>1579491009000000</td>\n",
       "      <td>2020-01-20 11:30:09</td>\n",
       "      <td>503182014.0</td>\n",
       "      <td>318648.7477</td>\n",
       "      <td>1704.1873</td>\n",
       "      <td>1705.0904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200121</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113009000000</td>\n",
       "      <td>1579577409000000</td>\n",
       "      <td>2020-01-21 11:30:09</td>\n",
       "      <td>609222652.0</td>\n",
       "      <td>343912.4795</td>\n",
       "      <td>1704.9506</td>\n",
       "      <td>1694.7180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200122</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113008000000</td>\n",
       "      <td>1579663808000000</td>\n",
       "      <td>2020-01-22 11:30:08</td>\n",
       "      <td>525713348.0</td>\n",
       "      <td>298071.2016</td>\n",
       "      <td>1688.2535</td>\n",
       "      <td>1683.7218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200123</td>\n",
       "      <td>3011041</td>\n",
       "      <td>113013000000</td>\n",
       "      <td>1579750213000000</td>\n",
       "      <td>2020-01-23 11:30:13</td>\n",
       "      <td>534105540.0</td>\n",
       "      <td>289670.7287</td>\n",
       "      <td>1676.0461</td>\n",
       "      <td>1663.7673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             skey          time    clockAtArrival            datetime  \\\n",
       "date                                                                    \n",
       "20200102  3011041  113010000000  1577935810000000 2020-01-02 11:30:10   \n",
       "20200103  3011041  113005000000  1578022205000000 2020-01-03 11:30:05   \n",
       "20200106  3011041  113014000000  1578281414000000 2020-01-06 11:30:14   \n",
       "20200107  3011041  113006000000  1578367806000000 2020-01-07 11:30:06   \n",
       "20200108  3011041  113012000000  1578454212000000 2020-01-08 11:30:12   \n",
       "20200109  3011041  113011000000  1578540611000000 2020-01-09 11:30:11   \n",
       "20200110  3011041  113011000000  1578627011000000 2020-01-10 11:30:11   \n",
       "20200113  3011041  113012000000  1578886212000000 2020-01-13 11:30:12   \n",
       "20200114  3011041  113010000000  1578972610000000 2020-01-14 11:30:10   \n",
       "20200115  3011041  113008000000  1579059008000000 2020-01-15 11:30:08   \n",
       "20200116  3011041  113010000000  1579145410000000 2020-01-16 11:30:10   \n",
       "20200117  3011041  113014000000  1579231814000000 2020-01-17 11:30:14   \n",
       "20200120  3011041  113009000000  1579491009000000 2020-01-20 11:30:09   \n",
       "20200121  3011041  113009000000  1579577409000000 2020-01-21 11:30:09   \n",
       "20200122  3011041  113008000000  1579663808000000 2020-01-22 11:30:08   \n",
       "20200123  3011041  113013000000  1579750213000000 2020-01-23 11:30:13   \n",
       "\n",
       "           cum_volume   cum_amount       open      close  \n",
       "date                                                      \n",
       "20200102  772485160.0  424815.7186  1735.0651  1741.5859  \n",
       "20200103  628413339.0  379335.8051  1742.8987  1743.9610  \n",
       "20200106  805051811.0  487146.1411  1741.6024  1742.3351  \n",
       "20200107  878877134.0  547650.8115  1740.8944  1744.4468  \n",
       "20200108  780714014.0  470604.1963  1745.7644  1740.6562  \n",
       "20200109  714193889.0  408481.9900  1737.7325  1733.4006  \n",
       "20200110  614834304.0  363937.6582  1740.6900  1728.7751  \n",
       "20200113  592575169.0  331360.4456  1727.9399  1728.5299  \n",
       "20200114  758172921.0  415742.9979  1737.1173  1739.7512  \n",
       "20200115  645663581.0  362140.7356  1739.2733  1729.4077  \n",
       "20200116  490353735.0  297014.7339  1728.6409  1722.5577  \n",
       "20200117  425623788.0  289523.8326  1715.5208  1704.9415  \n",
       "20200120  503182014.0  318648.7477  1704.1873  1705.0904  \n",
       "20200121  609222652.0  343912.4795  1704.9506  1694.7180  \n",
       "20200122  525713348.0  298071.2016  1688.2535  1683.7218  \n",
       "20200123  534105540.0  289670.7287  1676.0461  1663.7673  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['time'] < 130000000000].groupby('date').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(test['cum_amount'].astype(str).apply(lambda x: len(x.split('.')[1])).max() == 4)\n",
    "assert(test['open'].astype(str).apply(lambda x: len(x.split('.')[1])).max() == 4)\n",
    "assert(test['close'].astype(str).apply(lambda x: len(x.split('.')[1])).max() == 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017-05-29T08:59:51.000000000']\n",
      "['2017-05-29T08:59:51.000000000' '2017-05-31T08:59:55.000000000']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'M:\\\\tickData_AMAC\\\\tick_stock_201705'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'M:\\\\tickData_AMAC\\\\tick_stock_201706'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'M:\\\\tickData_AMAC\\\\tick_stock_201707'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'M:\\\\tickData_AMAC\\\\tick_stock_201708'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'M:\\\\tickData_AMAC\\\\tick_stock_201709'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'M:\\\\tickData_AMAC\\\\tick_stock_201710'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'M:\\\\tickData_AMAC\\\\tick_stock_201711'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'M:\\\\tickData_AMAC\\\\tick_stock_201712'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "year = \"2017\"\n",
    "startDate = '201701'\n",
    "endDate = '201712'\n",
    "readPath = 'M:\\\\tickData_AMAC\\\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([os.path.basename(i).split('_')[2] for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "wr_ong = []\n",
    "mi_ss = []\n",
    "less = []\n",
    "\n",
    "for data in np.sort(dataPathLs):\n",
    "    d = np.array(glob.glob(data + '\\\\***'))\n",
    "    test = []\n",
    "    for dd in d:\n",
    "        df = pd.read_pickle(dd)\n",
    "        test += [df]\n",
    "    test = pd.concat(test)\n",
    "    test['date'] = test['time'].astype(str).apply(lambda x: int(x.split(' ')[0].replace(\"-\", \"\")))\n",
    "    test['time'] = test['time'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(\":\", \"\")))\n",
    "    test['datetime'] = test['date'] * 1000000 + test['time']\n",
    "    test[\"clockAtArrival\"] = test[\"datetime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "    test['datetime'] = test[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    test['time'] = test['time'].astype('int64') * 1000000\n",
    "    test['skey'] = test['ID'].str[4:].astype(int) + 3000000\n",
    "    test = test.rename(columns={'industry_open':\"open\"})\n",
    "    test['open1'] = test.groupby(['skey', 'date'])['open'].transform('max')\n",
    "    test = test.sort_values(by=['skey', 'date', 'ordering'])\n",
    "    test['close1'] = test.groupby(['skey'])['close'].shift(1)\n",
    "    # 每天只有一条tick cum_volume==0, 且当时open==today's open, close==yesterday's close\n",
    "    # 20180215, 20180220, 20180221不满足条件，这几天无交易，close=0；20180215, 20180222 close close1也无法对上\n",
    "    assert(sum(test[test['cum_volume'] == 0].groupby(['skey', 'date'])['ordering'].size() != 1) == 0)\n",
    "    assert(sum(test[test['cum_volume'] == 0].groupby(['skey', 'date'])['ordering'].unique() != 0) == 0)\n",
    "    try:\n",
    "        assert((test[test['cum_volume'] == 0]['open'].min() > 0) & (test[test['open'] != test['open1']].shape[0] == 0))\n",
    "        assert(sum(test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close'] != \n",
    "        test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close1']) == 0)\n",
    "    except:\n",
    "        print(test[(test['cum_volume'] == 0) & (test['open'] == 0)]['datetime'].unique())\n",
    "        print(test[(test['cum_volume'] == 0) & (~test['close1'].isnull())][test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close'] != \n",
    "        test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close1']]['datetime'].unique())\n",
    "    test = test[test['cum_volume'] != 0]\n",
    "    test = test.sort_values(by=['skey', 'date', 'ordering'])\n",
    "    test = test[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"cum_volume\", \"cum_amount\", \n",
    "                 \"open\", \"close\"]]\n",
    "    # change to second level tick data\n",
    "    k1 = test.groupby(['date', 'skey'])['datetime'].min().reset_index()\n",
    "    k1 = k1.rename(columns={'datetime':'min'})\n",
    "    k2 = test.groupby(['date', 'skey'])['datetime'].max().reset_index()\n",
    "    k2 = k2.rename(columns={'datetime':'max'})\n",
    "    k = pd.merge(k1, k2, on=['date', 'skey'])\n",
    "    k['diff'] = (k['max']-k['min']).apply(lambda x: x.seconds)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for i in np.arange(k.shape[0]):\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['datetime1'] = [k.loc[i, 'min'] + datetime.timedelta(seconds=int(x)) for x in np.arange(0, k.loc[i, 'diff'] + 1)]\n",
    "        df1['skey'] = k.loc[i, 'skey']\n",
    "        df1['date'] = k.loc[i, 'date']\n",
    "        assert(df1['datetime1'].min() == k.loc[i, 'min'])\n",
    "        assert(df1['datetime1'].max() == k.loc[i, 'max'])\n",
    "        df = pd.concat([df, df1])\n",
    "    test = pd.merge(test, df, left_on=['skey', 'datetime', 'date'], right_on=['skey', 'datetime1', 'date'], how='outer').sort_values(by=['skey', 'date', 'datetime1']).reset_index(drop=True)\n",
    "    assert(test[test['datetime1'].isnull()].shape[0] == 0)\n",
    "    for cols in ['cum_volume', 'cum_amount', 'open', 'close']:\n",
    "        test[cols] = test.groupby(['skey', 'date'])[cols].ffill()\n",
    "    test.drop([\"datetime\"],axis=1,inplace=True)\n",
    "    test = test.rename(columns={'datetime1':'datetime'})\n",
    "    test['skey'] = test['skey'].astype('int32')\n",
    "    test[\"time\"] = test['datetime'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(':', \"\"))).astype(np.int64)\n",
    "    test['SendingTime'] = test['date'] * 1000000 + test['time']\n",
    "    test[\"clockAtArrival\"] = test[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "    test.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "    test['time'] = test['time'] * 1000000\n",
    "\n",
    "    assert(sum(test[test[\"open\"] != 0].groupby([\"skey\", 'date'])[\"open\"].nunique() != 1) == 0)\n",
    "    test[\"open\"] = np.where(test[\"cum_volume\"] > 0, test.groupby([\"skey\", 'date'])[\"open\"].transform(\"max\"), test[\"open\"])\n",
    "    assert(sum(test[test[\"open\"] != 0].groupby([\"skey\", 'date'])[\"open\"].nunique() != 1) == 0)\n",
    "    assert(test[test[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "\n",
    "    test['date'] = test['date'].astype('int32')\n",
    "    test['cum_volume'] = test['cum_volume'].astype('int64')\n",
    "    \n",
    "    m_in = test[test['time'] <= 113500000000].groupby('skey').last()['time'].min()\n",
    "    m_ax = test[test['time'] >= 125500000000].groupby('skey').first()['time'].max()\n",
    "    assert(test[(test['time'] >= m_in) & (test['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open',  \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0\n",
    "           & (sum(test[(test['time'] >= m_in) & (test['time'] <= m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "           (sum(test[(test['time'] >= m_in) & (test['time'] <= m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "    test = pd.concat([test[test['time'] <= 113500000000], test[test['time'] >= 125500000000]])\n",
    "    \n",
    "    test = test.sort_values(by=[\"skey\", 'date', 'time'])\n",
    "    test[\"ordering\"] = test.groupby([\"skey\", 'date']).cumcount() + 1\n",
    "    test['ordering'] = test['ordering'].astype('int32')\n",
    "\n",
    "    for cols in ['open', 'cum_amount', 'close']:\n",
    "        test[cols] = test[cols].apply(lambda x: round(x, 4)).astype('float64')\n",
    "    assert(test['time'].max() < 150500000000)\n",
    "\n",
    "    test = test[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"cum_volume\", \"cum_amount\", \n",
    "             \"open\", \"close\"]]\n",
    "    \n",
    "    display(data)\n",
    "    print(\"index finished\")\n",
    "    \n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    db1.write('md_index', test)\n",
    "    \n",
    "    del test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intdate</th>\n",
       "      <th>minute</th>\n",
       "      <th>morning</th>\n",
       "      <th>time</th>\n",
       "      <th>close</th>\n",
       "      <th>industry_open</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "      <th>ID</th>\n",
       "      <th>ordering</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>close1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>43437</td>\n",
       "      <td>541</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-03 09:00:01</td>\n",
       "      <td>1957.1232</td>\n",
       "      <td>1956.8878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53966</td>\n",
       "      <td>43462</td>\n",
       "      <td>540</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-28 09:00:00</td>\n",
       "      <td>1946.3828</td>\n",
       "      <td>1932.5982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>1945.9439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       intdate  minute  morning                time      close  industry_open  \\\n",
       "0        43437     541        1 2018-12-03 09:00:01  1957.1232      1956.8878   \n",
       "53966    43462     540        1 2018-12-28 09:00:00  1946.3828      1932.5982   \n",
       "\n",
       "       cum_volume  cum_amount         ID  ordering   month        date  \\\n",
       "0             0.0         0.0  CSIH11030       0.0  201812  2018-12-03   \n",
       "53966         0.0         0.0  CSIH11030       0.0  201812  2018-12-28   \n",
       "\n",
       "          close1  \n",
       "0            NaN  \n",
       "53966  1945.9439  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = pd.read_pickle(r'M:\\tickData_AMAC\\tick_stock_201812\\Tick_CSIH11030.pkl')\n",
    "k['date'] = k['time'].astype(str).str[:10]\n",
    "k = k.sort_values(by=['ID', 'date', 'ordering'])\n",
    "k['close1'] = k.groupby(['ID'])['close'].shift(1)\n",
    "k[(k['cum_volume'] == 0) & (k['close'] != k['close1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intdate</th>\n",
       "      <th>minute</th>\n",
       "      <th>morning</th>\n",
       "      <th>time</th>\n",
       "      <th>close</th>\n",
       "      <th>industry_open</th>\n",
       "      <th>cum_volume</th>\n",
       "      <th>cum_amount</th>\n",
       "      <th>ID</th>\n",
       "      <th>ordering</th>\n",
       "      <th>month</th>\n",
       "      <th>date</th>\n",
       "      <th>close1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>51116</td>\n",
       "      <td>43461</td>\n",
       "      <td>540</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-27 08:59:59</td>\n",
       "      <td>1967.3388</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1967.3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51117</td>\n",
       "      <td>43461</td>\n",
       "      <td>566</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-27 09:25:04</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>3234187.0</td>\n",
       "      <td>2502.0035</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1967.3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51118</td>\n",
       "      <td>43461</td>\n",
       "      <td>571</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-27 09:30:03</td>\n",
       "      <td>1990.5268</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>5975050.0</td>\n",
       "      <td>4547.6110</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>2.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1987.6867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51119</td>\n",
       "      <td>43461</td>\n",
       "      <td>571</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-27 09:30:08</td>\n",
       "      <td>1989.6967</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>7303493.0</td>\n",
       "      <td>5588.1182</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>3.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1990.5268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51120</td>\n",
       "      <td>43461</td>\n",
       "      <td>571</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-27 09:30:13</td>\n",
       "      <td>1991.0568</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>7753194.0</td>\n",
       "      <td>5871.2612</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>4.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1989.6967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53961</td>\n",
       "      <td>43461</td>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-27 14:56:51</td>\n",
       "      <td>1943.5599</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>287853119.0</td>\n",
       "      <td>216778.8715</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>2845.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1943.7983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53962</td>\n",
       "      <td>43461</td>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-27 14:56:56</td>\n",
       "      <td>1944.7028</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>288117008.0</td>\n",
       "      <td>216957.6529</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>2846.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1943.5599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53963</td>\n",
       "      <td>43461</td>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-27 14:57:02</td>\n",
       "      <td>1944.2455</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>288289067.0</td>\n",
       "      <td>217067.0984</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>2847.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1944.7028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53964</td>\n",
       "      <td>43461</td>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-27 15:00:01</td>\n",
       "      <td>1943.8656</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>292844255.0</td>\n",
       "      <td>219837.7588</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>2848.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1944.2455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53965</td>\n",
       "      <td>43461</td>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-27 15:00:06</td>\n",
       "      <td>1945.9439</td>\n",
       "      <td>1987.6867</td>\n",
       "      <td>292903915.0</td>\n",
       "      <td>219934.3371</td>\n",
       "      <td>CSIH11030</td>\n",
       "      <td>2849.0</td>\n",
       "      <td>201812</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>1943.8656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2850 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       intdate  minute  morning                time      close  industry_open  \\\n",
       "51116    43461     540        1 2018-12-27 08:59:59  1967.3388      1987.6867   \n",
       "51117    43461     566        1 2018-12-27 09:25:04  1987.6867      1987.6867   \n",
       "51118    43461     571        1 2018-12-27 09:30:03  1990.5268      1987.6867   \n",
       "51119    43461     571        1 2018-12-27 09:30:08  1989.6967      1987.6867   \n",
       "51120    43461     571        1 2018-12-27 09:30:13  1991.0568      1987.6867   \n",
       "...        ...     ...      ...                 ...        ...            ...   \n",
       "53961    43461     897        0 2018-12-27 14:56:51  1943.5599      1987.6867   \n",
       "53962    43461     897        0 2018-12-27 14:56:56  1944.7028      1987.6867   \n",
       "53963    43461     898        0 2018-12-27 14:57:02  1944.2455      1987.6867   \n",
       "53964    43461     901        0 2018-12-27 15:00:01  1943.8656      1987.6867   \n",
       "53965    43461     901        0 2018-12-27 15:00:06  1945.9439      1987.6867   \n",
       "\n",
       "        cum_volume   cum_amount         ID  ordering   month        date  \\\n",
       "51116          0.0       0.0000  CSIH11030       0.0  201812  2018-12-27   \n",
       "51117    3234187.0    2502.0035  CSIH11030       1.0  201812  2018-12-27   \n",
       "51118    5975050.0    4547.6110  CSIH11030       2.0  201812  2018-12-27   \n",
       "51119    7303493.0    5588.1182  CSIH11030       3.0  201812  2018-12-27   \n",
       "51120    7753194.0    5871.2612  CSIH11030       4.0  201812  2018-12-27   \n",
       "...            ...          ...        ...       ...     ...         ...   \n",
       "53961  287853119.0  216778.8715  CSIH11030    2845.0  201812  2018-12-27   \n",
       "53962  288117008.0  216957.6529  CSIH11030    2846.0  201812  2018-12-27   \n",
       "53963  288289067.0  217067.0984  CSIH11030    2847.0  201812  2018-12-27   \n",
       "53964  292844255.0  219837.7588  CSIH11030    2848.0  201812  2018-12-27   \n",
       "53965  292903915.0  219934.3371  CSIH11030    2849.0  201812  2018-12-27   \n",
       "\n",
       "          close1  \n",
       "51116  1967.3388  \n",
       "51117  1967.3388  \n",
       "51118  1987.6867  \n",
       "51119  1990.5268  \n",
       "51120  1989.6967  \n",
       "...          ...  \n",
       "53961  1943.7983  \n",
       "53962  1943.5599  \n",
       "53963  1944.7028  \n",
       "53964  1944.2455  \n",
       "53965  1943.8656  \n",
       "\n",
       "[2850 rows x 13 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[(k['date'] == '2018-12-27')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymongo \n",
    "import io \n",
    "import pandas as pd \n",
    "import pickle \n",
    "import datetime \n",
    "import time \n",
    "import gzip \n",
    "import lzma \n",
    "import pytz \n",
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq \n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db', version=3): \n",
    "        self.db_name = db_name \n",
    "        self.uri = uri \n",
    "        self.client = pymongo.MongoClient(self.uri) \n",
    "        self.db = self.client[self.db_name] \n",
    "        self.chunk_size = 20000 \n",
    "        self.symbol_column = symbol_column \n",
    "        self.date_column = 'date' \n",
    "        self.version = version\n",
    "\n",
    "    def parse_uri(self, uri): \n",
    "        # mongodb://user:password@example.com \n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"date must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid date type: \" + str(type(x)))\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "        return query\n",
    "\n",
    "    def read_tick(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name] \n",
    "        query = self.build_query(start_date, end_date, symbol) \n",
    "        if not query: \n",
    "            print('cannot read the whole table') \n",
    "            return None  \n",
    "        segs = [] \n",
    "        for x in collection.find(query): \n",
    "            x['data'] = self.deser(x['data'], x['ver']) \n",
    "            segs.append(x) \n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start'])) \n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def read_daily(self, table_name, start_date=None, end_date=None, skey=None, index_id=None, interval=None, index_name=None, col=None, return_sdi=True): \n",
    "        collection = self.db[table_name]\n",
    "        # Build projection \n",
    "        prj = {'_id': 0} \n",
    "        if col is not None: \n",
    "            if return_sdi: \n",
    "                col = ['skey', 'date', 'index_id'] + col \n",
    "            for col_name in col: \n",
    "                prj[col_name] = 1 \n",
    "        # Build query \n",
    "        query = {} \n",
    "        if skey is not None: \n",
    "            query['skey'] = {'$in': skey} \n",
    "        if interval is not None: \n",
    "            query['interval'] = {'$in': interval} \n",
    "        if index_id is not None: \n",
    "            query['index_id'] = {'$in': index_id}    \n",
    "        if index_name is not None:\n",
    "            n = '' \n",
    "            for name in index_name: \n",
    "                try: \n",
    "                    name = re.compile('[\\u4e00-\\u9fff]+').findall(name)[0] \n",
    "                    if len(n) == 0: \n",
    "                        n = n = \"|\".join(name) \n",
    "                    else: \n",
    "                        n = n + '|' + \"|\".join(name) \n",
    "                except: \n",
    "                    if len(n) == 0: \n",
    "                        n = name \n",
    "                    else: \n",
    "                        n = n + '|' + name \n",
    "            query['index_name'] = {'$regex': n}\n",
    "        if start_date is not None: \n",
    "            if end_date is not None: \n",
    "                query['date'] = {'$gte': start_date, '$lte': end_date} \n",
    "            else: \n",
    "                query['date'] = {'$gte': start_date} \n",
    "        elif end_date is not None: \n",
    "            query['date'] = {'$lte': end_date} \n",
    "        # Load data \n",
    "        cur = collection.find(query, prj) \n",
    "        df = pd.DataFrame.from_records(cur) \n",
    "        if df.empty: \n",
    "            df = pd.DataFrame() \n",
    "        else:\n",
    "            if 'index_id' in df.columns:\n",
    "                df = df.sort_values(by=['date', 'index_id', 'skey']).reset_index(drop=True)\n",
    "            else:\n",
    "                df = df.sort_values(by=['date','skey']).reset_index(drop=True)\n",
    "        return df \n",
    " \n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = self.version\n",
    "            ser_data = self.ser(df_seg, version)\n",
    "            seg = {'ver': version, 'data': ser_data, 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        elif version == 3:\n",
    "            # 32-bit number needs more space than 64-bit for parquet\n",
    "            for col_name in s.columns:\n",
    "                col = s[col_name]\n",
    "                if col.dtype == np.int32:\n",
    "                    s[col_name] = s[col_name].astype(np.int64)\n",
    "                elif col.dtype == np.uint32:\n",
    "                    s[col_name] = s[col_name].astype(np.uint64)\n",
    "            tbl = pa.Table.from_pandas(s)\n",
    "            f = io.BytesIO()\n",
    "            pq.write_table(tbl, f, use_dictionary=False, compression='ZSTD', compression_level=0)\n",
    "            f.seek(0)\n",
    "            data = f.read()\n",
    "            return data\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        elif version == 3:\n",
    "            f = io.BytesIO()\n",
    "            f.write(s)\n",
    "            f.seek(0)\n",
    "            return pq.read_table(f, use_threads=False).to_pandas()\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import numpy as np\n",
    "# import glob\n",
    "# import pickle\n",
    "# import os\n",
    "# import datetime\n",
    "# import time\n",
    "# import TSLPy3\n",
    "\n",
    "\n",
    "# ul = pd.read_csv(r'D:\\work\\project 17 AMAC\\tickStockList_AMAC.csv')\n",
    "# ul = ul['StockID'].values\n",
    "\n",
    "# startDate = '20201208T'\n",
    "# endDate = '20201208T'\n",
    "# for num in range(len(ul)):\n",
    "#     stock=ul[num]\n",
    "#     tickname = 'Tick_'+ stock\n",
    "#     if num%10 == 0: print('Processing ' + str(num) +' AMAC '+stock)\n",
    "#     tsstr=\"\"\"\n",
    "#            BegT :=%s;\n",
    "#            EndT :=%s + 0.99;\n",
    "#            setSysParam(pn_stock(),'%s');\n",
    "#            returnData := select ['date'],['close'],['sectional_open'],['sectional_vol'],['sectional_amount']\n",
    "#                          from tradetable datekey BegT to EndT of DefaultStockID() end;\n",
    "#            return returnData;\n",
    "#            \"\"\"%(startDate,endDate,stock)\n",
    "#     Tick_Stock = pd.DataFrame(TSLPy3.RemoteExecute(tsstr,{})[1])\n",
    "#     Tick_Stock.columns = list(pd.Series(Tick_Stock.columns).str.decode('GBK'))\n",
    "#     Tick_Stock['intdate'] = Tick_Stock.date.astype(int)\n",
    "#     Tick_Stock['time'] = Tick_Stock.date.map(lambda x: datetime.datetime.utcfromtimestamp(round((x - 25569) * 86400.0)))\n",
    "#     Tick_Stock['adjTime'] = Tick_Stock.date.map(lambda x: datetime.datetime.utcfromtimestamp(round((x - 25569) * 86400.0) - 1))\n",
    "#     Tick_Stock['minute'] = Tick_Stock.adjTime.map(lambda x: (x.hour*60 + x.minute + 1))\n",
    "#     assert (Tick_Stock.minute.max() >= 900) & (Tick_Stock.minute.min() <= 570)\n",
    "#     Tick_Stock['morning'] = np.where(Tick_Stock.minute <= 690, 1, 0)          \n",
    "#     Tick_Stock.rename(columns = {'sectional_open':'industry_open','sectional_vol':'cum_volume','sectional_amount':'cum_amount'}, inplace=True)            \n",
    "#     Tick_Stock = Tick_Stock[['intdate','minute','morning','time','close','industry_open','cum_volume','cum_amount']].reset_index(drop = True)\n",
    "#     Tick_Stock['ID'] = stock\n",
    "#     ## ordering per day per stock\n",
    "#     for intD in Tick_Stock.intdate.unique():\n",
    "#         Tick_Stock.loc[Tick_Stock.intdate == intD, 'ordering'] = range(0, len(Tick_Stock.loc[Tick_Stock.intdate == intD, 'ID']))\n",
    "#     Tick_Stock['month'] = Tick_Stock.time.dt.month + Tick_Stock.time.dt.year * 100\n",
    "#     test = Tick_Stock\n",
    "\n",
    "#     test['date'] = test['time'].astype(str).apply(lambda x: int(x.split(' ')[0].replace(\"-\", \"\")))\n",
    "#     test['time'] = test['time'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(\":\", \"\")))\n",
    "#     test['datetime'] = test['date'] * 1000000 + test['time']\n",
    "#     test[\"clockAtArrival\"] = test[\"datetime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "#     test['datetime'] = test[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "#     test['time'] = test['time'].astype('int64') * 1000000\n",
    "#     test['skey'] = test['ID'].str[4:].astype(int) + 3000000\n",
    "#     test = test.rename(columns={'industry_open':\"open\"})\n",
    "#     test['open1'] = test.groupby(['skey', 'date'])['open'].transform('max')\n",
    "#     test = test.sort_values(by=['skey', 'date', 'ordering'])\n",
    "#     test['close1'] = test.groupby(['skey'])['close'].shift(1)\n",
    "#     # 每天只有一条tick cum_volume==0, 且当时open==today's open, close==yesterday's close\n",
    "#     # 20180215, 20180220, 20180221不满足条件，这几天无交易，close=0；20180215, 20180222 close close1也无法对上\n",
    "#     assert(sum(test[test['cum_volume'] == 0].groupby(['skey', 'date'])['ordering'].size() != 1) == 0)\n",
    "#     assert(sum(test[test['cum_volume'] == 0].groupby(['skey', 'date'])['ordering'].unique() != 0) == 0)\n",
    "#     try:\n",
    "#         assert((test[test['cum_volume'] == 0]['open'].min() > 0) & (test[test['open'] != test['open1']].shape[0] == 0))\n",
    "#         assert(sum(test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close'] != \n",
    "#         test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close1']) == 0)\n",
    "#     except:\n",
    "#         print(test[(test['cum_volume'] == 0) & (test['open'] == 0)]['datetime'].unique())\n",
    "#         print(test[(test['cum_volume'] == 0) & (~test['close1'].isnull())][test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close'] != \n",
    "#         test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close1']]['datetime'].unique())\n",
    "#     test = test[test['cum_volume'] != 0]\n",
    "#     test = test.sort_values(by=['skey', 'date', 'ordering'])\n",
    "#     test = test[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"cum_volume\", \"cum_amount\", \n",
    "#                  \"open\", \"close\"]]\n",
    "    \n",
    "#     test['td'] = (test['datetime'] - test['datetime'].shift(1)).apply(lambda x: x.seconds).fillna(0).astype('int64')\n",
    "#     test['up'] = test[test['time'] >= 130000000000]['time'].min()\n",
    "#     assert(test[(test['time'] > 93000000000) & (test['time'].shift(1) > 93000000000) & (test['time'] < 150000000000)\\\n",
    "#         & (test['time'] != test['up'])]['td'].max() < 40)\n",
    "#     print(test[(test['time'] > 93000000000) & (test['time'].shift(1) > 93000000000) & (test['time'] < 150000000000)\\\n",
    "#         & (test['time'] != test['up'])]['td'].unique())\n",
    "#     print(test[(test['time'] > 93000000000) & (test['time'].shift(1) > 93000000000) & (test['time'] < 150000000000)\\\n",
    "#         & (test['time'] != test['up']) & (test['td'] == 30)])\n",
    "#     # change to second level tick data\n",
    "#     k1 = test.groupby(['date', 'skey'])['datetime'].min().reset_index()\n",
    "#     k1 = k1.rename(columns={'datetime':'min'})\n",
    "#     k2 = test.groupby(['date', 'skey'])['datetime'].max().reset_index()\n",
    "#     k2 = k2.rename(columns={'datetime':'max'})\n",
    "#     k = pd.merge(k1, k2, on=['date', 'skey'])\n",
    "#     k['diff'] = (k['max']-k['min']).apply(lambda x: x.seconds)\n",
    "\n",
    "#     df = pd.DataFrame()\n",
    "#     for i in np.arange(k.shape[0]):\n",
    "#         df1 = pd.DataFrame()\n",
    "#         df1['datetime1'] = [k.loc[i, 'min'] + datetime.timedelta(seconds=int(x)) for x in np.arange(0, k.loc[i, 'diff'] + 1)]\n",
    "#         df1['skey'] = k.loc[i, 'skey']\n",
    "#         df1['date'] = k.loc[i, 'date']\n",
    "#         assert(df1['datetime1'].min() == k.loc[i, 'min'])\n",
    "#         assert(df1['datetime1'].max() == k.loc[i, 'max'])\n",
    "#         df = pd.concat([df, df1])\n",
    "#     test = pd.merge(test, df, left_on=['skey', 'datetime', 'date'], right_on=['skey', 'datetime1', 'date'], how='outer').sort_values(by=['skey', 'date', 'datetime1']).reset_index(drop=True)\n",
    "#     assert(test[test['datetime1'].isnull()].shape[0] == 0)\n",
    "#     for cols in ['cum_volume', 'cum_amount', 'open', 'close']:\n",
    "#         test[cols] = test.groupby(['skey', 'date'])[cols].ffill()\n",
    "#     test.drop([\"datetime\"],axis=1,inplace=True)\n",
    "#     test = test.rename(columns={'datetime1':'datetime'})\n",
    "#     test['skey'] = test['skey'].astype('int32')\n",
    "#     test[\"time\"] = test['datetime'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(':', \"\"))).astype(np.int64)\n",
    "#     test['SendingTime'] = test['date'] * 1000000 + test['time']\n",
    "#     test[\"clockAtArrival\"] = test[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "#     test.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "#     test['time'] = test['time'] * 1000000\n",
    "\n",
    "#     assert(sum(test[test[\"open\"] != 0].groupby([\"skey\", 'date'])[\"open\"].nunique() != 1) == 0)\n",
    "#     test[\"open\"] = np.where(test[\"cum_volume\"] > 0, test.groupby([\"skey\", 'date'])[\"open\"].transform(\"max\"), test[\"open\"])\n",
    "#     assert(sum(test[test[\"open\"] != 0].groupby([\"skey\", 'date'])[\"open\"].nunique() != 1) == 0)\n",
    "#     assert(test[test[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "\n",
    "#     test['date'] = test['date'].astype('int32')\n",
    "#     test['cum_volume'] = test['cum_volume'].astype('int64')\n",
    "\n",
    "#     m_in = test[test['time'] <= 113500000000].groupby('skey').last()['time'].min()\n",
    "#     m_ax = test[test['time'] >= 125500000000].groupby('skey').first()['time'].max()\n",
    "#     assert(test[(test['time'] >= m_in) & (test['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open',  \n",
    "#                                                'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0\n",
    "#            & (sum(test[(test['time'] >= m_in) & (test['time'] <= m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "#            (sum(test[(test['time'] >= m_in) & (test['time'] <= m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "#     test = pd.concat([test[test['time'] <= 113500000000], test[test['time'] >= 125500000000]])\n",
    "\n",
    "#     test = test.sort_values(by=[\"skey\", 'date', 'time'])\n",
    "#     test[\"ordering\"] = test.groupby([\"skey\", 'date']).cumcount() + 1\n",
    "#     test['ordering'] = test['ordering'].astype('int32')\n",
    "\n",
    "#     for cols in ['open', 'cum_amount', 'close']:\n",
    "#         test[cols] = test[cols].apply(lambda x: round(x, 4)).astype('float64')\n",
    "#     assert(test['time'].max() < 150500000000)\n",
    "\n",
    "#     test = test[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"cum_volume\", \"cum_amount\", \n",
    "#              \"open\", \"close\"]]\n",
    "\n",
    "#     print(\"index finished\")\n",
    "\n",
    "#     database_name = 'com_md_eq_cn'\n",
    "#     user = \"zhenyuy\"\n",
    "#     password = \"bnONBrzSMGoE\"\n",
    "\n",
    "#     db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "#     db1.write('md_index', test)\n",
    "\n",
    "#     del test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "data = db1.read_tick('md_index', 20201223)\n",
    "db1.write('md_index', data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
