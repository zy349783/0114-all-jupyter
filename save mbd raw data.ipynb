{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200106\n",
      "{2000750}\n",
      "20200107\n",
      "{2000750}\n",
      "20200108\n",
      "{2000750}\n",
      "20200109\n",
      "{2000750}\n",
      "20200110\n",
      "{2000750}\n",
      "{1600150}\n",
      "20200113\n",
      "{2000750}\n",
      "20200114\n",
      "20200115\n",
      "20200116\n",
      "{2002266}\n",
      "20200117\n",
      "20200120\n",
      "20200121\n",
      "20200122\n",
      "20200123\n",
      "20200203\n",
      "20200204\n",
      "20200205\n",
      "20200206\n",
      "20200207\n",
      "20200210\n",
      "20200211\n",
      "20200212\n",
      "20200213\n",
      "20200214\n",
      "20200217\n",
      "20200218\n",
      "20200219\n",
      "20200220\n",
      "20200221\n",
      "20200224\n",
      "20200225\n",
      "20200226\n",
      "20200227\n",
      "20200228\n",
      "20200302\n",
      "20200303\n",
      "20200304\n",
      "20200305\n",
      "20200306\n",
      "{2300070}\n",
      "20200309\n",
      "{2300070}\n",
      "{1600167}\n",
      "20200310\n",
      "{2300070}\n",
      "{1600167}\n",
      "20200311\n",
      "{2300070}\n",
      "{1601162}\n",
      "20200312\n",
      "{1600745, 1601162, 1601127}\n",
      "20200313\n",
      "{1600745, 1601162, 1601555}\n",
      "20200316\n",
      "{1600745, 1601162, 1601555}\n",
      "20200317\n",
      "{1600745, 1601162, 1601555}\n",
      "20200318\n",
      "{1600745, 1601162, 1601555}\n",
      "20200319\n",
      "{1600745, 1601555, 1600158}\n",
      "20200320\n",
      "{1600745, 1601555}\n",
      "20200323\n",
      "{1600745, 1600079}\n",
      "20200324\n",
      "{1600816, 1600745, 1600079}\n",
      "20200325\n",
      "{1600816, 1600745, 1600079}\n",
      "20200326\n",
      "{1600816, 1601231, 1600079}\n",
      "20200327\n",
      "{1600816, 1600079}\n",
      "20200330\n",
      "{1600816, 1600079}\n",
      "20200331\n",
      "{1600816, 1600079}\n",
      "20200401\n",
      "{1600816, 1600079}\n",
      "20200402\n",
      "{1600816, 1600079}\n",
      "20200403\n",
      "{1600816}\n",
      "20200407\n",
      "{1600816, 1600039}\n",
      "20200408\n",
      "{1600816}\n",
      "20200409\n",
      "{1600816}\n",
      "20200410\n",
      "{1600816}\n",
      "20200413\n",
      "{2300197}\n",
      "{1600816}\n",
      "20200414\n",
      "{2300197}\n",
      "{1600816}\n",
      "20200415\n",
      "{2300197, 2000727}\n",
      "{1600816}\n",
      "20200416\n",
      "{2300197}\n",
      "{1600816}\n",
      "20200417\n",
      "{2300197}\n",
      "{1600816}\n",
      "20200420\n",
      "{1600816}\n",
      "20200421\n",
      "{1600816}\n",
      "20200422\n",
      "{1600816}\n",
      "20200423\n",
      "{2300015}\n",
      "{1600816}\n",
      "20200424\n",
      "{1600816}\n",
      "20200427\n",
      "{1600816}\n",
      "20200428\n",
      "{2000536, 2000732}\n",
      "{1600816}\n",
      "20200429\n",
      "{2002176, 2002426, 2000732}\n",
      "{1600816}\n",
      "20200430\n",
      "{2002681, 2000732}\n",
      "{1600816}\n",
      "20200506\n",
      "{2000732}\n",
      "{1600816, 1600729}\n",
      "20200507\n",
      "{2000732}\n",
      "{1600816}\n",
      "20200508\n",
      "{2000732}\n",
      "{1600816}\n",
      "20200511\n",
      "{2000732}\n",
      "{1600816, 1600893}\n",
      "20200512\n",
      "{2000732}\n",
      "{1600816}\n",
      "20200513\n",
      "{2000732}\n",
      "{1600816}\n",
      "20200514\n",
      "{1600816}\n",
      "20200515\n",
      "{1600816}\n",
      "20200518\n",
      "{1600816}\n",
      "20200519\n",
      "{1600816}\n",
      "20200520\n",
      "{2002074}\n",
      "{1600816}\n",
      "20200521\n",
      "{2002074}\n",
      "{1600816}\n",
      "20200522\n",
      "{2002074}\n",
      "{1600816}\n",
      "20200525\n",
      "{2002074}\n",
      "{1600816}\n",
      "20200526\n",
      "{2002074}\n",
      "{1600816}\n",
      "20200527\n",
      "{2002074}\n",
      "{1600816, 1600037}\n",
      "20200528\n",
      "{2002074}\n",
      "{1600816, 1600037}\n",
      "20200529\n",
      "{1600816, 1600037}\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "year = \"2020\"\n",
    "startDate = '20200102'\n",
    "endDate = '20200529'\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "mdOrderLog = db1.read('md_order', start_date=startDate, end_date=endDate, symbol=[2000001])\n",
    "datelist = mdOrderLog['date'].unique()\n",
    "import pickle\n",
    "import pandas as pd\n",
    "data1 = pd.read_pickle(r'A:\\index\\weight_table_IC.pkl')\n",
    "data1['date'] = (data1['date'].astype(str).str[:4] + data1['date'].astype(str).str[5:7] + data1['date'].astype(str).str[8:10]).astype(int)\n",
    "data1['skey'] = np.where(data1['ID'].str[:2] == 'SH', data1['ID'].str[2:].astype(int) + 1000000, data1['ID'].str[2:].astype(int) + 2000000)\n",
    "data2 = pd.read_pickle(r'A:\\index\\weight_table_IF.pkl')\n",
    "data2['date'] = (data2['date'].astype(str).str[:4] + data2['date'].astype(str).str[5:7] + data2['date'].astype(str).str[8:10]).astype(int)\n",
    "data2['skey'] = np.where(data2['ID'].str[:2] == 'SH', data2['ID'].str[2:].astype(int) + 1000000, data2['ID'].str[2:].astype(int) + 2000000)\n",
    "\n",
    "for d in datelist[2:]:\n",
    "    print(d)\n",
    "    sl = list(set(data1[data1['date'] == d]['skey'].unique()) | set(data2[data2['date'] == d]['skey'].unique()))\n",
    "    assert(len(sl) == 800)\n",
    "    sl1 = [i for i in sl if i >= 2000000]\n",
    "    sl2 = [i for i in sl if i < 2000000]\n",
    "    assert(len(sl1) + len(sl2) == 800)\n",
    "    order = db1.read('md_order', start_date=str(d), end_date=str(d), symbol=sl1)\n",
    "    assert(order[order['time'] < 0].shape[0] == 0)\n",
    "    try:\n",
    "        assert(order['skey'].nunique() == len(sl1))\n",
    "    except:\n",
    "        print(set(sl1) - set(order['skey'].unique()))\n",
    "    trade = db1.read('md_trade', start_date=str(d), end_date=str(d), symbol=sl)\n",
    "    assert(trade[trade['time'] < 0].shape[0] == 0)\n",
    "    snapshot = db1.read('md_snapshot_l2', start_date=str(d), end_date=str(d), symbol=sl2)\n",
    "    assert(snapshot[snapshot['time'] < 0].shape[0] == 0)\n",
    "    try:\n",
    "        assert(snapshot['skey'].nunique() == len(sl2))\n",
    "    except:\n",
    "        print(set(sl2) - set(snapshot['skey'].unique()))\n",
    "    os.mkdir('M:\\\\2020 raw data (IC&IF)\\\\' + str(d))\n",
    "    order.to_pickle('M:\\\\2020 raw data (IC&IF)\\\\' + str(d) + '\\\\order.pkl')\n",
    "    trade.to_pickle('M:\\\\2020 raw data (IC&IF)\\\\' + str(d) + '\\\\trade.pkl')\n",
    "    snapshot.to_pickle('M:\\\\2020 raw data (IC&IF)\\\\' + str(d) + '\\\\snapshot.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
