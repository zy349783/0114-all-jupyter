{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652 different ticks compared with Tianruan\n",
      "289 ticks no auction:\n",
      "  148 days\n",
      "  201 stocks\n",
      "352 ticks duplicated in Tianruan:\n",
      "  243 days\n",
      "['SZ001872' 'SZ001914']\n",
      "all data same as Tianruan\n",
      "0:05:51.038663\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "# deal with duplicates in 深交所数据\n",
    "startTm = datetime.datetime.now()\n",
    "F2 = open('F:\\\\SZ2018.pkl', 'rb')\n",
    "SZ = pickle.load(F2)\n",
    "p1 = SZ[SZ[\"date\"].isin(['2017-06-20', \"2017-06-22\"])].groupby([\"ID\", \"date\"]).first().reset_index()\n",
    "p2 = SZ[SZ[\"date\"].isin(['2017-06-20', \"2017-06-22\"])].groupby([\"ID\", \"date\"]).last().reset_index()\n",
    "p3 = SZ[~SZ[\"date\"].isin(['2017-06-20', \"2017-06-22\"])]\n",
    "SZ = pd.concat([p1, p3])\n",
    "\n",
    "readPath = r'\\\\192.168.10.30\\Kevin_zhenyu\\day_stock_20200416\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = dataPathLs[[np.array([os.path.basename(i).split('.')[0][:2] == 'SZ' for i in dataPathLs])]]\n",
    "db = pd.DataFrame()\n",
    "for p in dataPathLs:\n",
    "    dayData = pd.read_csv(p, compression='gzip')\n",
    "    db = pd.concat([db, dayData])\n",
    "date = SZ[\"date\"].unique()\n",
    "date1 = db[\"date\"].unique()\n",
    "date1 = date1[(date1>=date.min()) & (date1 <= date.max())]\n",
    "display(set(date1) - set(date))\n",
    "db = db[db[\"date\"].isin(date)]\n",
    "\n",
    "# 注意\n",
    "# 1. diff里包括所有的不匹配，可能是close price造成的，需要在下一步验证\n",
    "# 2. 第二次merge仍有不匹配，继续查找原因\n",
    "# 3. 天软数据包括当天停牌的股票，openPrice == 0\n",
    "# 4. 实际的收盘价调整是两次merge相减的结果\n",
    "# 5. 深交所数据那天会重复\n",
    "# 6. SZ包括当天停牌的股票，SH应该不包括，再确认\n",
    "df = pd.merge(SZ, db, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_close\", \"d_volume\"], how=\"outer\")\n",
    "if sum(df[\"d_amount_x\"].isnull()) != 0:\n",
    "    diff = df[df[\"d_amount_x\"].isnull()][[\"ID\", \"date\"]]\n",
    "    re = pd.merge(SZ, diff, how=\"right\")\n",
    "    print(\"%d different ticks compared with Tianruan\" % re.shape[0])\n",
    "    if sum(re[\"auction\"] == 1) == 0:\n",
    "        print(\"%d ticks no auction:\" % re[re[\"auction\"] == 0].shape[0])\n",
    "        print(\"  %d days\" % len(re[re[\"auction\"] == 0][\"date\"].unique()))\n",
    "        print(\"  %d stocks\" % len(re[re[\"auction\"] == 0][\"ID\"].unique()))\n",
    "        if re[re[\"auction\"].isnull()].shape[0] == 0:\n",
    "            df1 = pd.merge(SZ, db, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "            if sum(df1[\"d_amount_x\"].isnull()) == 0:\n",
    "                print(\"all data same as Tianruan\")\n",
    "            else:\n",
    "                print(df1[df1[\"d_amount_x\"].isnull()])\n",
    "        else:\n",
    "            re1 = re[re[\"auction\"].isnull()]\n",
    "            dup = db[db.duplicated(keep=False, subset=[\"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_close\", \"d_volume\"])]\n",
    "            dup1 = pd.merge(dup, re1[[\"ID\", \"date\"]], on=[\"ID\", \"date\"], how=\"outer\")\n",
    "            re2 = re[~re[\"auction\"].isnull()]\n",
    "            db1 = pd.merge(db, re2[[\"ID\", \"date\"]], on=[\"ID\", \"date\"])\n",
    "            df1 = pd.merge(SZ, db1, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "            if (dup1.shape[0] == dup.shape[0]) & (sum(df1[\"d_amount_x\"].isnull()) == 0):\n",
    "                print(\"%d ticks duplicated in Tianruan:\" % re1.shape[0])\n",
    "                print(\"  %d days\" % len(re1[\"date\"].unique()))\n",
    "                print(re1[\"ID\"].unique())\n",
    "                print(\"all data same as Tianruan\")\n",
    "            else:\n",
    "                print(\"%d ticks missing in HD\" % dup1[dup1[\"d_open\"].isnull()].shape[0])\n",
    "                print(dup1[dup1[\"d_open\"].isnull()][[\"ID\", \"date\"]])\n",
    "                print(df1[df1[\"d_amount_x\"].isnull()])\n",
    "    else:\n",
    "        print(\"%d ticks no auction:\" % re[re[\"auction\"] == 0].shape[0])\n",
    "        print(\"  %d days\" % len(re[re[\"auction\"] == 0][\"date\"].unique()))\n",
    "        print(\"  %d stocks\" % len(re[re[\"auction\"] == 0][\"ID\"].unique()))\n",
    "        if re[re[\"auction\"].isnull()].shape[0] == 0:\n",
    "            print(\"all data same as Tianruan\")\n",
    "        else:\n",
    "            re1 = re[re[\"auction\"].isnull()]\n",
    "            dup = db[db.duplicated(keep=False, subset=[\"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_close\", \"d_volume\"])]\n",
    "            dup1 = pd.merge(dup, re1[[\"ID\", \"date\"]], on=[\"ID\", \"date\"], how=\"outer\")\n",
    "            if dup1.shape[0] == dup.shape[0]:\n",
    "                print(\"%d ticks duplicated in Tianruan:\" % re1.shape[0])\n",
    "                print(\"  %d days\" % len(re1[\"date\"].unique()))\n",
    "                print(re1[\"ID\"].unique())\n",
    "                print(\"all data same as Tianruan\")\n",
    "            else:\n",
    "                print(\"%d ticks missing in HD\" % dup1[dup1[\"d_open\"].isnull()].shape[0])\n",
    "                print(dup1[dup1[\"d_open\"].isnull()][[\"ID\", \"date\"]])  \n",
    "            \n",
    "        SZ1 = pd.merge(SZ, re[re[\"auction\"] == 1][[\"ID\", \"date\"]], how=\"right\")\n",
    "        db1 = pd.merge(db, re[re[\"auction\"] == 1][[\"ID\", \"date\"]], how=\"left\")\n",
    "        df = pd.merge(SZ1, db1, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_close\"], how=\"outer\")\n",
    "        if sum(df[\"d_amount_x\"].isnull()) == 0:\n",
    "            print(\"%d ticks have different cum_volume\" % re[re[\"auction\"] == 1].shape[0])\n",
    "print(datetime.datetime.now() - startTm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>SZ300125</td>\n",
       "      <td>2018-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>SZ300632</td>\n",
       "      <td>2018-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>SZ300553</td>\n",
       "      <td>2018-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>SZ300442</td>\n",
       "      <td>2018-02-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>SZ300108</td>\n",
       "      <td>2018-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>SZ300286</td>\n",
       "      <td>2018-06-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>SZ000753</td>\n",
       "      <td>2018-08-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>SZ300530</td>\n",
       "      <td>2018-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>SZ300575</td>\n",
       "      <td>2018-09-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>SZ300516</td>\n",
       "      <td>2018-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>SZ300330</td>\n",
       "      <td>2018-10-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID        date\n",
       "10   SZ300125  2018-01-12\n",
       "19   SZ300632  2018-01-25\n",
       "35   SZ300553  2018-02-07\n",
       "41   SZ300442  2018-02-13\n",
       "57   SZ300108  2018-04-16\n",
       "91   SZ300286  2018-06-27\n",
       "147  SZ000753  2018-08-28\n",
       "188  SZ300530  2018-09-19\n",
       "189  SZ300575  2018-09-19\n",
       "219  SZ300516  2018-10-15\n",
       "228  SZ300330  2018-10-18"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re[re[\"auction\"] == 1][[\"ID\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>d_open</th>\n",
       "      <th>d_high</th>\n",
       "      <th>d_low</th>\n",
       "      <th>d_close</th>\n",
       "      <th>d_yclose</th>\n",
       "      <th>d_cumprodCAA</th>\n",
       "      <th>d_dayReturn</th>\n",
       "      <th>d_5dayReturn</th>\n",
       "      <th>...</th>\n",
       "      <th>allZT</th>\n",
       "      <th>hasZT</th>\n",
       "      <th>isZT</th>\n",
       "      <th>allDT</th>\n",
       "      <th>hasDT</th>\n",
       "      <th>isDT</th>\n",
       "      <th>tmrHalted</th>\n",
       "      <th>haltedDays</th>\n",
       "      <th>marketShares</th>\n",
       "      <th>totalShares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1328</td>\n",
       "      <td>SZ300125</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>25.14</td>\n",
       "      <td>25.67</td>\n",
       "      <td>25.14</td>\n",
       "      <td>25.62</td>\n",
       "      <td>25.43</td>\n",
       "      <td>0.987198</td>\n",
       "      <td>0.007471</td>\n",
       "      <td>-0.028073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90297460.0</td>\n",
       "      <td>118000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID        date  d_open  d_high  d_low  d_close  d_yclose  \\\n",
       "1328  SZ300125  2018-01-12   25.14   25.67  25.14    25.62     25.43   \n",
       "\n",
       "      d_cumprodCAA  d_dayReturn  d_5dayReturn  ...  allZT  hasZT  isZT  allDT  \\\n",
       "1328      0.987198     0.007471     -0.028073  ...    0.0    0.0   0.0    0.0   \n",
       "\n",
       "      hasDT  isDT  tmrHalted  haltedDays  marketShares  totalShares  \n",
       "1328    0.0   0.0        0.0         0.0    90297460.0  118000000.0  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[(db[\"ID\"] == \"SZ300125\") & (db[\"date\"] == \"2018-01-12\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SZ1 = pd.merge(SZ, re[re[\"auction\"] == 0][[\"ID\", \"date\"]], how=\"right\")\n",
    "db1 = pd.merge(db, re[re[\"auction\"] == 0][[\"ID\", \"date\"]], how=\"right\")\n",
    "df2 = pd.merge(SZ1, db1, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "df2[df2[\"d_amount_x\"].isnull()][\"date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>d_open</th>\n",
       "      <th>d_yclose</th>\n",
       "      <th>d_high</th>\n",
       "      <th>d_low</th>\n",
       "      <th>d_close_x</th>\n",
       "      <th>d_volume</th>\n",
       "      <th>d_amount_x</th>\n",
       "      <th>massive_missing</th>\n",
       "      <th>...</th>\n",
       "      <th>allZT</th>\n",
       "      <th>hasZT</th>\n",
       "      <th>isZT</th>\n",
       "      <th>allDT</th>\n",
       "      <th>hasDT</th>\n",
       "      <th>isDT</th>\n",
       "      <th>tmrHalted</th>\n",
       "      <th>haltedDays</th>\n",
       "      <th>marketShares</th>\n",
       "      <th>totalShares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, date, d_open, d_yclose, d_high, d_low, d_close_x, d_volume, d_amount_x, massive_missing, security_missing, auction, d_close_y, d_cumprodCAA, d_dayReturn, d_5dayReturn, d_ICDayReturn, d_CSIDayReturn, d_amount_y, TORate, allZT, hasZT, isZT, allDT, hasDT, isDT, tmrHalted, haltedDays, marketShares, totalShares]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SZ1 = pd.merge(SZ, re[re[\"auction\"] == 1][[\"ID\", \"date\"]], how=\"right\")\n",
    "db1 = pd.merge(db, re[re[\"auction\"] == 1][[\"ID\", \"date\"]], how=\"right\")\n",
    "df1 = pd.merge(SZ1, db1, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_volume\"], how=\"outer\")\n",
    "df1[df1[\"d_amount_x\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>d_open</th>\n",
       "      <th>d_yclose</th>\n",
       "      <th>d_high_x</th>\n",
       "      <th>d_low_x</th>\n",
       "      <th>d_close_x</th>\n",
       "      <th>d_volume_x</th>\n",
       "      <th>d_amount_x</th>\n",
       "      <th>massive_missing</th>\n",
       "      <th>...</th>\n",
       "      <th>allZT</th>\n",
       "      <th>hasZT</th>\n",
       "      <th>isZT</th>\n",
       "      <th>allDT</th>\n",
       "      <th>hasDT</th>\n",
       "      <th>isDT</th>\n",
       "      <th>tmrHalted</th>\n",
       "      <th>haltedDays</th>\n",
       "      <th>marketShares</th>\n",
       "      <th>totalShares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, date, d_open, d_yclose, d_high_x, d_low_x, d_close_x, d_volume_x, d_amount_x, massive_missing, security_missing, auction, d_high_y, d_low_y, d_close_y, d_cumprodCAA, d_dayReturn, d_5dayReturn, d_ICDayReturn, d_CSIDayReturn, d_volume_y, d_amount_y, TORate, allZT, hasZT, isZT, allDT, hasDT, isDT, tmrHalted, haltedDays, marketShares, totalShares]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 33 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.merge(SZ1, db1, on=[\"ID\", \"date\", \"d_open\", \"d_yclose\"], how=\"outer\")\n",
    "df1[df1[\"d_amount_x\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>d_open</th>\n",
       "      <th>d_yclose</th>\n",
       "      <th>d_high</th>\n",
       "      <th>d_low</th>\n",
       "      <th>d_close</th>\n",
       "      <th>d_volume</th>\n",
       "      <th>d_amount</th>\n",
       "      <th>massive_missing</th>\n",
       "      <th>security_missing</th>\n",
       "      <th>auction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>96292</td>\n",
       "      <td>SZ000584</td>\n",
       "      <td>2016-04-11</td>\n",
       "      <td>12.78</td>\n",
       "      <td>12.6</td>\n",
       "      <td>12.9</td>\n",
       "      <td>12.66</td>\n",
       "      <td>12.72</td>\n",
       "      <td>11974321</td>\n",
       "      <td>1.529006e+08</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID        date  d_open  d_yclose  d_high  d_low  d_close  \\\n",
       "96292  SZ000584  2016-04-11   12.78      12.6    12.9  12.66    12.72   \n",
       "\n",
       "       d_volume      d_amount  massive_missing  security_missing  auction  \n",
       "96292  11974321  1.529006e+08                1                 1        0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>date</th>\n",
       "      <th>d_open</th>\n",
       "      <th>d_yclose</th>\n",
       "      <th>d_high</th>\n",
       "      <th>d_low</th>\n",
       "      <th>d_close</th>\n",
       "      <th>d_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>SZ000584</td>\n",
       "      <td>2016-04-11</td>\n",
       "      <td>12.78</td>\n",
       "      <td>12.6</td>\n",
       "      <td>12.9</td>\n",
       "      <td>12.6</td>\n",
       "      <td>12.63</td>\n",
       "      <td>14222958.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID        date  d_open  d_yclose  d_high  d_low  d_close  \\\n",
       "876  SZ000584  2016-04-11   12.78      12.6    12.9   12.6    12.63   \n",
       "\n",
       "       d_volume  \n",
       "876  14222958.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(SZ[(SZ[\"ID\"] == \"SZ000584\") & (SZ[\"date\"] == \"2016-04-11\")])\n",
    "display(db[(db[\"ID\"] == \"SZ000584\") & (db[\"date\"] == \"2016-04-11\")][[\"ID\", \"date\", \"d_open\", \"d_yclose\",\"d_high\", \"d_low\", \"d_close\", \"d_volume\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orig</th>\n",
       "      <th>time</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19260</td>\n",
       "      <td>2019-06-10 13:51:00</td>\n",
       "      <td>135100000</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19261</td>\n",
       "      <td>2019-06-10 13:51:01</td>\n",
       "      <td>135101000</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19262</td>\n",
       "      <td>2019-06-10 13:51:02</td>\n",
       "      <td>135102000</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19263</td>\n",
       "      <td>2019-06-10 13:51:03</td>\n",
       "      <td>135103000</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19264</td>\n",
       "      <td>2019-06-10 13:51:04</td>\n",
       "      <td>135104000</td>\n",
       "      <td>4503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19675</td>\n",
       "      <td>2019-06-10 13:57:55</td>\n",
       "      <td>135755000</td>\n",
       "      <td>4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19676</td>\n",
       "      <td>2019-06-10 13:57:56</td>\n",
       "      <td>135756000</td>\n",
       "      <td>4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19677</td>\n",
       "      <td>2019-06-10 13:57:57</td>\n",
       "      <td>135757000</td>\n",
       "      <td>4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19678</td>\n",
       "      <td>2019-06-10 13:57:58</td>\n",
       "      <td>135758000</td>\n",
       "      <td>4525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19679</td>\n",
       "      <td>2019-06-10 13:57:59</td>\n",
       "      <td>135759000</td>\n",
       "      <td>4525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Orig       time  group\n",
       "19260 2019-06-10 13:51:00  135100000   4503\n",
       "19261 2019-06-10 13:51:01  135101000   4503\n",
       "19262 2019-06-10 13:51:02  135102000   4503\n",
       "19263 2019-06-10 13:51:03  135103000   4503\n",
       "19264 2019-06-10 13:51:04  135104000   4503\n",
       "...                   ...        ...    ...\n",
       "19675 2019-06-10 13:57:55  135755000   4525\n",
       "19676 2019-06-10 13:57:56  135756000   4525\n",
       "19677 2019-06-10 13:57:57  135757000   4525\n",
       "19678 2019-06-10 13:57:58  135758000   4525\n",
       "19679 2019-06-10 13:57:59  135759000   4525\n",
       "\n",
       "[420 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from unrar import rarfile\n",
    "import py7zr\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "columns1 = [\"Date\",\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",\"PreClosePx\",\n",
    "                   \"PxChnage1\",\"PXChange2\",\"openPrice\",\"HighPx\",\"LowPx\",\"close\",\"NumTrades\",\"cum_volume\",\"cum_amount\",\"PE1\",\"PE2\",\"TradingPhase\",\n",
    "                   \"totalofferqty\", \"wa_offerPrice\", \"totalbidqty\", \"wa_bidPrice\", \"PreNAV\", \"RealTimeNAV\", \"WarrantPremiumRate\", \"UpLimitPx\",\n",
    "                   \"DownLimitPx\", \"TotalLongPosition\", \"unknown1\", \"unknown2\", \"unknown3\"]\n",
    "columns2 = ['Date',\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",'ask1p','bid1p',\n",
    "                   \"ask1q\",\"bid1q\", 'ask2p','bid2p',\"ask2q\",\"bid2q\",'ask3p','bid3p',\"ask3q\",\"bid3q\",'ask4p','bid4p',\"ask4q\",\"bid4q\",'ask5p',\n",
    "                    'bid5p',\"ask5q\",\"bid5q\",'ask6p','bid6p',\"ask6q\",\"bid6q\",'ask7p','bid7p',\"ask7q\",\"bid7q\",'ask8p','bid8p',\"ask8q\",\"bid8q\",\n",
    "                   'ask9p','bid9p',\"ask9q\",\"bid9q\",'ask10p','bid10p',\"ask10q\",\"bid10q\",\"NUMORDERS_B1\",\"NOORDERS_B1\",\"ORDERQTY_B1\",\n",
    "                    \"NUMORDERS_S1\",\"NOORDERS_S1\",\"ORDERQTY_S1\"]\n",
    "columns3 = [\"Date\", \"SENDTIME\", \"SecurityID\", \"DATATIMESTAMP\", \"PreClosePx\", \"openPrice\", \"HighPx\", \"LowPx\", \"close\", \"NUMTRADES\", \"cum_volume\",\n",
    "           \"cum_amount\", \"TOTALLONGPOSITION\", \"PERATIO1\", \"PERATIO2\", \"ENDOFDAYMAKER\", \"TOTALOFFERQTY\", \"WEIGHTEDAVGOFFERPX\", \"TOTALBIDQTY\", \"WEIGHTEDAVGBIDPX\"]\n",
    "columns4 = [\"Date\", \"SENDTIME\", \"SecurityID\", \"DATATIMESTAMP\", \"ask10p\", \"ask9p\", \"ask8p\", \"ask7p\", \"ask6p\", \"ask5p\", \"ask4p\", \"ask3p\", \"ask2p\",\n",
    "           \"ask1p\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid6p\", \"bid7p\", \"bid8p\", \"bid9p\", \"bid10p\", \"ask10q\", \"ask9q\", \"ask8q\", \"ask7q\",\n",
    "           \"ask6q\", \"ask5q\", \"ask4q\", \"ask3q\", \"ask2q\", \"ask1q\", \"bid1q\", \"bid2q\", \"bid3q\", \"bid4q\", \"bid5q\", \"bid6q\", \"bid7q\", \"bid8q\", \"bid9q\", \n",
    "           \"bid10q\", \"NUMORDERS_B1\", \"NOORDERS _B1\", \"ORDERQTY_B1\", \"NUMORDERS_S1\", \"NOORDERS _S1\", \"ORDERQTY_S1\"]\n",
    "columns5 =  [\"Date\",\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",\"PreClosePx\",\n",
    "                   \"PxChnage1\",\"PXChange2\",\"openPrice\",\"HighPx\",\"LowPx\",\"close\",\"NumTrades\",\"cum_volume\",\"cum_amount\",\"PE1\",\"PE2\",\"TradingPhase\",\n",
    "                   \"totalofferqty\", \"wa_offerPrice\", \"totalbidqty\", \"wa_bidPrice\", \"PreNAV\", \"RealTimeNAV\", \"WarrantPremiumRate\", \"UpLimitPx\",\n",
    "                   \"DownLimitPx\", \"TotalLongPosition\"]\n",
    "\n",
    "# snap1 = pd.read_table(\"F:\\\\SZ\\\\2017\\\\0620\\\\am_hq_snap_spot.txt\", header=None)\n",
    "# # snap1 = pd.read_table(\"J:\\\\LEVEL2_shenzhen\\\\2016\\\\05\\\\05\\\\SZL2_SNAPSHOT_20160505_AM.txt\", header=None)\n",
    "# assert(snap1.shape[1] == len(columns5))\n",
    "# snap1.columns = columns5\n",
    "# snap2 = pd.read_table(\"F:\\\\SZ\\\\2017\\\\0620\\\\pm_hq_snap_spot.txt\", header=None)\n",
    "# # snap2 = pd.read_table(\"J:\\\\LEVEL2_shenzhen\\\\2016\\\\05\\\\05\\\\SZL2_SNAPSHOT_20160505_PM.txt\", header=None)\n",
    "# assert(snap2.shape[1] == len(columns5))\n",
    "# snap2.columns = columns5\n",
    "# snapshot = pd.concat([snap1, snap2]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "# # snapshot = pd.concat([snap1, snap2]).sort_values(by=[\"SecurityID\", \"DATATIMESTAMP\"])\n",
    "# del snap1\n",
    "# del snap2\n",
    "# # snapshot[\"SecurityID\"] = snapshot[\"SecurityID\"].astype(int)\n",
    "# # snapshot = snapshot.rename(columns={\"SecurityID\":\"StockID\"})\n",
    "# # snapshot['time'] = snapshot[\"DATATIMESTAMP\"]\n",
    "# # snapshot[\"OrigTime\"] = snapshot[\"Date\"] * 1000000000 + snapshot[\"DATATIMESTAMP\"]\n",
    "# # snapshot = snapshot[snapshot[\"time\"] != 0]\n",
    "# # snapshot = pd.read_table(\"F:\\\\SZ\\\\2017\\\\0620(深交所数据)\\\\hq_snap.txt\", header=None, encoding=\"UTF-8-sig\")\n",
    "# # assert(snapshot.shape[1] == len(columns1))\n",
    "# # snapshot.columns = columns1\n",
    "\n",
    "F1 = open(\"E:\\\\SZ\\\\2019\\\\0610\\\\Snapshot.pkl\", 'rb')\n",
    "snapshot = pickle.load(F1)\n",
    "snapshot[\"StockID\"] = snapshot[\"StockID\"].astype(int)\n",
    "# snapshot = snapshot.rename(columns={\"SecurityID\":\"StockID\"})\n",
    "snapshot['time'] = (snapshot['OrigTime'] - int(snapshot['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "        \n",
    "snapshot = snapshot[(snapshot[\"StockID\"] < 4000) | (snapshot[\"StockID\"] > 300000)]\n",
    "snapshot[\"cum_max\"] = snapshot.groupby(\"StockID\")[\"cum_volume\"].transform(max)\n",
    "\n",
    "date = pd.DataFrame(pd.date_range(start='2019-06-10 08:30:00', end='2019-06-10 18:00:00', freq='s'), columns=[\"Orig\"])\n",
    "date[\"time\"] = date[\"Orig\"].apply(lambda x: int(x.strftime(\"%H%M%S\"))*1000)\n",
    "date[\"group\"] = date[\"time\"]//30000\n",
    "snapshot[\"group\"] = snapshot[\"time\"]//30000\n",
    "gl = date[((date[\"time\"] >= 93000000) & (date[\"time\"] < 113000000))|((date[\"time\"] >= 130000000) & (date[\"time\"] <= 145700000))][\"group\"].unique()\n",
    "date[date[\"group\"].isin(list(set(gl) - set(snapshot[\"group\"].unique())))]\n",
    "\n",
    "# snapshot = snapshot.sort_values(by=[\"StockID\", \"OrigTime\"])    \n",
    "# data2 = snapshot[(snapshot[\"time\"] > 92500000) & (snapshot[\"time\"] <= 145700000) & (snapshot[\"cum_volume\"] > 0)][[\"StockID\", \"cum_volume\",\n",
    "#                                                                                                                      \"OrigTime\", \"time\"]]\n",
    "# data2 = data2.sort_values(by=[\"StockID\", \"OrigTime\"])\n",
    "# data2[\"volume_update\"] = data2.groupby(\"StockID\")[\"cum_volume\"].apply(lambda x: x - x.shift(1))\n",
    "# data2[\"time1\"] = pd.to_datetime(data2[\"OrigTime\"].apply(lambda x: str(x)[:-3]))\n",
    "# data2[\"time_interval\"] = data2.groupby(\"StockID\")[\"time1\"].apply(lambda x: x - x.shift(1))\n",
    "# data2[\"time_interval\"] = data2[\"time_interval\"].apply(lambda x: x.seconds)\n",
    "# data2 = pd.merge(data2, data2[data2[\"volume_update\"] > 0].groupby(\"StockID\")[\"time_interval\"].describe()[\"75%\"].reset_index(), on=\"StockID\")\n",
    "# data2 = data2.rename(columns={\"75%\": \"75%_time\"})\n",
    "# vu = data2[data2[\"volume_update\"] > 0].groupby(\"StockID\")[\"volume_update\"].apply(lambda x: x.describe([0.9])[\"90%\"]).reset_index()\n",
    "# vu = vu.rename(columns={\"volume_update\":\"90%\"})\n",
    "# data2 = pd.merge(data2, vu, on=\"StockID\")\n",
    "# data2[\"90%\"] = data2[\"90%\"].apply(lambda x: max(1000, x))\n",
    "# data2[\"indicator\"] = 0\n",
    "# data2.loc[data2[data2[\"time\"] >= 130000000].groupby(\"StockID\").head(1).index, 'indicator'] = 1\n",
    "# data2.loc[data2[data2[\"time\"] >= 93000000].groupby(\"StockID\").head(1).index, 'indicator'] = 1\n",
    "# data2 = data2[data2[\"indicator\"] != 1]    \n",
    "# data2[(data2[\"time_interval\"] > 180) & (data2[\"volume_update\"] >= data2[\"90%\"]) & (data2[\"75%_time\"] <= 4)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([135003000, 135006000, 135009000, 135012000, 135015000, 135018000,\n",
       "       135021000, 135024000, 135027000, 135030000, 135033000, 135036000,\n",
       "       135039000, 135042000, 135045000, 135048000, 135051000, 135054000,\n",
       "       135057000, 135806000, 135809000, 135812000, 135815000, 135818000,\n",
       "       135821000, 135824000, 135827000, 135830000, 135833000, 135836000,\n",
       "       135839000, 135842000, 135845000, 135848000, 135851000, 135854000,\n",
       "       135857000], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(snapshot[(snapshot[\"time\"] > 135000000) & (snapshot[\"time\"] < 135900000)][\"time\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135100000\n",
      "135101000\n",
      "135102000\n",
      "135103000\n",
      "135104000\n",
      "135105000\n",
      "135106000\n",
      "135107000\n",
      "135108000\n",
      "135109000\n",
      "135110000\n",
      "135111000\n",
      "135112000\n",
      "135113000\n",
      "135114000\n",
      "135115000\n",
      "135116000\n",
      "135117000\n",
      "135118000\n",
      "135119000\n",
      "135120000\n",
      "135121000\n",
      "135122000\n",
      "135123000\n",
      "135124000\n",
      "135125000\n",
      "135126000\n",
      "135127000\n",
      "135128000\n",
      "135129000\n",
      "135130000\n",
      "135131000\n",
      "135132000\n",
      "135133000\n",
      "135134000\n",
      "135135000\n",
      "135136000\n",
      "135137000\n",
      "135138000\n",
      "135139000\n",
      "135140000\n",
      "135141000\n",
      "135142000\n",
      "135143000\n",
      "135144000\n",
      "135145000\n",
      "135146000\n",
      "135147000\n",
      "135148000\n",
      "135149000\n",
      "135150000\n",
      "135151000\n",
      "135152000\n",
      "135153000\n",
      "135154000\n",
      "135155000\n",
      "135156000\n",
      "135157000\n",
      "135158000\n",
      "135159000\n",
      "135200000\n",
      "135201000\n",
      "135202000\n",
      "135203000\n",
      "135204000\n",
      "135205000\n",
      "135206000\n",
      "135207000\n",
      "135208000\n",
      "135209000\n",
      "135210000\n",
      "135211000\n",
      "135212000\n",
      "135213000\n",
      "135214000\n",
      "135215000\n",
      "135216000\n",
      "135217000\n",
      "135218000\n",
      "135219000\n",
      "135220000\n",
      "135221000\n",
      "135222000\n",
      "135223000\n",
      "135224000\n",
      "135225000\n",
      "135226000\n",
      "135227000\n",
      "135228000\n",
      "135229000\n",
      "135230000\n",
      "135231000\n",
      "135232000\n",
      "135233000\n",
      "135234000\n",
      "135235000\n",
      "135236000\n",
      "135237000\n",
      "135238000\n",
      "135239000\n",
      "135240000\n",
      "135241000\n",
      "135242000\n",
      "135243000\n",
      "135244000\n",
      "135245000\n",
      "135246000\n",
      "135247000\n",
      "135248000\n",
      "135249000\n",
      "135250000\n",
      "135251000\n",
      "135252000\n",
      "135253000\n",
      "135254000\n",
      "135255000\n",
      "135256000\n",
      "135257000\n",
      "135258000\n",
      "135259000\n",
      "135300000\n",
      "135301000\n",
      "135302000\n",
      "135303000\n",
      "135304000\n",
      "135305000\n",
      "135306000\n",
      "135307000\n",
      "135308000\n",
      "135309000\n",
      "135310000\n",
      "135311000\n",
      "135312000\n",
      "135313000\n",
      "135314000\n",
      "135315000\n",
      "135316000\n",
      "135317000\n",
      "135318000\n",
      "135319000\n",
      "135320000\n",
      "135321000\n",
      "135322000\n",
      "135323000\n",
      "135324000\n",
      "135325000\n",
      "135326000\n",
      "135327000\n",
      "135328000\n",
      "135329000\n",
      "135330000\n",
      "135331000\n",
      "135332000\n",
      "135333000\n",
      "135334000\n",
      "135335000\n",
      "135336000\n",
      "135337000\n",
      "135338000\n",
      "135339000\n",
      "135340000\n",
      "135341000\n",
      "135342000\n",
      "135343000\n",
      "135344000\n",
      "135345000\n",
      "135346000\n",
      "135347000\n",
      "135348000\n",
      "135349000\n",
      "135350000\n",
      "135351000\n",
      "135352000\n",
      "135353000\n",
      "135354000\n",
      "135355000\n",
      "135356000\n",
      "135357000\n",
      "135358000\n",
      "135359000\n",
      "135400000\n",
      "135401000\n",
      "135402000\n",
      "135403000\n",
      "135404000\n",
      "135405000\n",
      "135406000\n",
      "135407000\n",
      "135408000\n",
      "135409000\n",
      "135410000\n",
      "135411000\n",
      "135412000\n",
      "135413000\n",
      "135414000\n",
      "135415000\n",
      "135416000\n",
      "135417000\n",
      "135418000\n",
      "135419000\n",
      "135420000\n",
      "135421000\n",
      "135422000\n",
      "135423000\n",
      "135424000\n",
      "135425000\n",
      "135426000\n",
      "135427000\n",
      "135428000\n",
      "135429000\n",
      "135430000\n",
      "135431000\n",
      "135432000\n",
      "135433000\n",
      "135434000\n",
      "135435000\n",
      "135436000\n",
      "135437000\n",
      "135438000\n",
      "135439000\n",
      "135440000\n",
      "135441000\n",
      "135442000\n",
      "135443000\n",
      "135444000\n",
      "135445000\n",
      "135446000\n",
      "135447000\n",
      "135448000\n",
      "135449000\n",
      "135450000\n",
      "135451000\n",
      "135452000\n",
      "135453000\n",
      "135454000\n",
      "135455000\n",
      "135456000\n",
      "135457000\n",
      "135458000\n",
      "135459000\n",
      "135500000\n",
      "135501000\n",
      "135502000\n",
      "135503000\n",
      "135504000\n",
      "135505000\n",
      "135506000\n",
      "135507000\n",
      "135508000\n",
      "135509000\n",
      "135510000\n",
      "135511000\n",
      "135512000\n",
      "135513000\n",
      "135514000\n",
      "135515000\n",
      "135516000\n",
      "135517000\n",
      "135518000\n",
      "135519000\n",
      "135520000\n",
      "135521000\n",
      "135522000\n",
      "135523000\n",
      "135524000\n",
      "135525000\n",
      "135526000\n",
      "135527000\n",
      "135528000\n",
      "135529000\n",
      "135530000\n",
      "135531000\n",
      "135532000\n",
      "135533000\n",
      "135534000\n",
      "135535000\n",
      "135536000\n",
      "135537000\n",
      "135538000\n",
      "135539000\n",
      "135540000\n",
      "135541000\n",
      "135542000\n",
      "135543000\n",
      "135544000\n",
      "135545000\n",
      "135546000\n",
      "135547000\n",
      "135548000\n",
      "135549000\n",
      "135550000\n",
      "135551000\n",
      "135552000\n",
      "135553000\n",
      "135554000\n",
      "135555000\n",
      "135556000\n",
      "135557000\n",
      "135558000\n",
      "135559000\n",
      "135600000\n",
      "135601000\n",
      "135602000\n",
      "135603000\n",
      "135604000\n",
      "135605000\n",
      "135606000\n",
      "135607000\n",
      "135608000\n",
      "135609000\n",
      "135610000\n",
      "135611000\n",
      "135612000\n",
      "135613000\n",
      "135614000\n",
      "135615000\n",
      "135616000\n",
      "135617000\n",
      "135618000\n",
      "135619000\n",
      "135620000\n",
      "135621000\n",
      "135622000\n",
      "135623000\n",
      "135624000\n",
      "135625000\n",
      "135626000\n",
      "135627000\n",
      "135628000\n",
      "135629000\n",
      "135630000\n",
      "135631000\n",
      "135632000\n",
      "135633000\n",
      "135634000\n",
      "135635000\n",
      "135636000\n",
      "135637000\n",
      "135638000\n",
      "135639000\n",
      "135640000\n",
      "135641000\n",
      "135642000\n",
      "135643000\n",
      "135644000\n",
      "135645000\n",
      "135646000\n",
      "135647000\n",
      "135648000\n",
      "135649000\n",
      "135650000\n",
      "135651000\n",
      "135652000\n",
      "135653000\n",
      "135654000\n",
      "135655000\n",
      "135656000\n",
      "135657000\n",
      "135658000\n",
      "135659000\n",
      "135700000\n",
      "135701000\n",
      "135702000\n",
      "135703000\n",
      "135704000\n",
      "135705000\n",
      "135706000\n",
      "135707000\n",
      "135708000\n",
      "135709000\n",
      "135710000\n",
      "135711000\n",
      "135712000\n",
      "135713000\n",
      "135714000\n",
      "135715000\n",
      "135716000\n",
      "135717000\n",
      "135718000\n",
      "135719000\n",
      "135720000\n",
      "135721000\n",
      "135722000\n",
      "135723000\n",
      "135724000\n",
      "135725000\n",
      "135726000\n",
      "135727000\n",
      "135728000\n",
      "135729000\n",
      "135730000\n",
      "135731000\n",
      "135732000\n",
      "135733000\n",
      "135734000\n",
      "135735000\n",
      "135736000\n",
      "135737000\n",
      "135738000\n",
      "135739000\n",
      "135740000\n",
      "135741000\n",
      "135742000\n",
      "135743000\n",
      "135744000\n",
      "135745000\n",
      "135746000\n",
      "135747000\n",
      "135748000\n",
      "135749000\n",
      "135750000\n",
      "135751000\n",
      "135752000\n",
      "135753000\n",
      "135754000\n",
      "135755000\n",
      "135756000\n",
      "135757000\n",
      "135758000\n",
      "135759000\n"
     ]
    }
   ],
   "source": [
    "for i in date[date[\"group\"].isin(list(set(gl) - set(snapshot[\"group\"].unique())))][\"time\"].unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0102', '0103', '0104', '0107', '0108', '0109', '0110', '0111',\n",
       "       '0114', '0115', '0116', '0117', '0118', '0121', '0122', '0123',\n",
       "       '0124', '0125', '0128', '0129', '0130', '0131', '0201', '0211',\n",
       "       '0212', '0213', '0214', '0215', '0218', '0219', '0220', '0221',\n",
       "       '0222', '0225', '0226', '0227', '0228', '0301', '0304', '0305',\n",
       "       '0306', '0307', '0308', '0311', '0312', '0313', '0314', '0315',\n",
       "       '0318', '0319', '0320', '0321', '0322', '0325', '0326', '0327',\n",
       "       '0328', '0329', '0401', '0402', '0403', '0404', '0408', '0409',\n",
       "       '0410', '0411', '0412', '0415', '0416', '0417', '0418', '0419',\n",
       "       '0422', '0423', '0424', '0425', '0426', '0429', '0430', '0506',\n",
       "       '0507', '0508', '0509', '0510', '0513', '0514', '0515', '0516',\n",
       "       '0517', '0520', '0521', '0522', '0523', '0524', '0527', '0528',\n",
       "       '0529', '0530', '0531', '0603', '0604', '0605', '0606', '0610',\n",
       "       '0611', '0612', '0613', '0614', '0617', '0618', '0619', '0620',\n",
       "       '0621', '0624', '0625', '0626', '0627', '0628', '0701', '0702',\n",
       "       '0703', '0704', '0705', '0708', '0709', '0710', '0711', '0712',\n",
       "       '0715', '0716', '0717', '0718', '0719', '0722', '0723', '0724',\n",
       "       '0725', '0726', '0729', '0730', '0731', '0801', '0802', '0805',\n",
       "       '0806', '0807', '0808', '0809', '0812', '0813', '0814', '0815',\n",
       "       '0816', '0819', '0820', '0821', '0822', '0823', '0826', '0827',\n",
       "       '0828', '0829', '0830', '0902', '0903', '0904', '0905', '0906',\n",
       "       '0909', '0910', '0911', '0912', '0916', '0917', '0918', '0919',\n",
       "       '0920', '0923', '0924', '0925', '0926', '0927', '0930', '1008',\n",
       "       '1009', '1010', '1011', '1014', '1015', '1016', '1017', '1018',\n",
       "       '1021', '1022', '1023', '1024', '1025', '1028', '1029', '1030',\n",
       "       '1031', '1101', '1104', '1105', '1106', '1107', '1108', '1111',\n",
       "       '1112', '1113', '1114', '1115', '1118', '1119', '1120', '1121',\n",
       "       '1122', '1125', '1126', '1127', '1128', '1129', '1202', '1203',\n",
       "       '1204', '1205', '1206', '1209', '1210', '1211', '1212', '1213',\n",
       "       '1216', '1217', '1218', '1219', '1220', '1223', '1224', '1225',\n",
       "       '1226', '1227', '1230', '1231'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readPath = r'\\\\192.168.10.30\\Kevin_zhenyu\\day_stock_20200416\\SZ000001.csv.gz'\n",
    "dayData = pd.read_csv(readPath, compression='gzip')\n",
    "dayData[\"year\"] = dayData[\"date\"].apply(lambda x: x.split('-')[0])\n",
    "dayData[\"date1\"] = dayData[\"date\"].apply(lambda x: x.split('-')[1] + x.split('-')[2])\n",
    "da_te2 = dayData[dayData[\"year\"] == '2019'][\"date1\"].unique()\n",
    "da_te2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from unrar import rarfile\n",
    "import py7zr\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "year = \"2019\"\n",
    "readPath = 'E:\\\\SZ\\\\' + year + '\\\\***\\\\Snapshot.pkl'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "da_te1 = [x.split('\\\\')[3] for x in dataPathLs]\n",
    "display(len(da_te1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['20191211',\n",
       " '20190516',\n",
       " '20190627',\n",
       " '20190726',\n",
       " '20190823',\n",
       " '20190215',\n",
       " '20190523',\n",
       " '20191113',\n",
       " '20190312',\n",
       " '20190107',\n",
       " '20190320',\n",
       " '20190611',\n",
       " '20191226',\n",
       " '20190416',\n",
       " '20191112',\n",
       " '20190424',\n",
       " '20190812',\n",
       " '20191121',\n",
       " '20190807',\n",
       " '20191029',\n",
       " '20190410',\n",
       " '20191014',\n",
       " '20190829',\n",
       " '20190708',\n",
       " '20190628',\n",
       " '20190701',\n",
       " '20190211',\n",
       " '20191227',\n",
       " '20191231',\n",
       " '20191030',\n",
       " '20191111',\n",
       " '20190510',\n",
       " '20190612',\n",
       " '20190415',\n",
       " '20190129',\n",
       " '20190815',\n",
       " '20190305',\n",
       " '20190704',\n",
       " '20191218',\n",
       " '20190822',\n",
       " '20190903',\n",
       " '20190311',\n",
       " '20191118',\n",
       " '20190619',\n",
       " '20190417',\n",
       " '20190705',\n",
       " '20190315',\n",
       " '20191224',\n",
       " '20190930',\n",
       " '20191206',\n",
       " '20190709',\n",
       " '20190329',\n",
       " '20190618',\n",
       " '20190118',\n",
       " '20190403',\n",
       " '20191204',\n",
       " '20190912',\n",
       " '20190104',\n",
       " '20191223',\n",
       " '20190425',\n",
       " '20190430',\n",
       " '20190110',\n",
       " '20190918',\n",
       " '20190130',\n",
       " '20190906',\n",
       " '20191210',\n",
       " '20190925',\n",
       " '20191212',\n",
       " '20191024',\n",
       " '20190927',\n",
       " '20191108',\n",
       " '20191028',\n",
       " '20190301',\n",
       " '20191023',\n",
       " '20191126',\n",
       " '20190326',\n",
       " '20191031',\n",
       " '20191018',\n",
       " '20191230',\n",
       " '20190524',\n",
       " '20190605',\n",
       " '20191106',\n",
       " '20190225',\n",
       " '20190517',\n",
       " '20190228',\n",
       " '20191217',\n",
       " '20191219',\n",
       " '20190620',\n",
       " '20191105',\n",
       " '20190507',\n",
       " '20190610',\n",
       " '20190411',\n",
       " '20190902',\n",
       " '20190830',\n",
       " '20190325',\n",
       " '20191220',\n",
       " '20190724',\n",
       " '20190227',\n",
       " '20190916',\n",
       " '20190103',\n",
       " '20190717',\n",
       " '20190820',\n",
       " '20190710',\n",
       " '20190514',\n",
       " '20190319',\n",
       " '20190426',\n",
       " '20190530',\n",
       " '20190725',\n",
       " '20190814',\n",
       " '20190923',\n",
       " '20191022',\n",
       " '20190625',\n",
       " '20190422',\n",
       " '20190308',\n",
       " '20190613',\n",
       " '20190606',\n",
       " '20190719',\n",
       " '20190813',\n",
       " '20191011',\n",
       " '20190506',\n",
       " '20190904',\n",
       " '20190402',\n",
       " '20191015',\n",
       " '20190826',\n",
       " '20191025',\n",
       " '20191101',\n",
       " '20190218',\n",
       " '20190617',\n",
       " '20190911',\n",
       " '20190307',\n",
       " '20190314',\n",
       " '20190321',\n",
       " '20190909',\n",
       " '20190109',\n",
       " '20190123',\n",
       " '20190805',\n",
       " '20190220',\n",
       " '20190806',\n",
       " '20191127',\n",
       " '20190527',\n",
       " '20190114',\n",
       " '20190828',\n",
       " '20190521',\n",
       " '20190423',\n",
       " '20191119',\n",
       " '20191128',\n",
       " '20190905',\n",
       " '20190529',\n",
       " '20190419',\n",
       " '20191120',\n",
       " '20191104',\n",
       " '20191202',\n",
       " '20190924',\n",
       " '20191225',\n",
       " '20191213',\n",
       " '20190604',\n",
       " '20190723',\n",
       " '20190515',\n",
       " '20190917',\n",
       " '20191125',\n",
       " '20190328',\n",
       " '20190621',\n",
       " '20190226',\n",
       " '20190513',\n",
       " '20190213',\n",
       " '20190111',\n",
       " '20190429',\n",
       " '20190531',\n",
       " '20190125',\n",
       " '20191017',\n",
       " '20190712',\n",
       " '20190122',\n",
       " '20191209',\n",
       " '20190731',\n",
       " '20190614',\n",
       " '20190131',\n",
       " '20190520',\n",
       " '20191107',\n",
       " '20190910',\n",
       " '20190715',\n",
       " '20191016',\n",
       " '20190718',\n",
       " '20190327',\n",
       " '20190409',\n",
       " '20190802',\n",
       " '20191021',\n",
       " '20190522',\n",
       " '20191008',\n",
       " '20191205',\n",
       " '20190626',\n",
       " '20191122',\n",
       " '20190214',\n",
       " '20190124',\n",
       " '20190722',\n",
       " '20191009',\n",
       " '20190322',\n",
       " '20190508',\n",
       " '20190509',\n",
       " '20190115',\n",
       " '20190102',\n",
       " '20190222',\n",
       " '20190827',\n",
       " '20190920',\n",
       " '20190117',\n",
       " '20190702',\n",
       " '20190919',\n",
       " '20191115',\n",
       " '20190401',\n",
       " '20190528',\n",
       " '20190730',\n",
       " '20190821',\n",
       " '20190624',\n",
       " '20190404',\n",
       " '20190703',\n",
       " '20190808',\n",
       " '20190116',\n",
       " '20190716',\n",
       " '20190201',\n",
       " '20191129',\n",
       " '20190418',\n",
       " '20191010',\n",
       " '20190306',\n",
       " '20190926',\n",
       " '20191203',\n",
       " '20190108',\n",
       " '20190809',\n",
       " '20190121',\n",
       " '20190711',\n",
       " '20190801',\n",
       " '20191216',\n",
       " '20191114',\n",
       " '20190412',\n",
       " '20190816',\n",
       " '20190408',\n",
       " '20190128',\n",
       " '20190219',\n",
       " '20190313',\n",
       " '20190221',\n",
       " '20190212',\n",
       " '20190729',\n",
       " '20190318',\n",
       " '20190603',\n",
       " '20190304',\n",
       " '20190819']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from unrar import rarfile\n",
    "import py7zr\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "year = \"2019\"\n",
    "readPath = 'G:\\\\' + year + '\\\\***\\\\Snapshot.csv'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "da_te1 = [x.split('\\\\')[2] for x in dataPathLs]\n",
    "display(len(da_te1))\n",
    "year = \"2019\"\n",
    "readPath = 'F:\\\\' + year + '\\\\***\\\\Snapshot.csv'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "da_te = [x.split('\\\\')[2] for x in dataPathLs]\n",
    "display(len(da_te))\n",
    "da_te1 = list(set(da_te) | set(da_te1))\n",
    "display(len(da_te1))\n",
    "da_te1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(da_te2) - set(da_te1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0109(深交所数据)', '0314(深交所数据)', '0315(深交所数据)'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(da_te1) - set(da_te2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190102'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = open(\"\\\\\\\\192.168.10.30\\\\Kevin_zhenyu\\\\auction\\\\mdAucLog_SH_20190102.pkl\", 'rb')\n",
    "aucData = pickle.load(F1)\n",
    "aucData = aucData.rename(columns={\"time\":\"datetime\"})\n",
    "aucData[\"DateTime\"] = aucData[\"datetime\"].apply(lambda x: x.strftime(\"%Y%m%d%H%M%S\"))\n",
    "aucData[\"DateTime\"] = aucData[\"DateTime\"].astype(\"int64\")\n",
    "m_in = 91500 + int(aucData[\"DateTime\"].iloc[0]//1000000 * 1000000)\n",
    "m_ax = 92459 + int(aucData[\"DateTime\"].iloc[0]//1000000 * 1000000)\n",
    "aucData[\"DateTime\"] = aucData['DateTime'].clip(m_in, m_ax)\n",
    "aucData['time'] = ((aucData[\"DateTime\"] - int(aucData[\"DateTime\"].iloc[0]//1000000 * 1000000))*1000000).astype(np.int64)\n",
    "aucData[\"clockAtArrival\"] = aucData[\"DateTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "aucData[['date', 'time', 'datetime', 'clockAtArrival', 'ID', 'bid1p', 'ask1p', 'bid2q', 'bid1q', 'ask1q', 'ask2q']].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
