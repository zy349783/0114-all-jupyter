{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "pd.set_option('max_rows', 100)\n",
    "pd.set_option('max_columns', 100)\n",
    "\n",
    "perc = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "def generate_report(dd):\n",
    "    \n",
    "    import numpy as np\n",
    "    import os\n",
    "    import glob\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import pickle\n",
    "\n",
    "    \n",
    "    print('start')\n",
    "    readPath = 'L:\\\\orderLog\\\\data\\\\***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    path = np.sort(dataPathLs)[-11:]\n",
    "    cur = np.max([int(os.path.basename(i).split('.')[0]) for i in path])\n",
    "    assert(cur == int(dd))\n",
    "\n",
    "\n",
    "    body = \"<html><body><div>\" + 'Hi all,<p>The following is daily report based on ' + str(cur) + ' data.</p>'\n",
    "    body += '<p><b>I. Assertions</p></b>'\n",
    "    count = 0\n",
    "\n",
    "    # load data\n",
    "    print('-----------------------------------------------------------------------------------------------')\n",
    "    print('load data')\n",
    "    rawOrderLog = []\n",
    "    for thisPath in path:\n",
    "        print(thisPath)\n",
    "        data = pickle.load(open(thisPath, 'rb'))\n",
    "        data = data.rename(columns={'mdClockAtArrival': 'caamd'})\n",
    "        rawOrderLog += [data]\n",
    "    rawOrderLog = pd.concat(rawOrderLog, sort=False)\n",
    "\n",
    "    for col in ['clockAtArrival', 'caamd', 'secid', 'updateType', 'vai', 'absFilledThisUpdate', 'orderDirection', 'absOrderSize',\n",
    "                'absOrderSizeCumFilled', 'date', 'accCode', 'mse']:\n",
    "        rawOrderLog[col] = rawOrderLog[col].fillna(0).astype('int64')   \n",
    "    rawOrderLog = rawOrderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "    rawOrderLog = rawOrderLog[rawOrderLog[\"secid\"] >= 1000000]\n",
    "\n",
    "    if rawOrderLog[rawOrderLog.duplicated(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep=False)].shape[0] != 0:\n",
    "        print('There are accounts with duplicated ticks:')\n",
    "        print(rawOrderLog[rawOrderLog.duplicated(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep=False)]\\\n",
    "    .groupby(['date', 'colo', 'accCode'])['ars'].size())\n",
    "        rawOrderLog = rawOrderLog.drop_duplicates(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep='first')\n",
    "    \n",
    "    \n",
    "    print('There are ticks with orderDirection 0')\n",
    "    print(rawOrderLog[rawOrderLog['orderDirection'] == 0][['date', 'colo', 'accCode', \\\n",
    "                'secid', 'vai', 'updateType', 'sdd', 'orderDirection', 'absOrderSize', 'internalId', 'orderId']])\n",
    "    try:\n",
    "        num1 = rawOrderLog[(rawOrderLog['orderDirection'] == 0) & (rawOrderLog['date'] == cur)].shape[0]\n",
    "        assert(num1 < 30)\n",
    "    except:\n",
    "        count += 1\n",
    "        body += str(count) + '. There are in total ' + str(num1) + ' orders with orderDirection 0.<div>'\n",
    "\n",
    "\n",
    "    assert(rawOrderLog[rawOrderLog['updateType'] == 0][rawOrderLog[rawOrderLog['updateType'] == 0]\\\n",
    "                                                       .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                    'vai', 'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "    try:\n",
    "        assert(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "    except:\n",
    "        print('There are orders with all things same except sdd')\n",
    "        print(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId'], keep=False)])\n",
    "        assert(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId', 'sdd'], keep=False)].shape[0] == 0)\n",
    "    try:\n",
    "        assert(sum(rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() != 1) == 0) \n",
    "    except:\n",
    "        print('There are orders with same internalId but different orderId other than accCode 8856 case')\n",
    "        print(rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique()[rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() > 1])\n",
    "\n",
    "    r2 = rawOrderLog[(rawOrderLog['accCode'] != 8856) & (rawOrderLog['orderDirection'] != 0)]\n",
    "    r1 = rawOrderLog[(rawOrderLog['accCode'] == 8856) & (rawOrderLog['orderDirection'] != 0)]\n",
    "    r1['test'] = r1.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize']).grouper.group_info[0]\n",
    "    r1 = r1.sort_values(by=['test', 'clockAtArrival'])\n",
    "    r1.loc[r1['updateType'] != 0, 'vai'] = np.nan\n",
    "    r1['vai'] = r1.groupby('test')['vai'].ffill()\n",
    "    r2['test'] = r2.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize', 'internalId']).grouper.group_info[0]\n",
    "    r2 = r2.sort_values(by=['test', 'clockAtArrival'])\n",
    "    r2.loc[r2['updateType'] != 0, 'vai'] = np.nan\n",
    "    r2['vai'] = r2.groupby('test')['vai'].ffill()\n",
    "    try:\n",
    "        assert(sum(r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "    except:\n",
    "        a = r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique()[r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1].reset_index()\n",
    "        print('There are orders in 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print(pd.merge(r1, a[['test', 'vai']], on=['test', 'vai'], how='inner')[['secid', 'accCode', 'colo', 'vai', 'updateType', 'sdd', 'internalId', 'orderId', 'absOrderSize', 'absFilledThisUpdate', 'absOrderSizeCumFilled', 'orderPrice', 'tradePrice']])\n",
    "    \n",
    "    try:\n",
    "        assert(sum(r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "    except:\n",
    "        a = r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique()[r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1].reset_index()\n",
    "        print('There are orders out of 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print(pd.merge(r2, a[['test', 'vai']], on=['test', 'vai'], how='inner')[['secid', 'accCode', 'colo', 'vai', 'updateType', 'sdd', 'internalId', 'orderId', 'absOrderSize', 'absFilledThisUpdate', 'absOrderSizeCumFilled', 'orderPrice', 'tradePrice']])\n",
    "    orderLog = pd.concat([r1, r2])\n",
    "    del r1\n",
    "    del r2    \n",
    "\n",
    "    orderLog = orderLog.sort_values(by=['date', 'colo', 'accCode', 'secid', 'vai', 'clockAtArrival']).reset_index(drop=True)\n",
    "    orderLog['clock'] = orderLog['clockAtArrival'].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    orderLog[\"broker\"] = np.where(orderLog[\"accCode\"].astype(str).apply(lambda x: len(x) == 6), orderLog['accCode'] // 10000, orderLog['accCode'] // 100)\n",
    "    orderLog['colo_broker'] = orderLog['colo'].str[:2] + '_' + orderLog['broker'].astype('str')\n",
    "    orderLog['order'] = orderLog.groupby(['date', 'colo', 'accCode', 'secid', 'vai', 'orderDirection', 'absOrderSize', 'internalId']).grouper.group_info[0]\n",
    "    orderLog['group'] = orderLog.groupby(['date', 'secid', 'vai']).grouper.group_info[0]\n",
    "    orderLog['startClock'] = orderLog.groupby(['order'])['clockAtArrival'].transform('first')\n",
    "    orderLog['duration'] = orderLog['clockAtArrival'] - orderLog['startClock']\n",
    "    orderLog['orderPrice'] = orderLog['orderPrice'].apply(lambda x: round(x, 2))\n",
    "    orderLog['tradePrice'] = orderLog['tradePrice'].apply(lambda x: round(x, 2))\n",
    "    orderLog['orderDirection1'] = np.where(orderLog[\"orderDirection\"] == -2, -1, np.where(\n",
    "        orderLog[\"orderDirection\"] == 2, 1, orderLog[\"orderDirection\"]))\n",
    "    orderLog[\"ars\"] = orderLog.groupby(['order'])['ars'].transform('first')\n",
    "    orderLog['sdd'] = orderLog.groupby('order')['sdd'].transform('first')\n",
    "    orderLog['caamd'] = orderLog.groupby('order')['caamd'].transform('first')\n",
    "\n",
    "\n",
    "    ### Assertion 1:  make sure same direction in same date, secid, vai\n",
    "    print('=======================================================================================')\n",
    "    print('1. same date, secid, vai: same direction')\n",
    "    taking = orderLog[orderLog['ars'].isin([1, 7])]\n",
    "    making = orderLog[orderLog['ars'].isin([2, 3])]\n",
    "    else_orders = orderLog[~(orderLog['ars'].isin([1, 2, 3, 7]))]\n",
    "    if else_orders.shape[0] != 0:\n",
    "        print('orders with abnormal ars values')\n",
    "        print(else_orders[(else_orders['updateType'] == 0) & (else_orders['date'] == cur)].groupby('ars')['date'].size().sort_values(ascending=False))\n",
    "        print(else_orders[(else_orders['updateType'] == 0) & (else_orders['date'] == cur)].groupby('accCode')['date'].size().sort_values(ascending=False))\n",
    "        ars_list = else_orders[(else_orders['updateType'] == 0) & (else_orders['date'] == cur)].groupby('ars')['date'].size().sort_values(ascending=False).reset_index()['ars'].values\n",
    "        ars_list = ', '.join([str(x) for x in ars_list])\n",
    "        count += 1\n",
    "        body += str(count) + '. There are abnormal ars values in data: ' + ars_list + '.<div>'\n",
    "        kk = else_orders[(else_orders['updateType'] == 0) & (else_orders['date'] == cur)].groupby('accCode')['date'].size().sort_values(ascending=False)\n",
    "        if kk[kk > 20].shape[0] > 0:\n",
    "            count += 1\n",
    "            stock_list = ', '.join([str(x) for x in kk[kk > 20].index.values])\n",
    "            body += str(count) + '. These accounts have more than 20 abnormal ars orders: ' + stock_list + '.<div>'\n",
    "    taking['directNum'] = taking.groupby(['date', 'secid', 'vai', 'sdd'])['orderDirection1'].transform('nunique')\n",
    "    if taking[(taking['directNum'] != 1)].shape[0] > 0:\n",
    "        print('opposite direction for same date, same secid, same vai')\n",
    "        print(taking[(taking['directNum'] != 1) & (taking['updateType'] == 0)].groupby(['accCode'])['orderDirection'].size())\n",
    "        try:\n",
    "            num1 = taking[(taking['directNum'] != 1) & (taking['updateType'] == 0) & (taking['date'] == cur)].shape[0]\n",
    "            assert(num1 < 20)\n",
    "        except:\n",
    "            count += 1\n",
    "            body += str(count) + '. There are in total ' + str(num1) + ' orders with opposite directions under same date, secid, vai.<div>'\n",
    "        try:\n",
    "            num2 = list(taking[(taking['directNum'] != 1) & (taking['updateType'] == 0) & (taking['date'] == \\\n",
    "                cur)].groupby('accCode')['date'].size()[taking[(taking['directNum'] != 1) & \\\n",
    "               (taking['updateType'] == 0) & (taking['date'] == cur)].\\\n",
    "                                                                           groupby('accCode')['date'].size() > 10].index)\n",
    "            assert(len(num2) == 0)\n",
    "        except:\n",
    "            count += 1\n",
    "            num2 = ', '.join([str(x) for x in num2])\n",
    "            body += str(count) + '. ' + num2 + ' has more than 10 orders with opposite directions under same date, secid, vai.<div>'\n",
    "        taking = taking[taking['directNum'] == 1]\n",
    "\n",
    "    assert((taking.groupby(['date', 'secid', 'vai', 'sdd'])['orderDirection1'].nunique() == 1).all() == True)\n",
    "    orderLog = pd.concat([taking, making]).sort_values(by=['date', 'colo', 'accCode', 'secid', 'vai', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "    ## Assertion 2:  make sure each account, secid, vai only has one insertion\n",
    "    print('=======================================================================================')\n",
    "    print('2. same date, secid, vai, accCode: one insertion')\n",
    "    a = orderLog[orderLog['updateType'] == 0].groupby(['date', 'colo', 'accCode', 'secid', 'vai', 'sdd'])['clockAtArrival'].count().reset_index()\n",
    "    if a[a['clockAtArrival'] > 1].shape[0] > 0:\n",
    "        print('more than one insertion at same time')\n",
    "        a = a[(a['clockAtArrival'] > 1)]\n",
    "        print(a)\n",
    "        print(a.groupby(['date'])['accCode'].size())\n",
    "        print(a.groupby(['date', 'accCode'])['sdd'].size())\n",
    "        try:\n",
    "            num1 = a[a['date'] == cur].shape[0]\n",
    "            assert(num1 < 20)\n",
    "        except:\n",
    "            count += 1\n",
    "            body += str(count) + '. There are in total ' + str(num1) + ' orders with more than one insertion in same order.<div>'\n",
    "        try:\n",
    "            num2 = list(a[a['date'] == cur].groupby(['accCode'])['date'].size()[a[a['date'] == cur].groupby(['accCode'])['date'].size() > 10].index)\n",
    "            assert(len(num2) == 0)\n",
    "        except:\n",
    "            count += 1\n",
    "            num2 = ', '.join([str(x) for x in num2])\n",
    "            body += str(count) + '. ' + num2 + ' has more than 10 orders with more than one insertion in same order.<div>'        \n",
    "        d_el = pd.merge(orderLog, a, on=['date', 'colo', 'accCode', 'secid', 'vai', 'sdd'])['order'].unique()\n",
    "        orderLog = orderLog[~(orderLog['order'].isin(d_el))]\n",
    "\n",
    "    orderLog['isMsg'] = np.where(orderLog['updateType'] == 0, \n",
    "                                 np.where(orderLog['mse'] == 100, 1, 0), np.nan)\n",
    "    orderLog['isMsg'] = orderLog.groupby(['order'])['isMsg'].ffill()\n",
    "\n",
    "\n",
    "    ### Assertion 3:  check IPO stocks selling status\n",
    "    print('=======================================================================================')\n",
    "    print('3. IPO stocks selling (ars = 301, 302)')\n",
    "    if orderLog[(orderLog['ars'].isin([301, 302])) & (orderLog['date'] == cur)].shape[0] != 0:\n",
    "        kk = orderLog[(orderLog['ars'].isin([301, 302])) & (orderLog['date'] == cur)]\n",
    "        print(kk)\n",
    "        try:\n",
    "            assert(kk[kk['orderDirection1'] == 1].shape[0] == 0)\n",
    "            print('we only sell, never buy')\n",
    "        except:\n",
    "            print('There are IPO buy side orders!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            count += 1\n",
    "            num1 = kk[kk['orderDirection1'] == 1].shape[0]\n",
    "            body += str(count) + '. There are ' + str(num1) + ' IPO buy side orders.<div>'\n",
    "            print(kk[kk['orderDirection1'] == 1])\n",
    "        kk1 = kk[kk['updateType'] == 0]\n",
    "        kk1 = kk1.sort_values(by=['accCode', 'secid','clockAtArrival'])\n",
    "        kk1['diff'] = kk1.groupby(['accCode', 'secid'])['clockAtArrival'].apply(lambda x: x-x.shift(1))\n",
    "        kk1['diff'] = kk1['diff'].fillna(0)\n",
    "        try:\n",
    "            assert(kk1[kk1['diff'] < 10e6].shape[0] == 0)\n",
    "            print('for each stock in the same account, there is no insertion within 10 seconds of the previous insertion')\n",
    "        except:\n",
    "            count += 1\n",
    "            kk1 = kk1.reset_index()\n",
    "            num2 = kk1[kk1['diff'] < 10e6].shape[0]\n",
    "            body += str(count) + '. There are ' + str(num2) + ' over ' + str(kk1.shape[0]) + ' orders with insertion within 10 seconds for orders under same account same stock.<div>'\n",
    "            print('There are insertion within 10 seconds for orders under same account same stock!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')        \n",
    "            print(kk1[kk1['diff'] < 10e6])\n",
    "        kk2 = kk[(kk['updateType'] == 1)]\n",
    "\n",
    "        try:\n",
    "            assert(kk2[kk2['duration'] < 3e6].shape[0] == 0)\n",
    "            print('for each stock in the same account, the cancellation of an order happens more than 3 seconds after the insertion')\n",
    "        except:\n",
    "            count += 1\n",
    "            num2 = kk2[kk2['duration'] < 3e6].shape[0]\n",
    "            body += str(count) + '. There are ' + str(num2) + ' over ' + str(kk1.shape[0]) + ' orders with cancellation within 3 seconds after insertion.<div>'        \n",
    "            print('There are cancellation within 3 seconds for orders under same account same stock!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            print(kk2[kk2['duration'] < 3e6])\n",
    "\n",
    "\n",
    "    ### Assertion 4: check updateType == 7 orders, make sure updateType == 7 orders < 20 per account, < 100 in total\n",
    "    print('=======================================================================================')\n",
    "    print('4. updateType 7 orders')\n",
    "    if orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)].shape[0] != 0:\n",
    "        try:\n",
    "            assert(orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)].groupby(['date', 'accCode'])['order'].nunique().max() < 20)\n",
    "        except:\n",
    "            print('There are more than 20 updateType 7 orders per account')\n",
    "            count += 1\n",
    "            a = list(orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)].groupby(['accCode'])['order'].nunique()[\n",
    "                orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)].groupby(['accCode'])['order'].nunique() >= 20\n",
    "            ].index)\n",
    "            a = ', '.join([str(x) for x in a])\n",
    "            body += str(count) + '. ' + a + ' has more than 20 updateType 7 orders.<div>'\n",
    "        try:      \n",
    "            assert(orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)]['order'].nunique() < 100)\n",
    "        except:\n",
    "            print('Ther are more than 100 updateType 7 orders in total')\n",
    "            count += 1\n",
    "            body += str(count) + '. There are more than 100 updateType 7 orders in total.<div>'\n",
    "\n",
    "\n",
    "    ### Assertion 5: check updateType == 6 orders, make sure updateType == 6 orders < 5% per account\n",
    "    ### order being rejected by broker\n",
    "    print('=======================================================================================')\n",
    "    print('5. updateType 6 orders')\n",
    "    k1 = orderLog[(orderLog['updateType'] == 6) & (orderLog['date'] == cur)].groupby(['accCode'])['order'].nunique().reset_index()\n",
    "    k2 = orderLog[(orderLog['date'] == cur)].groupby(['accCode'])['order'].nunique().reset_index()\n",
    "    k = pd.merge(k1, k2, on=['accCode'], how='left')\n",
    "    k['prob'] = k['order_x']/k['order_y']\n",
    "    try:\n",
    "        assert(sum(k['prob'] >= 0.05) == 0)\n",
    "    except:\n",
    "        print('There are accounts with more than 5% updateType 6 orders!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print(k[k['prob'] >= 0.05])\n",
    "        a = k[k['prob'] >= 0.05]['accCode'].unique()\n",
    "        a = ', '.join([str(x) for x in a])\n",
    "        count += 1\n",
    "        body += str(count) + '. ' + a + ' has more than 5% updateType 6 orders.<div>'\n",
    "\n",
    "    ### Assertion 6: check CYB orders, make sure all CYB stocks have absOrderSize < 30w\n",
    "    print('=======================================================================================')\n",
    "    print('6. CYB stocks order size < 30w')\n",
    "    try:\n",
    "        cyb = orderLog[(orderLog['secid'] >= 2300000) & (orderLog['updateType'] == 0) & (orderLog['date'] == cur)]\n",
    "        assert(cyb[cyb['absOrderSize'] > 300000].shape[0] == 0)\n",
    "    except:\n",
    "        print('CYB stocks total absOrderSize >= 30w!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        num1 = cyb[cyb['absOrderSize'] > 300000].shape[0]\n",
    "        count += 1\n",
    "        body += str(count) + '. There are ' + str(num1) + ' orders with CYB absOrderSize > 30w.<div>'\n",
    "\n",
    "\n",
    "    ### Assertion 7:  make sure there is no unexpected updateType \n",
    "    print('=======================================================================================')\n",
    "    print('7. unexpected updateType')\n",
    "    def getTuple(x):\n",
    "        return tuple(i for i in x)\n",
    "\n",
    "    checkLog = orderLog[~((orderLog['updateType'] == 4) & (orderLog.groupby(['order'])['updateType'].shift(-1) == 4))]\n",
    "    checkLog = checkLog.groupby(['order'])['updateType'].apply(lambda x: getTuple(x)).reset_index()\n",
    "    checkLog['status'] = np.where(checkLog['updateType'].isin([(0, 2, 4), (0, 2, 1, 4), (0, 2, 1, 2, 4), (0, 2, 4, 1, 4), (0, 4), (0, 1, 4), (0, 4, 1, 4), (0, 2, 2, 4), (0, 4, 2, 4), (0, 2, 2, 1, 4), (0, 2, 2, 4, 1, 4)]),0,\n",
    "                         np.where(checkLog['updateType'].isin([(0, 2, 4, 1, 3), (0, 2, 4, 1, 4, 3), (0, 2, 1, 4, 3), (0, 4, 1, 3), (0, 1, 4, 3),\n",
    "                                                                   (0, 2, 2, 4, 1, 3), (0, 2, 2, 4, 1, 4, 3), (0, 2, 2, 1, 4, 3), (0, 4, 2, 4, 1, 3),\n",
    "                                                                   (0, 4, 2, 1, 3), (0, 4, 1, 4, 3), (0, 4, 1)]), 1,\n",
    "                         np.where(checkLog['updateType'].isin([(0, 2, 1, 3), (0, 2, 2, 1, 3), (0, 2, 3), (0, 3), (0, 1, 3), (0, ), (0, 2), (0, 2, 1), (0, 2, 2)]), 2, 3)))\n",
    "    print(checkLog[checkLog['status'] == 3].groupby('updateType')['order'].size())\n",
    "    orderLog = pd.merge(orderLog, checkLog[['order', 'status']], how='left', on=['order'], validate='many_to_one')\n",
    "    orderLog = orderLog[orderLog['status'].isin([0, 1, 2])].reset_index(drop=True)\n",
    "\n",
    "    ### Assertion 8:  make sure status==0 got all traded\n",
    "    print('=======================================================================================')\n",
    "    print('8. status == 0: all traded')\n",
    "    a = orderLog[(orderLog['status'] == 0)]\n",
    "    a = a.groupby(['date', 'order'])[['absOrderSizeCumFilled', 'absOrderSize']].max().reset_index()\n",
    "    a.columns = ['date', 'order', 'filled', 'total']\n",
    "    print('in total trade, any fill != total cases')\n",
    "    print(a[a['filled'] != a['total']])\n",
    "    if a[a['filled'] != a['total']].shape[0] > 0:\n",
    "        removeOrderLs = a[a['filled'] != a['total']]['order'].unique()\n",
    "        count += 1\n",
    "        body += str(count) + '. There are ' + str(a[(a['filled'] != a['total']) & (a['date'] == cur)]['order'].nunique()) + \\\n",
    "        ' over ' + str(a[(a['date'] == cur)]['order'].nunique()) + ' orders status == 0 but not all traded.<div>'\n",
    "        orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "    ### Assertion 9:  make sure status==1 got partial traded\n",
    "    print('=======================================================================================')\n",
    "    print('9. status == 1: partial traded')\n",
    "    a = orderLog[orderLog['status'] == 1]\n",
    "    a = a.groupby(['date', 'order'])[['absOrderSizeCumFilled', 'absOrderSize']].max().reset_index()\n",
    "    a.columns = ['date', 'order', 'filled', 'total']\n",
    "    print('in partial trade, any fill >= total or fill is 0 cases for updateType 4')\n",
    "    print(a[(a['filled'] >= a['total']) | (a['filled'] == 0)])\n",
    "    if a[(a['filled'] >= a['total']) | (a['filled'] == 0)].shape[0] > 0:\n",
    "        removeOrderLs = a[(a['filled'] >= a['total']) | (a['filled'] == 0)]['order'].unique()\n",
    "        count += 1\n",
    "        body += str(count) + '. There are ' + str(a[((a['filled'] >= a['total']) | (a['filled'] == 0)) & (a['date'] == cur)]['order'].nunique()) + \\\n",
    "        ' over ' + str(a[(a['date'] == cur)]['order'].nunique()) + ' orders status == 1 but not partial traded.<div>'\n",
    "        orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "    ### Assertion 10: make sure no cancellation within 1 sec\n",
    "    print('=======================================================================================')\n",
    "    print('10. no cancellation within 1 sec')\n",
    "    a = orderLog[(orderLog['updateType'] == 1) & (orderLog['duration'] < 1e6)]\n",
    "    print('any cancellation within 1 sec')\n",
    "    print(a)\n",
    "    if a[a['date'] == cur].shape[0] > 0:\n",
    "        count += 1\n",
    "        body += str(count) + '. There are ' + str(a[(a['date'] == cur)]['order'].nunique()) + ' orders cancel within 1s.<div>'    \n",
    "    if a.shape[0] > 0:\n",
    "        removeOrderLs = a['order'].unique()\n",
    "        orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "\n",
    "    ### Assertion 11: make sure no order has shares > 80w or notional > 800w\n",
    "    print('=======================================================================================')\n",
    "    print('11. Orders with size > 80w or notional > 800w')\n",
    "    orderLog['orderNtl'] = orderLog['absOrderSize'] * orderLog['orderPrice']\n",
    "    if orderLog[(orderLog['absOrderSize'] > 800000) & (orderLog['date'] == cur)].shape[0] > 0:\n",
    "        count += 1\n",
    "        body += str(count) + '. There are ' + str(orderLog[(orderLog['absOrderSize'] > 800000) & (orderLog['date'] == cur)]['order'].nunique()) + ' orders shares > 80w.<div>'    \n",
    "    if orderLog[orderLog['absOrderSize'] > 800000].shape[0] > 0:\n",
    "        print('some order quantity are > 80w')\n",
    "        print(orderLog[orderLog['absOrderSize'] > 800000].groupby(['colo', 'accCode'])['order'].nunique())\n",
    "        print(orderLog[orderLog['absOrderSize'] > 800000][['date', 'accCode', 'secid', 'vai', 'absOrderSize', 'orderPrice',\n",
    "                                                             'orderNtl', 'orderDirection', 'clock', 'order']])\n",
    "    if orderLog[(orderLog['orderNtl'] > 8000000) & (orderLog['date'] == cur)].shape[0] > 0:\n",
    "        count += 1\n",
    "        body += str(count) + '. There are ' + str(orderLog[(orderLog['orderNtl'] > 8000000) & (orderLog['date'] == cur)]['order'].nunique()) + ' orders notional > 800w.<div>'                \n",
    "    if orderLog[orderLog['orderNtl'] > 8000000].shape[0] > 0:\n",
    "        print('some order ntl are > 800w')\n",
    "        print(orderLog[orderLog['orderNtl'] > 8000000].groupby(['colo', 'accCode'])['order'].nunique())\n",
    "        print(orderLog[orderLog['orderNtl'] > 8000000][['date', 'accCode', 'secid', 'vai', 'absOrderSize', 'orderPrice',\n",
    "                                                          'orderNtl', 'orderDirection', 'clock', 'order', \"updateType\", \n",
    "                                                          \"tradePrice\", \"absOrderSizeCumFilled\", \"absFilledThisUpdate\"]])\n",
    "\n",
    "    removeOrderLs = list(set(orderLog[orderLog['absOrderSize'] > 800000]['order'].unique()) | set(orderLog[orderLog['orderNtl'] > 8000000]['order'].unique()))\n",
    "    orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "\n",
    "    orderLog = orderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'order', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "    orderLog['exchange'] = np.where(orderLog['secid'] >= 2000000, 'SZE', 'SSE')\n",
    "    orderLog['orderNtl'] = orderLog['orderPrice'] * orderLog['absOrderSize']\n",
    "    orderLog['tradeNtl'] = np.where(orderLog['updateType'] == 4, orderLog['tradePrice']*orderLog['absFilledThisUpdate'], 0)\n",
    "    orderLog[\"mrstaat\"] = orderLog.groupby(['order'])['mrstaat'].transform('first')\n",
    "    orderLog[\"ars\"] = orderLog.groupby(['order'])['ars'].transform('first')\n",
    "    orderLog[\"mrstauc\"] = orderLog.groupby(['order'])['mrstauc'].transform('first')\n",
    "    orderLog[\"mrsb90\"] = orderLog.groupby(['order'])['mrsb90'].transform('first')\n",
    "    orderLog[\"mrss90\"] = orderLog.groupby(['order'])['mrss90'].transform('first')\n",
    "    orderLog[\"aaa\"] = orderLog.groupby(['order'])['aaa'].transform('first')\n",
    "    orderLog = orderLog[~orderLog['ars'].isnull()]\n",
    "    # orderLog = orderLog[orderLog['ars'] % 10 == 1]\n",
    "\n",
    "\n",
    "    orderLog['m1'] = orderLog['mrstaat'].apply(lambda x: x - (x // 10000) * 10000)\n",
    "    orderLog['m2'] = orderLog['mrstauc'].apply(lambda x: x - (x // 10000) * 10000)\n",
    "    try:\n",
    "        orderLog['mrsb90'] = orderLog['mrsb90'].astype(float)\n",
    "    except:\n",
    "        print(orderLog[orderLog['mrsb90'] == '-'])\n",
    "        orderLog = orderLog[orderLog['mrsb90'] != '-']\n",
    "        orderLog['mrsb90'] = orderLog['mrsb90'].astype(float)\n",
    "    try:\n",
    "        orderLog['mrss90'] = orderLog['mrss90'].astype(float)\n",
    "    except:\n",
    "        print(orderLog[orderLog['mrss90'] == '-'])\n",
    "        orderLog = orderLog[orderLog['mrss90'] != '-']\n",
    "        orderLog['mrss90'] = orderLog['mrss90'].astype(float)\n",
    "    try:\n",
    "        orderLog['aaa'] = orderLog['aaa'].astype(float)\n",
    "    except:\n",
    "        print(orderLog[orderLog['aaa'] == '-'])\n",
    "        orderLog = orderLog[orderLog['aaa'] != '-']\n",
    "        orderLog['aaa'] = orderLog['aaa'].astype(float)\n",
    "    \n",
    "    orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'mrstauc'] = \\\n",
    "    orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'm2']\n",
    "\n",
    "    orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'mrstaat'] = \\\n",
    "    orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'm1']\n",
    "\n",
    "    orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'mrstauc'] = \\\n",
    "    orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'm2']\n",
    "\n",
    "    orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'mrstaat'] = \\\n",
    "    orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'm1']    \n",
    "\n",
    "\n",
    "    orderLog['sta'] = np.where(orderLog['mrstaat'] == 1000, '1. staone', np.where(\n",
    "    orderLog['mrstaat'] == 3000, '2. statwo', np.where(\n",
    "    orderLog['mrstaat'].isin([11000, 13000]), '3. sta300', 'else')))\n",
    "    print(orderLog[(orderLog['sta'] == 'else') & (orderLog['updateType'] == 0)].groupby(['date', 'accCode'])['secid'].size())\n",
    "    print(orderLog[(orderLog['sta'] == 'else') & (orderLog['date'] == cur) & (orderLog['updateType'] == 0)])\n",
    "    m1 = orderLog[(orderLog['sta'] == 'else') & (orderLog['date'] == cur) & (orderLog['updateType'] == 0)].shape[0]\n",
    "    if m1 != 0:\n",
    "        count += 1\n",
    "        body += str(count) + '. There are ' + str(m1) + ' orders with invalid strategy.<div>'                    \n",
    "    orderLog = orderLog[orderLog['mrstaat'].isin([11000, 13000, 1000, 3000])]\n",
    "    print(orderLog[orderLog['updateType'] == 0].groupby(['mrstaat', 'mrstauc'])['date'].size())\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    body += '<p><b>II. fill rate</b></p>'\n",
    "\n",
    "    import pandas as pd\n",
    "    def convertToHtml(result,title):\n",
    "        #将数据转换为html的table\n",
    "        #result是list[list1,list2]这样的结构\n",
    "        #title是list结构；和result一一对应。titleList[0]对应resultList[0]这样的一条数据对应html表格中的一列\n",
    "        d = {}\n",
    "        index = 0\n",
    "        for t in title:\n",
    "            d[t]=result[index]\n",
    "            index = index+1\n",
    "        df = pd.DataFrame(d)\n",
    "        df = df[title]\n",
    "        h = df.to_html(index=False)\n",
    "        return h\n",
    "\n",
    "    placeSZE = orderLog[(orderLog['secid'] >= 2000000) & (orderLog['updateType'] == 0)]\n",
    "    print('%.2f%% SZE orders triggered by msg data'%(placeSZE[placeSZE['isMsg'] == 1].shape[0]/placeSZE.shape[0]*100))\n",
    "    body += '%.2f%% SZE orders triggered by msg data<div>'%(placeSZE[placeSZE['isMsg'] == 1].shape[0]/placeSZE.shape[0]*100)\n",
    "    placeSZE = orderLog[(orderLog['secid'] >= 2000000) & (orderLog['updateType'] == 0)]\n",
    "    r1 = placeSZE[placeSZE['isMsg'] == 1].groupby(['date', 'accCode'])['secid'].size().reset_index()\n",
    "    r2 = placeSZE.groupby(['date', 'accCode'])['secid'].size().reset_index()\n",
    "    re = pd.merge(r1, r2, on=['date', 'accCode'], how='inner')\n",
    "    re['perc'] = re['secid_x'] / re['secid_y']\n",
    "    print(re[re['perc'] < 0.8])\n",
    "    for i in re[re['perc'] < 0.8]['accCode'].unique():\n",
    "        a = list(re[(re['perc'] < 0.8) & (re['accCode'] == i)]['date'].unique())\n",
    "        a = ', '.join([str(x) for x in a])\n",
    "        body += 'accCode ' + str(i) + ' SZE orders on ' + a + ' has msg triggered percentage < 80%<div>'\n",
    "\n",
    "    orderLog['tag'] = 'previous'\n",
    "    orderLog.loc[orderLog['date'] == cur, 'tag'] = 'current'\n",
    "    o1 = orderLog[orderLog['updateType'] == 0].groupby(['tag', 'exchange'])['orderNtl'].sum().reset_index()\n",
    "    o2 = orderLog[orderLog['updateType'] == 4].groupby(['tag', 'exchange'])['tradeNtl'].sum().reset_index()\n",
    "    o = pd.merge(o1, o2, on=['tag', 'exchange'])\n",
    "    o['perc'] = o['tradeNtl'] / o['orderNtl']\n",
    "    o['perc'] = o['perc'].apply(lambda x: '%.f'%(x*100))\n",
    "    o1 = o[o['tag'] == 'current']\n",
    "    o1 = o1.rename(columns={'perc':'cur_perc'})\n",
    "    o2 = o[o['tag'] == 'previous']\n",
    "    o2 = o2.rename(columns={'perc':'prev_perc'})\n",
    "    re = pd.merge(o1[['exchange', 'cur_perc']], o2[['exchange', 'prev_perc']], on='exchange')\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(re.groupby(['exchange']).first().to_html()))\n",
    "    result = [re['exchange'].values, re['cur_perc'].values, re['prev_perc'].values]\n",
    "    title=['exchange', 'cur_perc', 'prev_perc']\n",
    "    body += convertToHtml(result,title)\n",
    "\n",
    "    orderLog['num'] = orderLog.groupby(['exchange', 'colo', 'accCode', 'secid'])['tag'].transform('nunique')\n",
    "    checkLog = orderLog[orderLog['num'] == 2]\n",
    "    # checkLog = orderLog\n",
    "    d1 = checkLog[(checkLog['updateType'] == 0)].groupby(['tag', 'exchange', 'sta', 'colo', 'accCode'])['orderNtl'].sum().reset_index()\n",
    "    d2 = checkLog[(checkLog['updateType'] == 4)].groupby(['tag', 'exchange', 'sta', 'colo', 'accCode'])['tradeNtl'].sum().reset_index()\n",
    "    dd = pd.merge(d1, d2, on=['tag', 'exchange', 'sta', 'colo', 'accCode'])\n",
    "    dd['fill rate'] = dd['tradeNtl'] / dd['orderNtl']\n",
    "    add = checkLog[(checkLog['updateType'] == 0)].groupby(['tag', 'exchange', 'sta', 'colo', 'accCode'])['order'].nunique().reset_index()\n",
    "    add = add.rename(columns={'order':'num'})\n",
    "    dd = pd.merge(dd, add, on=['tag', 'exchange', 'sta', 'colo', 'accCode'])\n",
    "    prev = dd[dd['tag'] == 'previous']\n",
    "    prev = prev.rename(columns={'num':'prev_num', 'fill rate':'prev_fillRate'})\n",
    "    cur = dd[dd['tag'] == 'current']\n",
    "    cur = cur.rename(columns={'num':'cur_num', 'fill rate':'cur_fillRate'})\n",
    "    report = pd.merge(prev[['exchange', 'sta', 'colo', 'accCode', 'prev_num', 'prev_fillRate']], \n",
    "                  cur[['exchange', 'sta', 'colo', 'accCode', 'cur_num', 'cur_fillRate']], on=['exchange', 'sta', 'colo', 'accCode'], how='inner')\n",
    "\n",
    "    a1 = checkLog[(checkLog['updateType'] == 0) & (checkLog['tag'] == 'previous')].groupby(['date', 'exchange', 'sta', 'colo', 'accCode'])['orderNtl'].sum().reset_index()\n",
    "    a2 = checkLog[(checkLog['updateType'] == 4) & (checkLog['tag'] == 'previous')].groupby(['date', 'exchange', 'sta', 'colo', 'accCode'])['tradeNtl'].sum().reset_index()\n",
    "    aa = pd.merge(a1, a2, on=['date', 'exchange', 'sta', 'colo', 'accCode'])\n",
    "    aa['fill rate'] = aa['tradeNtl'] / aa['orderNtl']\n",
    "    aa['count'] = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['date'].transform('nunique')\n",
    "    aa1 = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['fill rate'].describe()[['mean', 'std']].reset_index()\n",
    "    aa1 = aa1.fillna(0)\n",
    "    aa = pd.merge(aa, aa1, on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "    for j in [1]:\n",
    "        aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "        aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "        aa['count1'] = np.where((aa['fill rate'] <= aa[str(j) + 'std_high']) & (aa['fill rate'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "        re1 = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['count1'].sum().reset_index()\n",
    "        re2 = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['count'].first().reset_index()\n",
    "        re1 = pd.merge(re2, re1, on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "        re1['count1'] = re1['count1'] / re1['count']\n",
    "        re1 = re1.rename(columns={'count1':str(j)+'*std'})\n",
    "    for j in [1.5, 2, 2.5, 3, 3.5, 4, 4.5]:\n",
    "        aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "        aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "        aa['count1'] = np.where((aa['fill rate'] <= aa[str(j) + 'std_high']) & (aa['fill rate'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "        re = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['count1'].sum().reset_index()\n",
    "        re2 = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['count'].first().reset_index()\n",
    "        re = pd.merge(re2, re, on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "        re['count1'] = re['count1'] / re['count']\n",
    "        re = re.rename(columns={'count1':str(j)+'*std'})\n",
    "        re1 = pd.merge(re1, re[['exchange', 'sta', 'colo', 'accCode', str(j)+'*std']], on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "    nc = []\n",
    "    for i in range(0, re1.shape[0]):\n",
    "        nc.append(np.float(re1.columns[5:][(re1.iloc[i, 5:] == 1)][0].split('*')[0]))\n",
    "    re1['n'] = nc\n",
    "    print(re1.shape[0])\n",
    "    print(aa1.shape[0])\n",
    "    aa1 = pd.merge(aa1, re1[['exchange', 'sta', 'colo', 'accCode', 'n']], on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "    aa1['min'] = aa1['mean'] - aa1['std'] * aa1['n']\n",
    "    aa1['max'] = aa1['mean'] + aa1['std'] * aa1['n']\n",
    "    report = pd.merge(report, aa1, on=['exchange', 'sta', 'colo', 'accCode'], how='left')\n",
    "    assert(report[report['mean'].isnull()].shape[0] == 0)\n",
    "    print(report[((report['cur_fillRate'] > report['max']) | (report['cur_fillRate'] < report['min'])) & (report['cur_num'] > 100) & (abs(report['cur_fillRate'] - report['prev_fillRate']) > 0.15)].groupby(['exchange', 'sta', 'colo', 'accCode'])['prev_fillRate', 'cur_fillRate'].first())\n",
    "    report = report[((report['cur_fillRate'] > report['max']) | (report['cur_fillRate'] < report['min'])) & (report['cur_num'] > 100) & (abs(report['cur_fillRate'] - report['prev_fillRate']) > 0.15)].groupby(['exchange', 'sta', 'colo', 'accCode'])['prev_fillRate', 'cur_fillRate'].first().reset_index()\n",
    "\n",
    "    for cols in ['prev_fillRate', 'cur_fillRate']:\n",
    "        report[cols] = report[cols].apply(lambda x: '%.f%%'%(100*x))\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(report.groupby(['exchange', 'sta', 'colo', 'accCode']).first().to_html()))\n",
    "    body += '<div>In the following cases, fill rate under given accCode pass the hurdle we set:'\n",
    "    result = [report['exchange'].values, report['sta'].values, report['colo'].values, report['accCode'].values, \n",
    "             report['prev_fillRate'].values, report['cur_fillRate'].values]\n",
    "    title = ['exchange', 'sta', 'colo', 'accCode', 'prev_fillRate', 'cur_fillRate']\n",
    "    body += convertToHtml(result,title)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    body += '<p><b>III. internal latency</b></p>'\n",
    "\n",
    "    orderLog['num'] = orderLog.groupby(['exchange', 'colo', 'accCode', 'secid'])['tag'].transform('nunique')\n",
    "    checkLog = orderLog[(orderLog[\"updateType\"] == 0) & (orderLog['num'] == 2)]\n",
    "    checkLog = checkLog[checkLog['caamd'] != 0]\n",
    "    checkLog['internal_latency'] = checkLog[\"clockAtArrival\"] - checkLog[\"caamd\"]\n",
    "    SZE = checkLog[checkLog['secid'] >= 2000000]\n",
    "    SSE = checkLog[checkLog['secid'] < 2000000]\n",
    "    SZE = SZE[SZE['isMsg'] == 1]\n",
    "    c1 = SZE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].quantile(.95).reset_index()\n",
    "    c2 = SZE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].median().reset_index()\n",
    "    c3 = SZE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].count().reset_index()\n",
    "\n",
    "    re1 = pd.merge(c3, c1, on=['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])\n",
    "    re1 = re1.rename(columns = {'internal_latency_x': 'count', 'internal_latency_y': '95 percentile'})\n",
    "    re1 = pd.merge(re1, c2, on=['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])\n",
    "    re1 = re1.rename(columns = {'internal_latency': 'median'})\n",
    "    re1['isMsg'] = 1\n",
    "\n",
    "    c1 = SSE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].quantile(.95).reset_index()\n",
    "    c2 = SSE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].median().reset_index()\n",
    "    c3 = SSE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].count().reset_index()\n",
    "\n",
    "    re2 = pd.merge(c3, c1, on=['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])\n",
    "    re2 = re2.rename(columns = {'internal_latency_x': 'count', 'internal_latency_y': '95 percentile'})\n",
    "    re2 = pd.merge(re2, c2, on=['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])\n",
    "    re2 = re2.rename(columns = {'internal_latency': 'median'})\n",
    "    re2\n",
    "\n",
    "    re = pd.concat([re1, re2]).reset_index(drop=True)\n",
    "\n",
    "    for col in ['isMsg','median', '95 percentile']:\n",
    "        re[col] = re[col].astype(int)\n",
    "\n",
    "    re1 = re[re['tag'] == 'current']\n",
    "    re1 = re1.rename(columns={'count':'cur_count', 'median':'cur_med', '95 percentile':'cur_95p'})\n",
    "    re2 = re[re['tag'] == 'previous']\n",
    "    re2 = re2.rename(columns={'count':'prev_count', 'median':'prev_med', '95 percentile':'prev_95p'})\n",
    "    report = pd.merge(re1, re2, on=['exchange', 'colo', 'accCode', 'sta', 'isMsg'])\n",
    "\n",
    "    checkLog[(checkLog['exchange'] == 'SSE') | ((checkLog['exchange'] == 'SZE') & (checkLog['isMsg'] == 1))]\n",
    "    aa = checkLog[(checkLog['tag'] == 'previous')].groupby(['date', 'exchange', 'sta', 'colo', 'accCode', 'isMsg'])['internal_latency'].median().reset_index()\n",
    "    aa['count'] = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['date'].transform('nunique')\n",
    "    aa1 = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['internal_latency'].describe()[['mean', 'std']].reset_index()\n",
    "    aa1 = aa1.fillna(0)\n",
    "    aa = pd.merge(aa, aa1, on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "    for j in [1]:\n",
    "        aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "        aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "        aa['count1'] = np.where((aa['internal_latency'] <= aa[str(j) + 'std_high']) & (aa['internal_latency'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "        re1 = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['count1'].sum().reset_index()\n",
    "        re2 = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['count'].first().reset_index()\n",
    "        re1 = pd.merge(re2, re1, on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "        re1['count1'] = re1['count1'] / re1['count']\n",
    "        re1 = re1.rename(columns={'count1':str(j)+'*std'})\n",
    "    for j in [1.5, 2, 2.5, 3, 3.5, 4, 4.5]:\n",
    "        aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "        aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "        aa['count1'] = np.where((aa['internal_latency'] <= aa[str(j) + 'std_high']) & (aa['internal_latency'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "        re = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['count1'].sum().reset_index()\n",
    "        re2 = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['count'].first().reset_index()\n",
    "        re = pd.merge(re2, re, on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "        re['count1'] = re['count1'] / re['count']\n",
    "        re = re.rename(columns={'count1':str(j)+'*std'})\n",
    "        re1 = pd.merge(re1, re[['exchange', 'sta', 'colo', 'accCode', 'isMsg', str(j)+'*std']], on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "    nc = []\n",
    "    for i in range(0, re1.shape[0]):\n",
    "        nc.append(np.float(re1.columns[6:][(re1.iloc[i, 6:] == 1)][0].split('*')[0]))\n",
    "    re1['n'] = nc\n",
    "    print(re1.shape[0])\n",
    "    print(aa1.shape[0])\n",
    "    aa1 = pd.merge(aa1, re1[['exchange', 'sta', 'colo', 'accCode', 'isMsg', 'n']], on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "    aa1['min'] = aa1['mean'] - aa1['std'] * aa1['n']\n",
    "    aa1['max'] = aa1['mean'] + aa1['std'] * aa1['n']\n",
    "    report = pd.merge(report, aa1, on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'], how='left')\n",
    "    assert(report[report['mean'].isnull()].shape[0] == 0)\n",
    "\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(report[((report['cur_med'] > report['max']) | (report['cur_med'] < report['min'])) \\\n",
    "                        & (report['cur_count'] > 100) & (abs(report['cur_med'] - report['prev_med']) > 20)] \\\n",
    "                 .groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['prev_med', 'cur_med', 'prev_95p', 'cur_95p'].first().to_html()))\n",
    "    report = report[((report['cur_med'] > report['max']) | (report['cur_med'] < report['min'])) \\\n",
    "                        & (report['cur_count'] > 100) & (abs(report['cur_med'] - report['prev_med']) > 20)] \\\n",
    "                 .groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['prev_med', 'cur_med', 'prev_95p', 'cur_95p'].first().reset_index()\n",
    "\n",
    "    body += '<div>In the following cases, internal latency under given accCode pass the hurdle we set:'\n",
    "    result = [report['exchange'].values, report['sta'].values, report['colo'].values, report['accCode'].values, report['isMsg'].values, \n",
    "             report['prev_med'].values, report['prev_95p'].values, report['cur_med'].values, report['cur_95p'].values]\n",
    "    title = ['exchange', 'sta', 'colo', 'accCode', 'isMsg', 'prev_med', 'prev_95p', 'cur_med', 'cur_95p']\n",
    "    body += convertToHtml(result,title)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    body += '<p><b>IV. tickToMBD</b></p>'\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    checkLog = orderLog[~orderLog['start_time'].isnull()]\n",
    "    checkLog = checkLog.drop_duplicates(['date', 'secid', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'], keep=False)\n",
    "    checkLog = checkLog[~checkLog['accCode'].isnull()]\n",
    "    checkLog['tag'] = 'previous'\n",
    "    checkLog.loc[checkLog['date'] == orderLog['date'].max(), 'tag'] = 'current'\n",
    "\n",
    "    cc1 = checkLog\n",
    "    cc1 = cc1.reset_index(drop=True)\n",
    "    cc1['ordering'] = cc1.index\n",
    "    cc1['time_diff'] = cc1['caa_orderLog'] - cc1['start_time']\n",
    "    cc1['colo1'] = cc1['colo'].str[:2] + cc1['colo'].str[3:5] + cc1['colo'].str[6:8]\n",
    "    cc1['colo_broker'] = cc1['colo1'] + '_' + cc1[\"accCode\"].astype(int).astype(str)\n",
    "\n",
    "\n",
    "    re1 = cc1.groupby(['tag', 'sta', 'colo', 'accCode'])['time_diff'].describe().fillna(0).astype(int).reset_index()\n",
    "    # re1 = re1[re1['count'] > 20].reset_index()\n",
    "    c1 = cc1.groupby(['tag', 'sta', 'colo', 'accCode'])['time_diff'].apply(lambda x: x.describe([0.1])['10%']).astype(int).reset_index()\n",
    "    c1 = c1.rename(columns={\"time_diff\":\"10%\"})\n",
    "    re1 = pd.merge(re1, c1[['tag', 'sta', 'colo', 'accCode', '10%']], on=['tag', 'sta', 'colo', 'accCode'])\n",
    "    c1 = cc1.groupby(['tag', 'sta', 'colo', 'accCode'])['time_diff'].apply(lambda x: x.describe([0.9])['90%']).astype(int).reset_index()\n",
    "    c1 = c1.rename(columns={\"time_diff\":\"90%\"})\n",
    "    re1 = pd.merge(re1, c1[['tag', 'sta', 'colo', 'accCode', '90%']], on=['tag', 'sta', 'colo', 'accCode'])\n",
    "    ree1 = re1[re1['tag'] == 'previous']\n",
    "    ree1 = ree1.rename(columns={'50%':'prev_med', 'count':'prev_count'})\n",
    "    ree2 = re1[re1['tag'] == 'current']\n",
    "    ree2 = ree2.rename(columns={'50%':'cur_med', 'count':'cur_count'})\n",
    "    report = pd.merge(ree1[['sta', 'colo', 'accCode', 'prev_count', 'prev_med']], \n",
    "                   ree2[['sta', 'colo', 'accCode', 'cur_count', 'cur_med']], on=['sta', 'colo', 'accCode'])\n",
    "\n",
    "\n",
    "    aa = cc1[(cc1['updateType'] == 0) & (cc1['tag'] == 'previous')].groupby(['date', 'sta', 'colo', 'accCode'])['time_diff'].median().reset_index()\n",
    "    aa['count'] = aa.groupby(['sta', 'colo', 'accCode'])['date'].transform('nunique')\n",
    "    aa1 = aa.groupby(['sta', 'colo', 'accCode'])['time_diff'].describe()[['mean', 'std']].reset_index()\n",
    "    aa1 = aa1.fillna(0)\n",
    "    aa = pd.merge(aa, aa1, on=['sta', 'colo', 'accCode'])\n",
    "    for j in [1]:\n",
    "        aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "        aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "        aa['count1'] = np.where((aa['time_diff'] <= aa[str(j) + 'std_high']) & (aa['time_diff'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "        re1 = aa.groupby(['sta', 'colo', 'accCode'])['count1'].sum().reset_index()\n",
    "        re2 = aa.groupby(['sta', 'colo', 'accCode'])['count'].first().reset_index()\n",
    "        re1 = pd.merge(re2, re1, on=['sta', 'colo', 'accCode'])\n",
    "        re1['count1'] = re1['count1'] / re1['count']\n",
    "        re1 = re1.rename(columns={'count1':str(j)+'*std'})\n",
    "    for j in [1.5, 2, 2.5, 3, 3.5, 4, 4.5]:\n",
    "        aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "        aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "        aa['count1'] = np.where((aa['time_diff'] <= aa[str(j) + 'std_high']) & (aa['time_diff'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "        re = aa.groupby(['sta', 'colo', 'accCode'])['count1'].sum().reset_index()\n",
    "        re2 = aa.groupby(['sta', 'colo', 'accCode'])['count'].first().reset_index()\n",
    "        re = pd.merge(re2, re, on=['sta', 'colo', 'accCode'])\n",
    "        re['count1'] = re['count1'] / re['count']\n",
    "        re = re.rename(columns={'count1':str(j)+'*std'})\n",
    "        re1 = pd.merge(re1, re[['sta', 'colo', 'accCode', str(j)+'*std']], on=['sta', 'colo', 'accCode'])\n",
    "    nc = []\n",
    "    for i in range(0, re1.shape[0]):\n",
    "        nc.append(np.float(re1.columns[6:][(re1.iloc[i, 6:] == 1)][0].split('*')[0]))\n",
    "    re1['n'] = nc\n",
    "    print(re1.shape[0])\n",
    "    print(aa1.shape[0])\n",
    "    aa1 = pd.merge(aa1, re1[['sta', 'colo', 'accCode', 'n']], on=['sta', 'colo', 'accCode'])\n",
    "    aa1['min'] = aa1['mean'] - aa1['std'] * aa1['n']\n",
    "    aa1['max'] = aa1['mean'] + aa1['std'] * aa1['n']\n",
    "    report = pd.merge(report, aa1, on=['sta', 'colo', 'accCode'], how='left')\n",
    "    assert(report[report['mean'].isnull()].shape[0] == 0)\n",
    "\n",
    "    from IPython.display import display, HTML\n",
    "    display(HTML(report[((report['cur_med'] > report['max']) | (report['cur_med'] < report['min'])) \\\n",
    "                        & (report['cur_count'] > 100) & (abs(report['cur_med'] - report['prev_med']) > 1000)] \\\n",
    "                 .groupby(['sta', 'colo', 'accCode'])['prev_med', 'cur_med'].first().to_html()))\n",
    "    report = report[((report['cur_med'] > report['max']) | (report['cur_med'] < report['min'])) \\\n",
    "                        & (report['cur_count'] > 100) & (abs(report['cur_med'] - report['prev_med']) > 1000)] \\\n",
    "                 .groupby(['sta', 'colo', 'accCode'])['prev_med', 'cur_med'].first().reset_index()\n",
    "    body += '<div>In the following cases, tickToMBD under given accCode pass the hurdle we set:<div>'\n",
    "    result = [report['sta'].values, report['colo'].values, report['accCode'].values,\n",
    "             report['prev_med'].values, report['cur_med'].values]\n",
    "    title = ['sta', 'colo', 'accCode', 'prev_med', 'cur_med']\n",
    "    body += convertToHtml(result,title)\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    body += '<p><b>V. order return</b></p>'\n",
    "    checkLog = orderLog[orderLog['updateType'] == 4]\n",
    "    if checkLog[checkLog['absFilledThisUpdate'] == 0].shape[0] != 0:\n",
    "        print('There are stocks with zero trade size')\n",
    "        print(checkLog[checkLog['absFilledThisUpdate'] == 0].groupby(['colo', 'accCode'])['secid'].size())\n",
    "        checkLog = checkLog[checkLog['absFilledThisUpdate'] != 0]\n",
    "    if checkLog[checkLog['beta_60'].isnull()].shape[0] != 0:\n",
    "        print('There are stocks with null beta')\n",
    "        print(checkLog[checkLog['beta_60'].isnull()])\n",
    "        checkLog = checkLog[~checkLog['beta_60'].isnull()]\n",
    "    checkLog['max_trade'] = checkLog.groupby('order')['absOrderSizeCumFilled'].transform('max')\n",
    "    checkLog['last'] = 0\n",
    "    checkLog.loc[checkLog['max_trade'] == checkLog['absOrderSizeCumFilled'], 'last'] = 1\n",
    "    checkLog[\"buyRet\"] = np.where(checkLog[\"orderDirection\"].isin([1, 2]), checkLog[\"adjMid_F90s\"] / checkLog[\"tradePrice\"] - 1, np.nan)\n",
    "    checkLog[\"buyRet1\"] = np.where(checkLog[\"orderDirection\"].isin([1, 2]), checkLog[\"adjMid_F300s\"] / checkLog[\"tradePrice\"] - 1, np.nan)\n",
    "    checkLog[\"sellRet\"] = np.where(checkLog[\"orderDirection\"].isin([-1, -2]), checkLog[\"tradePrice\"] / checkLog[\"adjMid_F90s\"] - 1, np.nan)\n",
    "    checkLog[\"sellRet1\"] = np.where(checkLog[\"orderDirection\"].isin([-1, -2]), checkLog[\"tradePrice\"] / checkLog[\"adjMid_F300s\"] - 1, np.nan)\n",
    "    checkLog[\"buyNum\"] = np.where((checkLog[\"orderDirection\"].isin([1, 2])) & (checkLog['last'] == 1), 1, 0)\n",
    "    checkLog[\"sellNum\"] = np.where((checkLog[\"orderDirection\"].isin([-1, -2])) & (checkLog['last'] == 1), 1, 0)\n",
    "    checkLog[\"server\"] = checkLog[\"colo\"].apply(lambda x: x.split(\"_\")[0] + x.split(\"_\")[1] + x.split(\"_\")[2])\n",
    "    checkLog[\"server_account\"] = checkLog[\"server\"] + '_' + checkLog['accCode'].astype('str')\n",
    "    df = checkLog[(checkLog['ars']%10 == 1)]\n",
    "\n",
    "    df['tradeNtl'] = df['tradePrice']*df['absFilledThisUpdate']\n",
    "    df[\"buyNtl\"] = np.where(~df[\"buyRet\"].isnull(), df[\"tradeNtl\"], np.nan)\n",
    "    df[\"sellNtl\"] = np.where(~df[\"sellRet\"].isnull(), df[\"tradeNtl\"], np.nan)\n",
    "    df[\"sumbuyNtl\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"buyNtl\"].transform(sum)\n",
    "    df[\"sumsellNtl\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"sellNtl\"].transform(sum)\n",
    "\n",
    "    df[\"sumsellRet\"] = df[\"tradeNtl\"] * df[\"sellRet\"]\n",
    "    df[\"sumsellRet\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"sumsellRet\"].transform(sum)\n",
    "\n",
    "\n",
    "    df[\"sumbuyRet\"] = df[\"tradeNtl\"] * df[\"buyRet\"]\n",
    "    df[\"sumbuyRet\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"sumbuyRet\"].transform(sum)\n",
    "\n",
    "    df[\"buyRet\"] = df[\"sumbuyRet\"] / df[\"sumbuyNtl\"]\n",
    "    df[\"sellRet\"] = df[\"sumsellRet\"] / df[\"sumsellNtl\"]\n",
    "    df[\"buyOrderNum\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"buyNum\"].transform(sum)\n",
    "    df[\"sellOrderNum\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"sellNum\"].transform(sum)\n",
    "\n",
    "\n",
    "    from IPython.display import display, HTML\n",
    "    for col in [\"buyRet\", \"sellRet\"]:\n",
    "        df[col] = (df[col] * 10000).round(2)\n",
    "\n",
    "    re = df.groupby([\"exchange\", \"date\", \"sta\", \"server_account\"])[\"buyOrderNum\", \"buyRet\", \"sellOrderNum\", \"sellRet\"].first().reset_index()\n",
    "    re1 = re[(re['sellRet'] < -20) & (re['sellOrderNum'] > 100)]\n",
    "    re2 = re[(re['buyRet'] < -20) & (re['buyOrderNum'] > 100)]\n",
    "\n",
    "    display(HTML(re1.groupby([\"exchange\", 'date', 'sta', \"server_account\"])[['sellOrderNum', 'sellRet']].first().to_html()))\n",
    "    display(HTML(re2.groupby([\"exchange\", 'date', 'sta', \"server_account\"])[['buyOrderNum', 'buyRet']].first().to_html()))\n",
    "    pr1 = re1[re1['date'] == checkLog['date'].max()].groupby([\"exchange\", 'date', 'sta', \"server_account\"])[['sellOrderNum', 'sellRet']].first().reset_index()\n",
    "    pr2 = re2[re2['date'] == checkLog['date'].max()].groupby([\"exchange\", 'date', 'sta', \"server_account\"])[['buyOrderNum', 'buyRet']].first().reset_index()\n",
    "\n",
    "    if pr1.shape[0] != 0:\n",
    "        body += '<div>In the following cases, sell order return under given strategy and accCode < -20bps:<div>'\n",
    "        result = [pr1['exchange'].values, pr1['date'].values, pr1['sta'].values,\n",
    "                 pr1['server_account'].values, pr1['sellOrderNum'].values, pr1['sellRet'].values]\n",
    "        title = ['exchange', 'date', 'sta', 'server_account', 'sellOrderNum', 'sellRet']\n",
    "        body += convertToHtml(result,title)\n",
    "\n",
    "    if pr2.shape[0] != 0:\n",
    "        body += '<div>In the following cases, buy order return under given strategy and accCode < -20bps:<div>'\n",
    "        result = [pr2['exchange'].values, pr2['date'].values, pr2['sta'].values,\n",
    "                 pr2['server_account'].values, pr2['buyOrderNum'].values, pr2['buyRet'].values]\n",
    "        title = ['exchange', 'date', 'sta', 'server_account', 'buyOrderNum', 'buyRet']\n",
    "        body += convertToHtml(result,title)\n",
    "\n",
    "    pr1 = re1.groupby(['exchange', 'sta', 'server_account'])['date'].size()[\n",
    "        re1.groupby(['exchange', 'sta', 'server_account'])['date'].size() >= 2].reset_index().sort_values(by='date', ascending=False)\n",
    "    pr2 = re2.groupby(['exchange', 'sta', 'server_account'])['date'].size()[\n",
    "        re2.groupby(['exchange', 'sta', 'server_account'])['date'].size() >= 2].reset_index().sort_values(by='date', ascending=False)\n",
    "    add1 = pd.merge(re, pr1, on=['exchange', 'sta', 'server_account'], how='inner')\n",
    "    add1 = add1[(add1['sellRet'] < -20) & (add1['sellOrderNum'] > 100)]\n",
    "    add2 = pd.merge(re, pr2, on=['exchange', 'sta', 'server_account'], how='inner')\n",
    "    add2 = add2[(add2['buyRet'] < -20) & (add2['buyOrderNum'] > 100)]\n",
    "    \n",
    "    if add1.shape[0] != 0:\n",
    "        add1 = add1.groupby(['exchange', 'sta', 'server_account'])['sellRet'].describe()[['count', 'mean', 'min', 'max']].reset_index().sort_values(by='count', ascending=False)\n",
    "        for i in ['mean', 'min', 'max']:\n",
    "            add1[i] = add1[i].round(2)\n",
    "        for i in ['count']:\n",
    "            add1[i] = add1[i].astype(int)\n",
    "        body += '<div>In the following cases, more than 2 days sell order return under given strategy and accCode < -20bps in previous 10 days (include today):<div>'\n",
    "        result = [add1['exchange'].values, add1['sta'].values,\n",
    "                 add1['server_account'].values, add1['count'].values, add1['mean'].values, add1['min'].values, add1['max'].values]\n",
    "        title = ['exchange', 'sta', 'server_account', 'count', 'mean', 'min', 'max']\n",
    "        body += convertToHtml(result,title)\n",
    "\n",
    "    if add2.shape[0] != 0:\n",
    "        add2 = add2.groupby(['exchange', 'sta', 'server_account'])['buyRet'].describe()[['count', 'mean', 'min', 'max']].reset_index().sort_values(by='count', ascending=False)\n",
    "        for i in ['mean', 'min', 'max']:\n",
    "            add2[i] = add2[i].round(2)\n",
    "        for i in ['count']:\n",
    "            add2[i] = add2[i].astype(int)\n",
    "        body += '<div>In the following cases, more than 2 days buy order return under given strategy and accCode < -20bps in previous 10 days (include today):<div>'\n",
    "        result = [add2['exchange'].values, add2['sta'].values,\n",
    "                 add2['server_account'].values, add2['count'].values, add2['mean'].values, add2['min'].values, add2['max'].values]\n",
    "        title = ['exchange', 'sta', 'server_account', 'count', 'mean', 'min', 'max']\n",
    "        body += convertToHtml(result,title)\n",
    "    \n",
    "    \n",
    "    import smtplib\n",
    "    from email.mime.text import MIMEText\n",
    "    from email.mime.multipart import MIMEMultipart    \n",
    "    \n",
    "    title = str(orderLog['date'].max()) + ' daily report'\n",
    "    body = body + \"</div></body></html>\"\n",
    "    smtp_server = '42.120.226.4' # 'smtp.mxhichina.com'\n",
    "    user = 'zhenyu.yin@general-int.com'\n",
    "    passwd = 'Yqzy0063!'\n",
    "    from_addr = 'zhenyu.yin@general-int.com'\n",
    "    to_addr = ['zhenyu.yin@general-int.com', 'kevin.zhang@general-int.com']\n",
    "#     to_addr = ['zhenyu.yin@general-int.com']\n",
    "\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = from_addr\n",
    "    msg['To'] = ', '.join(to_addr)\n",
    "    msg['Subject'] = title\n",
    "    txt = MIMEText(body, _subtype='html', _charset='UTF-8')\n",
    "    msg.attach(txt)\n",
    "\n",
    "    smtp = None\n",
    "    while True:\n",
    "        try:\n",
    "            smtp = smtplib.SMTP(smtp_server)\n",
    "            print('smtp server connected')\n",
    "            smtp.login(user, passwd)\n",
    "            print('login')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    print('send mail')\n",
    "    smtp.sendmail(from_addr, to_addr, msg.as_string())\n",
    "    smtp.quit()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# from twisted.internet import task, reactor\n",
    "# import schedule\n",
    "\n",
    "\n",
    "# def sleeptime(hour,min,sec):\n",
    "#     return hour*3600 + min*60 + sec\n",
    "\n",
    "\n",
    "# class Test(object):\n",
    "#     def __init__(self):\n",
    "#         self.status = True        \n",
    "#     def test(self):\n",
    "#         while self.status == True:\n",
    "#             try:\n",
    "#                 print(datetime.datetime.now())\n",
    "#                 date = (datetime.datetime.today()).strftime('%Y%m%d')\n",
    "# #                 date = '20210108'\n",
    "#                 readPath = 'L:\\\\orderLog\\\\data\\\\***'\n",
    "#                 dataPathLs = np.array(glob.glob(readPath))\n",
    "#                 dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "#                 assert(np.max(dateLs) == int(date))\n",
    "#                 print('We start to generate report now')\n",
    "#                 generate_report(date)\n",
    "#                 self.status = False\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 print('still wait for data coming')  \n",
    "#                 second = sleeptime(0,5,0)\n",
    "#                 time.sleep(second)\n",
    "\n",
    "# test1 = Test()\n",
    "# schedule.every().day.at(\"18:22\").do(test1.test)\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "-----------------------------------------------------------------------------------------------\n",
      "load data\n",
      "L:\\orderLog\\data\\20201228.pkl\n",
      "L:\\orderLog\\data\\20201229.pkl\n",
      "L:\\orderLog\\data\\20201230.pkl\n",
      "L:\\orderLog\\data\\20201231.pkl\n",
      "L:\\orderLog\\data\\20210104.pkl\n",
      "L:\\orderLog\\data\\20210105.pkl\n",
      "L:\\orderLog\\data\\20210106.pkl\n",
      "L:\\orderLog\\data\\20210107.pkl\n",
      "L:\\orderLog\\data\\20210108.pkl\n",
      "L:\\orderLog\\data\\20210111.pkl\n",
      "L:\\orderLog\\data\\20210112.pkl\n",
      "There are ticks with orderDirection 0\n",
      "             date      colo  accCode    secid  vai  updateType      sdd  \\\n",
      "314035   20201228  zs_66_01     6634  2000007   -1           1  49597.0   \n",
      "342328   20201228  zs_52_09     5291  2000553   -1           1  53524.0   \n",
      "347596   20201228  zs_54_01     5474  2000608   -1           1  47253.0   \n",
      "349238   20201228  zs_54_01     5474  2000619   -1           7  47362.0   \n",
      "423299   20201228  zs_96_08     9685  2002100   -1           1  48345.0   \n",
      "...           ...       ...      ...      ...  ...         ...      ...   \n",
      "9522286  20210112  zs_64_01     6480  2300578   -1           1     -1.0   \n",
      "9576821  20210112  zs_96_08     9685  2300724   -1           7  40220.0   \n",
      "9587065  20210112  zs_64_01     6480  2300753   -1           1     -1.0   \n",
      "9597083  20210112  zs_64_01     6480  2300780    0           1      NaN   \n",
      "9622407  20210112  zs_66_01     6634  2300853   -1           1  53798.0   \n",
      "\n",
      "         orderDirection  absOrderSize  internalId       orderId  \n",
      "314035                0             0        -1.0  1.812811e+10  \n",
      "342328                0             0        -1.0  1.854590e+06  \n",
      "347596                0             0        -1.0  8.689127e+08  \n",
      "349238                0             0        -1.0  8.689128e+08  \n",
      "423299                0             0        -1.0  7.221449e+17  \n",
      "...                 ...           ...         ...           ...  \n",
      "9522286               0             0        -1.0  3.629200e+04  \n",
      "9576821               0             0        -1.0 -1.000000e+00  \n",
      "9587065               0             0        -1.0  4.117100e+04  \n",
      "9597083               0             0        -1.0  3.489100e+04  \n",
      "9622407               0             0        -1.0  1.812813e+10  \n",
      "\n",
      "[213 rows x 11 columns]\n",
      "There are orders with all things same except sdd\n",
      "         Unnamed: 0  ApplSeqNum       aaa  absFilledThisUpdate  absOrderSize  \\\n",
      "1361311    466842.0   1444460.0  0.000687                    0           100   \n",
      "1361371    466902.0   9795909.0  0.000694                    0           100   \n",
      "\n",
      "         absOrderSizeCumFilled  accCode  ars             caamd  \\\n",
      "1361311                      0   966301  1.0  1609205575014635   \n",
      "1361371                      0   966301  1.0  1609208372125359   \n",
      "\n",
      "         cancellationPending        cfe         clock    clockAtArrival  \\\n",
      "1361311                  0.0  128034.25  1.609206e+15  1609205575014666   \n",
      "1361371                  0.0   52600.79  1.609208e+15  1609208372125429   \n",
      "\n",
      "         cumSharesBought  cumSharesBuyInserted  cumSharesSellInserted  \\\n",
      "1361311              0.0                   0.0                  300.0   \n",
      "1361371              0.0                   0.0                  600.0   \n",
      "\n",
      "         cumSharesSold      date  finalState        gfe       hee  \\\n",
      "1361311          200.0  20201229         0.0  128033.25  0.000504   \n",
      "1361371          400.0  20201229         0.0   52599.79 -0.000102   \n",
      "\n",
      "         insertedShortOrder  insertionPending  internalId  inv_L  inv_L0  \\\n",
      "1361311                 0.0               1.0         9.0  700.0     0.0   \n",
      "1361371                 0.0               1.0         9.0  100.0     0.0   \n",
      "\n",
      "         inv_S  inv_S0            l4algoDebug  l4tr  locateShares  \\\n",
      "1361311    0.0     0.0                    NaN   0.0           0.0   \n",
      "1361371    0.0     0.0  1015.000000|2.000000|   0.0           0.0   \n",
      "\n",
      "         locateSharesTotal  mfe  mra100  mrb100       mrm     mrm25  mrmum  \\\n",
      "1361311                0.0 -1.0  2765.0  2764.0 -0.000118  -0.00139   -1.0   \n",
      "1361371                0.0 -1.0  2774.0  2771.0  0.000174 -0.002573   -1.0   \n",
      "\n",
      "         mrrlma mrsb300    mrsb90  mrss300    mrss90  mrstaat  mrstauc  \\\n",
      "1361311    -1.0      -1 -0.001033     -1.0  0.000687   1000.0      0.0   \n",
      "1361371    -1.0      -1 -0.001770     -1.0  0.000694   1000.0      0.0   \n",
      "\n",
      "         mrstaum        mrv               ms  mse   mt  mta   mv  \\\n",
      "1361311     -1.0   140020.0  09:32:55.014196  100  0.0 -999  0.0   \n",
      "1361371     -1.0  1976879.0  10:19:32.125029  100  0.0 -999  0.0   \n",
      "\n",
      "         orderDirection  orderId  orderOutstanding  orderPrice orderSysId  \\\n",
      "1361311              -1      9.0               0.0       27.64        NaN   \n",
      "1361371              -1     -1.0               0.0       27.71        NaN   \n",
      "\n",
      "         resa          sdd    secid  sequenceNo  session  threadId  \\\n",
      "1361311   2.0   93255060.0  2002595   8271885.0        5   29229.0   \n",
      "1361371   2.0  101932190.0  2002595  56341419.0        1   29306.0   \n",
      "\n",
      "         totalActions  totalCanceled tradeId  tradePrice  underlyingIndex  \\\n",
      "1361311           7.0            0.0     NaN        -1.0              905   \n",
      "1361371          10.0            1.0     NaN        -1.0              905   \n",
      "\n",
      "         updateType      vai                            zipFile      colo  \\\n",
      "1361311           0   140020  logs_20201229_zt_96_01_day_966301  zt_96_01   \n",
      "1361371           0  1976879  logs_20201229_zt_96_01_day_966301  zt_96_01   \n",
      "\n",
      "         caa_orderLog    start_time     Price  OrderQty  Side statusLs  \\\n",
      "1361311  1.609206e+15  1.609206e+15  276400.0     100.0   2.0   (0, 4)   \n",
      "1361371           NaN           NaN       NaN       NaN   NaN      NaN   \n",
      "\n",
      "        TradePriceLs TradeQtyLs   beta_60  adjMid_F30s  adjMid_F90s  \\\n",
      "1361311  (0, 276400)   (0, 100)  0.581536    27.643913    27.627500   \n",
      "1361371          NaN        NaN  0.581536    27.649275    27.629737   \n",
      "\n",
      "         adjMid_F300s  indexClose  indexClose_F30s  indexClose_F90s  \\\n",
      "1361311     27.473333   6239.2341        6241.7701        6238.8906   \n",
      "1361371     27.631667   6235.4744        6234.7954        6229.9708   \n",
      "\n",
      "         indexClose_F300s  \n",
      "1361311         6226.1042  \n",
      "1361371         6237.2874  \n",
      "There are orders with same internalId but different orderId other than accCode 8856 case\n",
      "date      colo      accCode  secid    orderDirection  absOrderSize  internalId\n",
      "20201228  zs_54_01  5456     2300036  -1              1000           2093.0       2\n",
      "20201229  zt_96_01  966301   1600376   0              0             -1.0          3\n",
      "                             2002595  -1              100            9.0          2\n",
      "Name: orderId, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are orders in 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "     secid  accCode      colo      vai  updateType         sdd  internalId  \\\n",
      "0  2300271     8856  zs_88_04  22400.0           0  93007340.0        30.0   \n",
      "1  2300271     8856  zs_88_04  22400.0           2        -1.0        30.0   \n",
      "2  2300271     8856  zs_88_04  22400.0           1     34209.0        30.0   \n",
      "3  2300271     8856  zs_88_04  22400.0           3        -1.0        30.0   \n",
      "4  2300271     8856  zs_88_04  22400.0           0  93009860.0        34.0   \n",
      "5  2300271     8856  zs_88_04  22400.0           2        -1.0        34.0   \n",
      "6  2300271     8856  zs_88_04  22400.0           2        -1.0        34.0   \n",
      "\n",
      "   orderId  absOrderSize  absFilledThisUpdate  absOrderSizeCumFilled  \\\n",
      "0     30.0           100                    0                      0   \n",
      "1     30.0           100                    0                      0   \n",
      "2     30.0           100                    0                      0   \n",
      "3     30.0           100                    0                      0   \n",
      "4     34.0           100                    0                      0   \n",
      "5     34.0           100                    0                      0   \n",
      "6     34.0           100                  100                    100   \n",
      "\n",
      "   orderPrice  tradePrice  \n",
      "0       24.73        -1.0  \n",
      "1       24.73        -1.0  \n",
      "2       24.73        -1.0  \n",
      "3       24.73        -1.0  \n",
      "4       24.70        -1.0  \n",
      "5       24.70        -1.0  \n",
      "6       24.70        24.7  \n",
      "There are orders out of 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "     secid  accCode      colo        vai  updateType          sdd  internalId  \\\n",
      "0  2300036     5456  zs_54_01  2652411.0           0  112946610.0      2093.0   \n",
      "1  2300036     5456  zs_54_01  2652411.0           2         -1.0      2093.0   \n",
      "2  2300036     5456  zs_54_01  2652411.0           1      46800.0      2093.0   \n",
      "3  2300036     5456  zs_54_01  2652411.0           3         -1.0      2093.0   \n",
      "\n",
      "       orderId  absOrderSize  absFilledThisUpdate  absOrderSizeCumFilled  \\\n",
      "0         -1.0          1000                    0                      0   \n",
      "1  868912271.0          1000                    0                      0   \n",
      "2       2093.0          1000                    0                      0   \n",
      "3       2093.0          1000                    0                      0   \n",
      "\n",
      "   orderPrice  tradePrice  \n",
      "0       17.63        -1.0  \n",
      "1       17.63        -1.0  \n",
      "2       17.63        -1.0  \n",
      "3       17.63        -1.0  \n",
      "=======================================================================================\n",
      "1. same date, secid, vai: same direction\n",
      "orders with abnormal ars values\n",
      "ars\n",
      "0.0    9\n",
      "Name: date, dtype: int64\n",
      "accCode\n",
      "6623    2\n",
      "9765    1\n",
      "9758    1\n",
      "9685    1\n",
      "9471    1\n",
      "9454    1\n",
      "9435    1\n",
      "6683    1\n",
      "6634    1\n",
      "6631    1\n",
      "6627    1\n",
      "5470    1\n",
      "5291    1\n",
      "5232    1\n",
      "Name: date, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:165: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opposite direction for same date, same secid, same vai\n",
      "accCode\n",
      "5222      1\n",
      "5225      1\n",
      "5230      2\n",
      "5232      1\n",
      "5289      1\n",
      "5290      3\n",
      "5291      2\n",
      "5326      2\n",
      "5329      2\n",
      "5377      1\n",
      "5384      2\n",
      "5386      1\n",
      "5470      2\n",
      "5474      2\n",
      "6480      3\n",
      "6623      2\n",
      "6627      4\n",
      "6631      1\n",
      "6634      2\n",
      "6878      1\n",
      "8924      1\n",
      "8943      2\n",
      "8970      1\n",
      "9208      1\n",
      "9243      1\n",
      "9248      3\n",
      "9448      5\n",
      "9454      1\n",
      "9461      1\n",
      "9471      1\n",
      "9551      1\n",
      "9655      2\n",
      "9741      1\n",
      "9754      2\n",
      "9756      1\n",
      "9765      2\n",
      "522501    1\n",
      "523101    2\n",
      "527101    3\n",
      "528101    1\n",
      "528701    2\n",
      "529001    1\n",
      "529101    1\n",
      "897102    2\n",
      "974102    2\n",
      "975601    1\n",
      "Name: orderDirection, dtype: int64\n",
      "=======================================================================================\n",
      "2. same date, secid, vai, accCode: one insertion\n",
      "more than one insertion at same time\n",
      "             date      colo  accCode    secid        vai          sdd  \\\n",
      "1797851  20210108  zs_96_08     6237  1603317  1375708.0  103134000.0   \n",
      "\n",
      "         clockAtArrival  \n",
      "1797851               2  \n",
      "date\n",
      "20210108    1\n",
      "Name: accCode, dtype: int64\n",
      "date      accCode\n",
      "20210108  6237       1\n",
      "Name: sdd, dtype: int64\n",
      "=======================================================================================\n",
      "3. IPO stocks selling (ars = 301, 302)\n",
      "=======================================================================================\n",
      "4. updateType 7 orders\n",
      "=======================================================================================\n",
      "5. updateType 6 orders\n",
      "=======================================================================================\n",
      "6. CYB stocks order size < 30w\n",
      "=======================================================================================\n",
      "7. unexpected updateType\n",
      "updateType\n",
      "(0, 2, 1, 1, 3)                     14\n",
      "(0, 2, 1, 3, 4)                     33\n",
      "(0, 2, 1, 4, 1, 3)                   1\n",
      "(0, 2, 1, 4, 3, 4)                   5\n",
      "(0, 2, 1, 6)                         2\n",
      "(0, 2, 2, 2)                        64\n",
      "(0, 2, 2, 2, 1, 3)                   2\n",
      "(0, 2, 2, 2, 2)                      7\n",
      "(0, 2, 2, 2, 2, 2)                   1\n",
      "(0, 2, 2, 2, 2, 2, 2)                2\n",
      "(0, 2, 2, 2, 2, 2, 2, 2, 1, 3)       1\n",
      "(0, 2, 4, 1)                        14\n",
      "(0, 2, 4, 1, 1, 3)                   2\n",
      "(0, 2, 4, 1, 3, 4)                  24\n",
      "(0, 2, 4, 1, 4, 3, 4)                6\n",
      "(0, 2, 4, 3)                         6\n",
      "(0, 2, 6)                          114\n",
      "(0, 6)                            1046\n",
      "(0, 8)                              34\n",
      "Name: order, dtype: int64\n",
      "=======================================================================================\n",
      "8. status == 0: all traded\n",
      "in total trade, any fill != total cases\n",
      "             date    order  filled  total\n",
      "886      20201228     1331    2100   3900\n",
      "3866     20201228     6069     100    200\n",
      "11090    20201228    16279    1200   1500\n",
      "15559    20201228    22472    3700   4600\n",
      "15927    20201228    22931    5200   6200\n",
      "...           ...      ...     ...    ...\n",
      "1757833  20210112  2333266     200    300\n",
      "1766327  20210112  2345941     100    200\n",
      "1768105  20210112  2348795     300    900\n",
      "1776523  20210112  2361478     300    700\n",
      "1789335  20210112  2379003   30311  33000\n",
      "\n",
      "[303 rows x 4 columns]\n",
      "=======================================================================================\n",
      "9. status == 1: partial traded\n",
      "in partial trade, any fill >= total or fill is 0 cases for updateType 4\n",
      "Empty DataFrame\n",
      "Columns: [date, order, filled, total]\n",
      "Index: []\n",
      "=======================================================================================\n",
      "10. no cancellation within 1 sec\n",
      "any cancellation within 1 sec\n",
      "         Unnamed: 0  ApplSeqNum aaa  absFilledThisUpdate  absOrderSize  \\\n",
      "2262518    148575.0         0.0  -1                    0           100   \n",
      "2262630    189358.0         0.0  -1                    0           100   \n",
      "2262661    195686.0         0.0  -1                    0           200   \n",
      "2262732    205040.0         0.0  -1                    0           100   \n",
      "2262803    213178.0         0.0  -1                    0           100   \n",
      "...             ...         ...  ..                  ...           ...   \n",
      "4075488    530282.0         0.0  -1                    0           200   \n",
      "4075527    547850.0         0.0  -1                    0           900   \n",
      "4075531    547853.0         0.0  -1                    0           900   \n",
      "4075567    574317.0         0.0  -1                    0           100   \n",
      "4075627    652581.0         0.0  -1                    0          1300   \n",
      "\n",
      "         absOrderSizeCumFilled  accCode  ars             caamd  \\\n",
      "2262518                      0     8854  1.0  1609307843130063   \n",
      "2262630                      0     8854  1.0  1609298373762077   \n",
      "2262661                      0     8854  1.0  1609296809192182   \n",
      "2262732                      0     8854  1.0  1609306975678291   \n",
      "2262803                      0     8854  1.0  1609307131162699   \n",
      "...                        ...      ...  ...               ...   \n",
      "4075488                      0   966301  2.0  1609723936630655   \n",
      "4075527                      0   966301  2.0  1609723949533510   \n",
      "4075531                      0   966301  2.0  1609723950622143   \n",
      "4075567                      0   966301  1.0  1609724838773633   \n",
      "4075627                    800   966301  2.0  1609723930017591   \n",
      "\n",
      "         cancellationPending         cfe                      clock  \\\n",
      "2262518                  1.0  5423341.79 2020-12-30 13:57:23.309056   \n",
      "2262630                  1.0  4339395.87 2020-12-30 11:19:34.180024   \n",
      "2262661                  1.0  4226087.47 2020-12-30 10:53:29.509160   \n",
      "2262732                  1.0  5409570.09 2020-12-30 13:42:56.338860   \n",
      "2262803                  1.0  5418525.78 2020-12-30 13:45:31.933621   \n",
      "...                      ...         ...                        ...   \n",
      "4075488                  1.0    53455.40 2021-01-04 09:32:17.501808   \n",
      "4075527                  1.0    58174.40 2021-01-04 09:32:30.495846   \n",
      "4075531                  1.0    58174.40 2021-01-04 09:32:30.914660   \n",
      "4075567                  1.0    34669.75 2021-01-04 09:47:19.569383   \n",
      "4075627                  1.0    29716.18 2021-01-04 09:32:10.278895   \n",
      "\n",
      "           clockAtArrival  cumSharesBought  cumSharesBuyInserted  \\\n",
      "2262518  1609307843309056              0.0                   0.0   \n",
      "2262630  1609298374180024              0.0                   0.0   \n",
      "2262661  1609296809509160            600.0                 800.0   \n",
      "2262732  1609306976338860              0.0                   0.0   \n",
      "2262803  1609307131933621           2800.0                5600.0   \n",
      "...                   ...              ...                   ...   \n",
      "4075488  1609723937501808              0.0                 200.0   \n",
      "4075527  1609723950495846              0.0                1800.0   \n",
      "4075531  1609723950914660              0.0                2700.0   \n",
      "4075567  1609724839569383              0.0                 100.0   \n",
      "4075627  1609723930278895            800.0                1300.0   \n",
      "\n",
      "         cumSharesSellInserted  cumSharesSold      date  finalState  \\\n",
      "2262518                  100.0            0.0  20201230         0.0   \n",
      "2262630                  200.0          100.0  20201230         0.0   \n",
      "2262661                    0.0            0.0  20201230         0.0   \n",
      "2262732                  100.0            0.0  20201230         0.0   \n",
      "2262803                    0.0            0.0  20201230         0.0   \n",
      "...                        ...            ...       ...         ...   \n",
      "4075488                    0.0            0.0  20210104         0.0   \n",
      "4075527                    0.0            0.0  20210104         0.0   \n",
      "4075531                    0.0            0.0  20210104         0.0   \n",
      "4075567                    0.0            0.0  20210104         0.0   \n",
      "4075627                    0.0            0.0  20210104         0.0   \n",
      "\n",
      "                gfe  hee  insertedShortOrder  insertionPending  internalId  \\\n",
      "2262518  5423340.79 -1.0                 0.0               0.0       190.0   \n",
      "2262630  4339394.87 -1.0                 0.0               0.0       287.0   \n",
      "2262661  4226086.47 -1.0                 0.0               0.0       201.0   \n",
      "2262732  5409569.09 -1.0                 0.0               0.0       152.0   \n",
      "2262803  5418524.78 -1.0                 0.0               0.0       170.0   \n",
      "...             ...  ...                 ...               ...         ...   \n",
      "4075488    53454.40 -1.0                 0.0               0.0        20.0   \n",
      "4075527    58173.40 -1.0                 0.0               0.0        27.0   \n",
      "4075531    58173.40 -1.0                 0.0               0.0        28.0   \n",
      "4075567    34668.75 -1.0                 0.0               0.0        80.0   \n",
      "4075627    29715.18 -1.0                 0.0               0.0         5.0   \n",
      "\n",
      "          inv_L  inv_L0  inv_S  inv_S0   l4algoDebug  l4tr  locateShares  \\\n",
      "2262518   100.0     0.0    0.0     0.0  1345.000000|   0.0           0.0   \n",
      "2262630  6700.0     0.0    0.0     0.0  1115.000000|   0.0           0.0   \n",
      "2262661   600.0   600.0    0.0     0.0  1045.000000|   0.0           0.0   \n",
      "2262732  4250.0     0.0    0.0     0.0  1330.000000|   0.0           0.0   \n",
      "2262803  5710.0  5700.0    0.0     0.0  1345.000000|   0.0           0.0   \n",
      "...         ...     ...    ...     ...           ...   ...           ...   \n",
      "4075488     0.0     0.0    0.0     0.0   932.000000|   0.0           0.0   \n",
      "4075527     0.0     0.0    0.0     0.0   932.000000|   0.0           0.0   \n",
      "4075531     0.0     0.0    0.0     0.0   932.000000|   0.0           0.0   \n",
      "4075567     0.0     0.0    0.0     0.0   945.000000|   0.0           0.0   \n",
      "4075627   800.0   800.0    0.0     0.0   932.000000|   0.0           0.0   \n",
      "\n",
      "         locateSharesTotal  mfe   mra100   mrb100       mrm     mrm25  mrmum  \\\n",
      "2262518                0.0 -1.0   1957.0   1951.0 -0.004078  0.000603   -1.0   \n",
      "2262630                0.0 -1.0   3494.0   3475.0 -0.000817  0.002538   -1.0   \n",
      "2262661                0.0 -1.0   7262.0   7261.0  0.001725  0.004244   -1.0   \n",
      "2262732                0.0 -1.0   6699.0   6637.0  0.002238  -6.9e-05   -1.0   \n",
      "2262803                0.0 -1.0   1559.0   1552.0  0.000841  0.003929   -1.0   \n",
      "...                    ...  ...      ...      ...       ...       ...    ...   \n",
      "4075488                0.0 -1.0  10561.0  10504.0 -0.001579  0.005775   -1.0   \n",
      "4075527                0.0 -1.0   2645.0   2643.0 -0.000005  0.004194   -1.0   \n",
      "4075531                0.0 -1.0   2645.0   2639.0 -0.000018  0.004194   -1.0   \n",
      "4075567                0.0 -1.0  14511.0  14507.0  0.001088  0.002808   -1.0   \n",
      "4075627                0.0 -1.0   2408.0   2406.0  0.000951  0.003843   -1.0   \n",
      "\n",
      "         mrrlma   mrsb300    mrsb90   mrss300    mrss90  mrstaat  mrstauc  \\\n",
      "2262518    -1.0 -0.003175 -0.001534  0.000110 -0.001057  13000.0  11000.0   \n",
      "2262630    -1.0 -0.002911 -0.003110 -0.002534 -0.002361  13000.0  11000.0   \n",
      "2262661    -1.0  0.001422  0.000137 -0.001558 -0.000264  13000.0  11000.0   \n",
      "2262732    -1.0 -0.004424 -0.006091 -0.004853 -0.002701  13000.0  11000.0   \n",
      "2262803    -1.0 -0.001153 -0.001560 -0.003341 -0.002308  13000.0  11000.0   \n",
      "...         ...       ...       ...       ...       ...      ...      ...   \n",
      "4075488    -1.0        -1 -0.002121 -1.000000 -0.003390   1000.0      0.0   \n",
      "4075527    -1.0        -1 -0.001647 -1.000000  0.000912   1000.0      0.0   \n",
      "4075531    -1.0        -1 -0.002272 -1.000000  0.000001   1000.0      0.0   \n",
      "4075567    -1.0        -1  0.000265 -1.000000 -0.000506   1000.0      0.0   \n",
      "4075627    -1.0        -1 -0.000592 -1.000000 -0.000350   1000.0      0.0   \n",
      "\n",
      "         mrstaum         mrv               ms  mse   mt mta   mv  \\\n",
      "2262518     -1.0  17761035.0  13:57:23.307447   22 -1.0  -1  0.0   \n",
      "2262630     -1.0   3483963.0  11:19:34.178594   22 -1.0  -1  0.0   \n",
      "2262661     -1.0   1703499.0  10:53:29.507618   22 -1.0  -1  0.0   \n",
      "2262732     -1.0   6335649.0  13:42:56.337460   22 -1.0  -1  0.0   \n",
      "2262803     -1.0   1747688.0  13:45:31.932458   22 -1.0  -1  0.0   \n",
      "...          ...         ...              ...  ...  ...  ..  ...   \n",
      "4075488     -1.0     89807.0  09:32:17.501095  100 -1.0  -1  0.0   \n",
      "4075527     -1.0    221900.0  09:32:30.495095  100 -1.0  -1  0.0   \n",
      "4075531     -1.0    222000.0  09:32:30.914095  100 -1.0  -1  0.0   \n",
      "4075567     -1.0    229400.0  09:47:19.569041  100 -1.0  -1  0.0   \n",
      "4075627     -1.0   2887159.0  09:32:10.278096  100 -1.0  -1  0.0   \n",
      "\n",
      "         orderDirection       orderId  orderOutstanding  orderPrice  \\\n",
      "2262518              -1  8.885000e+03               1.0       19.58   \n",
      "2262630              -1  6.477000e+03               1.0       34.96   \n",
      "2262661               1  5.459000e+03               1.0       72.54   \n",
      "2262732              -1  8.476000e+03               1.0       67.00   \n",
      "2262803               1  8.624000e+03               1.0       15.51   \n",
      "...                 ...           ...               ...         ...   \n",
      "4075488               1  7.213655e+17               1.0      105.03   \n",
      "4075527               1  7.213655e+17               1.0       26.38   \n",
      "4075531               1  7.213655e+17               1.0       26.38   \n",
      "4075567               1  7.213655e+17               1.0      144.96   \n",
      "4075627               1  7.213655e+17               1.0       24.05   \n",
      "\n",
      "                    orderSysId  resa          sdd    secid  sequenceNo  \\\n",
      "2262518            5.10089e+07   2.0  135722000.0  1603008  10463942.0   \n",
      "2262630            5.10065e+07   2.0  111933000.0  1603456   4386801.0   \n",
      "2262661            5.10055e+07   2.0  105328000.0  1603583   2334346.0   \n",
      "2262732            5.10085e+07   2.0  134254000.0  1603678   9310481.0   \n",
      "2262803            5.10086e+07   2.0  134530000.0  1603728   9527330.0   \n",
      "...                        ...   ...          ...      ...         ...   \n",
      "4075488  7.213654928407931e+17   2.0   93216690.0  2002568   8205452.0   \n",
      "4075527  7.213654928407931e+17   2.0   93229590.0  2002648   8582197.0   \n",
      "4075531  7.213654928407931e+17   2.0   93230680.0  2002648   8598538.0   \n",
      "4075567  7.213654928407932e+17   2.0   94718840.0  2002791  28116354.0   \n",
      "4075627  7.213654928407931e+17   2.0   93210080.0  2300146   7993595.0   \n",
      "\n",
      "         session  threadId  totalActions  totalCanceled tradeId  tradePrice  \\\n",
      "2262518        2  122924.0         571.0           42.0     NaN        -1.0   \n",
      "2262630        3  114097.0         307.0           20.0     NaN        -1.0   \n",
      "2262661        3  114104.0         213.0           12.0     NaN        -1.0   \n",
      "2262732        2  122930.0         530.0           39.0     NaN        -1.0   \n",
      "2262803        2  122924.0         551.0           41.0     NaN        -1.0   \n",
      "...          ...       ...           ...            ...     ...         ...   \n",
      "4075488        0     354.0          23.0            3.0     NaN        -1.0   \n",
      "4075527        0     354.0          33.0            6.0     NaN        -1.0   \n",
      "4075531        0     354.0          35.0            7.0     NaN        -1.0   \n",
      "4075567        0     347.0          97.0           17.0     NaN        -1.0   \n",
      "4075627        0     352.0           6.0            1.0     NaN        -1.0   \n",
      "\n",
      "         underlyingIndex  updateType         vai  \\\n",
      "2262518              852           1  17760235.0   \n",
      "2262630              852           1   3483663.0   \n",
      "2262661              852           1   1703299.0   \n",
      "2262732              852           1   6317849.0   \n",
      "2262803              852           1   1738888.0   \n",
      "...                  ...         ...         ...   \n",
      "4075488              905           1     89807.0   \n",
      "4075527              905           1    221900.0   \n",
      "4075531              905           1    222000.0   \n",
      "4075567              905           1    229300.0   \n",
      "4075627              905           1   2882959.0   \n",
      "\n",
      "                                   zipFile      colo  caa_orderLog  \\\n",
      "2262518    logs_20201230_zt_88_06_day_8854  zt_88_06           NaN   \n",
      "2262630    logs_20201230_zt_88_06_day_8854  zt_88_06           NaN   \n",
      "2262661    logs_20201230_zt_88_06_day_8854  zt_88_06           NaN   \n",
      "2262732    logs_20201230_zt_88_06_day_8854  zt_88_06           NaN   \n",
      "2262803    logs_20201230_zt_88_06_day_8854  zt_88_06           NaN   \n",
      "...                                    ...       ...           ...   \n",
      "4075488  logs_20210104_zt_96_01_day_966301  zt_96_01           NaN   \n",
      "4075527  logs_20210104_zt_96_01_day_966301  zt_96_01           NaN   \n",
      "4075531  logs_20210104_zt_96_01_day_966301  zt_96_01           NaN   \n",
      "4075567  logs_20210104_zt_96_01_day_966301  zt_96_01           NaN   \n",
      "4075627  logs_20210104_zt_96_01_day_966301  zt_96_01           NaN   \n",
      "\n",
      "         start_time  Price  OrderQty  Side statusLs TradePriceLs TradeQtyLs  \\\n",
      "2262518         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "2262630         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "2262661         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "2262732         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "2262803         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "...             ...    ...       ...   ...      ...          ...        ...   \n",
      "4075488         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "4075527         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "4075531         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "4075567         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "4075627         NaN    NaN       NaN   NaN      NaN          NaN        NaN   \n",
      "\n",
      "          beta_60  adjMid_F30s  adjMid_F90s  adjMid_F300s  indexClose  \\\n",
      "2262518  0.823186    19.564615    19.568571     19.830833   6527.2886   \n",
      "2262630  1.138497    34.924545    35.016154     34.854000   6534.0868   \n",
      "2262661  1.583953    72.606667    72.725000     73.204615   6526.7336   \n",
      "2262732  1.003676    66.483333    66.538750     66.908438   6523.0712   \n",
      "2262803  0.832495    15.549796    15.545113     15.560455   6524.4181   \n",
      "...           ...          ...          ...           ...         ...   \n",
      "4075488  0.405798   105.458824   105.917500    105.505000   6391.4393   \n",
      "4075527  0.653811    26.336667    26.395000     25.999286   6390.1339   \n",
      "4075531  0.653811    26.336667    26.395000     25.999286   6390.1339   \n",
      "4075567  0.861326   144.945000   144.833333    145.330000   6405.5747   \n",
      "4075627  0.400156    24.163182    24.185405     24.091569   6391.1441   \n",
      "\n",
      "         indexClose_F30s  indexClose_F90s  indexClose_F300s     test  broker  \\\n",
      "2262518        6527.3770        6527.7307         6532.0858   567341      88   \n",
      "2262630        6535.3699        6538.0923         6536.0611   567378      88   \n",
      "2262661        6528.0027        6531.0701         6533.7960   567387      88   \n",
      "2262732        6522.6492        6523.4124         6526.4314   567405      88   \n",
      "2262803        6524.2923        6525.1404         6526.1806   567415      88   \n",
      "...                  ...              ...               ...      ...     ...   \n",
      "4075488        6390.4400        6379.3929         6371.2586  1006909      96   \n",
      "4075527        6389.4658        6378.2599         6372.1196  1006920      96   \n",
      "4075531        6389.4658        6378.2599         6372.1196  1006921      96   \n",
      "4075567        6405.8063        6404.6290         6406.3410  1006932      96   \n",
      "4075627        6389.3186        6379.6238         6371.3220  1006950      96   \n",
      "\n",
      "        colo_broker    order   group        startClock  duration  \\\n",
      "2262518       zt_88   567878  282055  1609307843130266    178790   \n",
      "2262630       zt_88   567913  287957  1609298373762174    417850   \n",
      "2262661       zt_88   567921  288887  1609296809192395    316765   \n",
      "2262732       zt_88   567942  290330  1609306975678489    660371   \n",
      "2262803       zt_88   567965  291448  1609307131162941    770680   \n",
      "...             ...      ...     ...               ...       ...   \n",
      "4075488       zt_96  1008081  571375  1609723936630724    871084   \n",
      "4075527       zt_96  1008093  574237  1609723949533576    962270   \n",
      "4075531       zt_96  1008094  574238  1609723950622199    292461   \n",
      "4075567       zt_96  1008105  578288  1609724838773703    795680   \n",
      "4075627       zt_96  1008122  591103  1609723930017655    261240   \n",
      "\n",
      "         orderDirection1  directNum  isMsg  status  \n",
      "2262518               -1        1.0    0.0       2  \n",
      "2262630               -1        1.0    0.0       2  \n",
      "2262661                1        1.0    0.0       2  \n",
      "2262732               -1        1.0    0.0       2  \n",
      "2262803                1        1.0    0.0       2  \n",
      "...                  ...        ...    ...     ...  \n",
      "4075488                1        NaN    1.0       2  \n",
      "4075527                1        NaN    1.0       2  \n",
      "4075531                1        NaN    1.0       2  \n",
      "4075567                1        1.0    1.0       2  \n",
      "4075627                1        NaN    1.0       0  \n",
      "\n",
      "[119 rows x 99 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================\n",
      "11. Orders with size > 80w or notional > 800w\n",
      "date      accCode\n",
      "20201228  5269       1\n",
      "          687801     1\n",
      "          897002     1\n",
      "20201229  9448       1\n",
      "20201230  5289       1\n",
      "          9551       1\n",
      "          527101     1\n",
      "          968501     1\n",
      "20201231  5225       1\n",
      "          5273       1\n",
      "          6237       1\n",
      "          6634       2\n",
      "          8865       1\n",
      "          9435       1\n",
      "          9655       1\n",
      "          522501     1\n",
      "20210104  5470       1\n",
      "          522601     1\n",
      "          529001     1\n",
      "          966701     1\n",
      "20210105  5474       1\n",
      "          6627       1\n",
      "          8886       1\n",
      "          9448       1\n",
      "          522901     1\n",
      "20210106  5230       1\n",
      "          5275       1\n",
      "          5292       1\n",
      "          5386       1\n",
      "20210107  8943       1\n",
      "          9448       1\n",
      "20210108  9758       1\n",
      "          522501     1\n",
      "          527101     1\n",
      "          537403     1\n",
      "          968501     1\n",
      "          975601     1\n",
      "20210111  5292       1\n",
      "          523001     1\n",
      "20210112  5264       1\n",
      "          5332       1\n",
      "          8833       1\n",
      "          8865       1\n",
      "          8970       1\n",
      "          9471       1\n",
      "Name: secid, dtype: int64\n",
      "         Unnamed: 0  ApplSeqNum       aaa  absFilledThisUpdate  absOrderSize  \\\n",
      "8913317         NaN         NaN  0.000788                    0           900   \n",
      "9199230         NaN         NaN  0.001264                    0           200   \n",
      "9261373         NaN         NaN  0.001931                    0           100   \n",
      "9355676         NaN         NaN  0.002255                    0           100   \n",
      "9413368         NaN         NaN  0.001927                    0          1200   \n",
      "9557655         NaN         NaN  0.002713                    0           600   \n",
      "\n",
      "         absOrderSizeCumFilled  accCode  ars             caamd  \\\n",
      "8913317                      0     5264  1.0  1610433912394328   \n",
      "9199230                      0     8970  1.0  1610434598296798   \n",
      "9261373                      0     5332  1.0  1610434564761950   \n",
      "9355676                      0     8833  1.0  1610434594240143   \n",
      "9413368                      0     8865  1.0  1610434591107242   \n",
      "9557655                      0     9471  1.0  1610434598914496   \n",
      "\n",
      "         cancellationPending           cfe                      clock  \\\n",
      "8913317                  0.0  2.777256e+05 2021-01-12 14:45:12.395283   \n",
      "9199230                  0.0  1.016735e+06 2021-01-12 14:56:38.296847   \n",
      "9261373                  0.0  4.978369e+05 2021-01-12 14:56:04.762024   \n",
      "9355676                  0.0  4.053538e+04 2021-01-12 14:56:34.240197   \n",
      "9413368                  0.0  2.047160e+05 2021-01-12 14:56:31.107295   \n",
      "9557655                  0.0  9.095035e+06 2021-01-12 14:56:38.914553   \n",
      "\n",
      "           clockAtArrival  cumSharesBought  cumSharesBuyInserted  \\\n",
      "8913317  1610433912395283              NaN                   NaN   \n",
      "9199230  1610434598296847              NaN                   NaN   \n",
      "9261373  1610434564762024              NaN                   NaN   \n",
      "9355676  1610434594240197              NaN                   NaN   \n",
      "9413368  1610434591107295              NaN                   NaN   \n",
      "9557655  1610434598914553              NaN                   NaN   \n",
      "\n",
      "         cumSharesSellInserted  cumSharesSold      date  finalState  \\\n",
      "8913317                    NaN            NaN  20210112         0.0   \n",
      "9199230                    NaN            NaN  20210112         0.0   \n",
      "9261373                    NaN            NaN  20210112         0.0   \n",
      "9355676                    NaN            NaN  20210112         0.0   \n",
      "9413368                    NaN            NaN  20210112         0.0   \n",
      "9557655                    NaN            NaN  20210112         0.0   \n",
      "\n",
      "                  gfe       hee  insertedShortOrder  insertionPending  \\\n",
      "8913317  2.777246e+05  0.000424                 0.0               1.0   \n",
      "9199230  1.205756e+08  0.001085                 0.0               1.0   \n",
      "9261373  4.684292e+05  0.001624                 0.0               1.0   \n",
      "9355676  4.053438e+04  0.001528                 0.0               1.0   \n",
      "9413368  2.047150e+05 -0.000100                 0.0               1.0   \n",
      "9557655  9.095034e+06  0.002318                 0.0               1.0   \n",
      "\n",
      "         internalId    inv_L  inv_L0    inv_S   inv_S0 l4algoDebug  l4tr  \\\n",
      "8913317      1971.0   5500.0  4600.0      0.0      0.0         NaN   NaN   \n",
      "9199230      4872.0  25300.0     0.0  19946.0  19900.0         NaN   NaN   \n",
      "9261373      4423.0   3900.0  1200.0      0.0      0.0         NaN   NaN   \n",
      "9355676      2709.0      0.0     0.0      0.0      0.0         NaN   NaN   \n",
      "9413368      2403.0   3050.0  1800.0      0.0      0.0         NaN   NaN   \n",
      "9557655      2163.0   4100.0  4100.0      0.0      0.0         NaN   NaN   \n",
      "\n",
      "         locateShares  locateSharesTotal           mfe  mra100  mrb100  \\\n",
      "8913317           0.0                0.0 -1.000000e+00     NaN     NaN   \n",
      "9199230         554.0            23900.0  1.195589e+08     NaN     NaN   \n",
      "9261373           0.0                0.0 -2.940770e+04     NaN     NaN   \n",
      "9355676           0.0                0.0 -1.000000e+00     NaN     NaN   \n",
      "9413368           0.0                0.0 -1.000000e+00     NaN     NaN   \n",
      "9557655           0.0                0.0 -1.000000e+00     NaN     NaN   \n",
      "\n",
      "              mrm mrm25  mrmum  mrrlma mrsb300  mrsb90  mrss300  mrss90  \\\n",
      "8913317       NaN   NaN    NaN     NaN     NaN     NaN      NaN     NaN   \n",
      "9199230  0.002211   NaN    NaN     NaN     NaN     NaN      NaN     NaN   \n",
      "9261373       NaN   NaN    NaN     NaN     NaN     NaN      NaN     NaN   \n",
      "9355676       NaN   NaN    NaN     NaN     NaN     NaN      NaN     NaN   \n",
      "9413368       NaN   NaN    NaN     NaN     NaN     NaN      NaN     NaN   \n",
      "9557655       NaN   NaN    NaN     NaN     NaN     NaN      NaN     NaN   \n",
      "\n",
      "         mrstaat  mrstauc  mrstaum  mrv               ms  mse   mt  ...  \\\n",
      "8913317      NaN      NaN      NaN  NaN  14:45:12.394261  100  0.0  ...   \n",
      "9199230      NaN      NaN      NaN  NaN  14:56:38.295696  100  0.0  ...   \n",
      "9261373      NaN      NaN      NaN  NaN  14:56:04.760698  100  0.0  ...   \n",
      "9355676      NaN      NaN      NaN  NaN  14:56:34.239527  100  0.0  ...   \n",
      "9413368      NaN      NaN      NaN  NaN  14:56:31.106297  100  0.0  ...   \n",
      "9557655      NaN      NaN      NaN  NaN  14:56:38.913527  100  0.0  ...   \n",
      "\n",
      "        orderPrice  orderSysId  resa          sdd    secid   sequenceNo  \\\n",
      "8913317      38.32         NaN   NaN  144511000.0  1601877  249152997.0   \n",
      "9199230      23.59         NaN   1.0  145636660.0  2002236  147750938.0   \n",
      "9261373     109.37         NaN   NaN  145605420.0  2002568  261884767.0   \n",
      "9355676      66.19         NaN   NaN  145630110.0  2002982  147665552.0   \n",
      "9413368       7.91         NaN   NaN  145630500.0  2300229  290133624.0   \n",
      "9557655      54.82         NaN   NaN  145638190.0  2300709  265146005.0   \n",
      "\n",
      "        session  threadId  totalActions  totalCanceled  tradeId  tradePrice  \\\n",
      "8913317       0   62400.0        1972.0          255.0      NaN        -1.0   \n",
      "9199230       0  205365.0        5756.0         1026.0      NaN        -1.0   \n",
      "9261373       0  357805.0        4423.0         1249.0      NaN        -1.0   \n",
      "9355676       0    8886.0        3638.0          929.0      NaN        -1.0   \n",
      "9413368       0  219390.0        2677.0          274.0      NaN        -1.0   \n",
      "9557655       0   21299.0        2163.0          105.0      NaN        -1.0   \n",
      "\n",
      "         underlyingIndex  updateType         vai  \\\n",
      "8913317              300           0         0.0   \n",
      "9199230              300           0  70049053.0   \n",
      "9261373              905           0         0.0   \n",
      "9355676              852           0         1.0   \n",
      "9413368              852           0       597.0   \n",
      "9557655              852           0        19.0   \n",
      "\n",
      "                                          zipFile      colo  caa_orderLog  \\\n",
      "8913317           logs_20210112_zs_52_09_day_5264  zs_52_09           NaN   \n",
      "9199230  logs_20210112_zt_88_06_day_8970longshort  zt_88_06           NaN   \n",
      "9261373           logs_20210112_zs_52_06_day_5332  zs_52_06           NaN   \n",
      "9355676           logs_20210112_zt_88_03_day_8833  zt_88_03           NaN   \n",
      "9413368           logs_20210112_zs_88_04_day_8865  zs_88_04           NaN   \n",
      "9557655           logs_20210112_zs_94_05_day_9471  zs_94_05           NaN   \n",
      "\n",
      "         start_time  Price OrderQty Side  statusLs  TradePriceLs  TradeQtyLs  \\\n",
      "8913317         NaN    NaN      NaN  NaN       NaN           NaN         NaN   \n",
      "9199230         NaN    NaN      NaN  NaN       NaN           NaN         NaN   \n",
      "9261373         NaN    NaN      NaN  NaN       NaN           NaN         NaN   \n",
      "9355676         NaN    NaN      NaN  NaN       NaN           NaN         NaN   \n",
      "9413368         NaN    NaN      NaN  NaN       NaN           NaN         NaN   \n",
      "9557655         NaN    NaN      NaN  NaN       NaN           NaN         NaN   \n",
      "\n",
      "          beta_60  adjMid_F30s adjMid_F90s adjMid_F300s indexClose  \\\n",
      "8913317  1.408190    37.988400   38.139091    38.225455  5419.4525   \n",
      "9199230  0.912613    23.590000   23.500000    23.539208  5593.3723   \n",
      "9261373  0.444831   108.805385  110.447778   111.377500  6425.9878   \n",
      "9355676  1.445265    64.981538   64.963333    66.456667  6453.2314   \n",
      "9413368  1.112110     7.684400    7.655702     7.658015  6450.9616   \n",
      "9557655  1.316276    52.537500   52.345000    52.984286  6450.9616   \n",
      "\n",
      "         indexClose_F30s  indexClose_F90s  indexClose_F300s     test  broker  \\\n",
      "8913317        5426.4846        5446.2497         5438.4974  2211867      52   \n",
      "9199230        5594.1694        5594.1694         5596.3525  2364855      89   \n",
      "9261373        6443.1173        6452.6847         6459.9599  2192069      53   \n",
      "9355676        6441.5200        6439.6562         6476.3828  2356763      88   \n",
      "9413368        6439.0713        6441.6596         6476.0417  2260157      88   \n",
      "9557655        6439.0713        6441.6596         6476.0417  2275720      94   \n",
      "\n",
      "         colo_broker    order    group        startClock  duration  \\\n",
      "8913317        zs_52  2214934  1392621  1610433912395283         0   \n",
      "9199230        zt_89  2368290  1436068  1610434598296847         0   \n",
      "9261373        zs_53  2195113  1445744  1610434564762024         0   \n",
      "9355676        zt_88  2360192  1461030  1610434594240197         0   \n",
      "9413368        zs_88  2263587  1470566  1610434591107295         0   \n",
      "9557655        zs_94  2279145  1495229  1610434598914553         0   \n",
      "\n",
      "        orderDirection1  directNum  isMsg  status  orderNtl  exchange  \\\n",
      "8913317              -1        1.0    1.0       2   34488.0       SSE   \n",
      "9199230               1        1.0    1.0       2    4718.0       SZE   \n",
      "9261373               1        1.0    1.0       2   10937.0       SZE   \n",
      "9355676               1        1.0    1.0       2    6619.0       SZE   \n",
      "9413368              -1        1.0    1.0       2    9492.0       SZE   \n",
      "9557655               1        1.0    1.0       2   32892.0       SZE   \n",
      "\n",
      "         tradeNtl  m1  m2   sta  \n",
      "8913317       0.0 NaN NaN  else  \n",
      "9199230       0.0 NaN NaN  else  \n",
      "9261373       0.0 NaN NaN  else  \n",
      "9355676       0.0 NaN NaN  else  \n",
      "9413368       0.0 NaN NaN  else  \n",
      "9557655       0.0 NaN NaN  else  \n",
      "\n",
      "[6 rows x 105 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrstaat  mrstauc\n",
      "1000.0   0.0         321613\n",
      "3000.0   1000.0     1109423\n",
      "         2000.0       88859\n",
      "         3000.0       36647\n",
      "11000.0  10000.0      30345\n",
      "         20000.0       2545\n",
      "         30000.0       1602\n",
      "13000.0  11000.0     690319\n",
      "         12000.0      21440\n",
      "         13000.0       3962\n",
      "         21000.0      21163\n",
      "         22000.0      21010\n",
      "         23000.0       3587\n",
      "         31000.0      10122\n",
      "         32000.0       7681\n",
      "         33000.0       9046\n",
      "Name: date, dtype: int64\n",
      "99.25% SZE orders triggered by msg data\n",
      "         date  accCode  secid_x  secid_y      perc\n",
      "153  20201230     8970     1204     1952  0.616803\n",
      "266  20210104     7293      139      220  0.631818\n",
      "320  20210105     7293      126      223  0.565022\n",
      "371  20210106     7293      253      397  0.637280\n",
      "372  20210106     8833      228     1054  0.216319\n",
      "373  20210106     8854       16      584  0.027397\n",
      "377  20210106     8970      116     2146  0.054054\n",
      "392  20210106   897102      316     1833  0.172395\n",
      "600  20210112     6878      287      481  0.596674\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cur_perc</th>\n",
       "      <th>prev_perc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SSE</th>\n",
       "      <td>70</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SZE</th>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n",
      "236\n",
      "                                     prev_fillRate  cur_fillRate\n",
      "exchange sta       colo     accCode                             \n",
      "SSE      2. statwo zt_94_06 9535          0.742280      0.589824\n",
      "         3. sta300 zs_96_06 9765          0.830120      0.562325\n",
      "SZE      1. staone zs_96_08 974102        0.695933      0.434165\n",
      "         2. statwo zs_68_01 6878          0.581095      0.854348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:581: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:582: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>prev_fillRate</th>\n",
       "      <th>cur_fillRate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <th>sta</th>\n",
       "      <th>colo</th>\n",
       "      <th>accCode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SSE</th>\n",
       "      <th>2. statwo</th>\n",
       "      <th>zt_94_06</th>\n",
       "      <th>9535</th>\n",
       "      <td>74%</td>\n",
       "      <td>59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3. sta300</th>\n",
       "      <th>zs_96_06</th>\n",
       "      <th>9765</th>\n",
       "      <td>83%</td>\n",
       "      <td>56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SZE</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zs_96_08</th>\n",
       "      <th>974102</th>\n",
       "      <td>70%</td>\n",
       "      <td>43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. statwo</th>\n",
       "      <th>zs_68_01</th>\n",
       "      <th>6878</th>\n",
       "      <td>58%</td>\n",
       "      <td>85%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n",
      "443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:678: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>prev_med</th>\n",
       "      <th>cur_med</th>\n",
       "      <th>prev_95p</th>\n",
       "      <th>cur_95p</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <th>sta</th>\n",
       "      <th>colo</th>\n",
       "      <th>accCode</th>\n",
       "      <th>isMsg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SSE</th>\n",
       "      <th>3. sta300</th>\n",
       "      <th>zs_52_09</th>\n",
       "      <th>5264</th>\n",
       "      <th>0</th>\n",
       "      <td>134</td>\n",
       "      <td>187</td>\n",
       "      <td>805</td>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:681: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:770: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>prev_med</th>\n",
       "      <th>cur_med</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sta</th>\n",
       "      <th>colo</th>\n",
       "      <th>accCode</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2. statwo</th>\n",
       "      <th>zs_68_01</th>\n",
       "      <th>6878</th>\n",
       "      <td>5988</td>\n",
       "      <td>4222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3. sta300</th>\n",
       "      <th>zs_96_08</th>\n",
       "      <th>6282</th>\n",
       "      <td>82353</td>\n",
       "      <td>85630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:773: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are stocks with zero trade size\n",
      "colo      accCode\n",
      "zs_88_04  8924       6\n",
      "zt_88_06  8886       2\n",
      "          8971       1\n",
      "Name: secid, dtype: int64\n",
      "There are stocks with null beta\n",
      "         Unnamed: 0  ApplSeqNum       aaa  absFilledThisUpdate  absOrderSize  \\\n",
      "28232       28295.0         0.0  0.001804                 2400          2400   \n",
      "28235       28297.0         0.0  0.002871                 1000          1000   \n",
      "59128       59360.0         0.0  0.003085                  400           400   \n",
      "83398       83757.0         0.0  0.001261                 4300          4300   \n",
      "83401       83759.0         0.0  0.001341                  200           800   \n",
      "...             ...         ...       ...                  ...           ...   \n",
      "3231277    879156.0         0.0  0.002240                  100           100   \n",
      "3231280    879035.0         0.0  0.001978                  400           400   \n",
      "3231283    879037.0         0.0  0.003710                  300           300   \n",
      "3231294    879039.0         0.0  0.003615                  900          1200   \n",
      "3231315    879191.0         0.0  0.001952                  300           300   \n",
      "\n",
      "         absOrderSizeCumFilled  accCode  ars             caamd  \\\n",
      "28232                     2400     9248  1.0  1609132613362575   \n",
      "28235                     1000     9248  1.0  1609132615860491   \n",
      "59128                      400     9248  1.0  1609137218846089   \n",
      "83398                     4300     9441  1.0  1609122270511835   \n",
      "83401                      200     9441  1.0  1609122271788443   \n",
      "...                        ...      ...  ...               ...   \n",
      "3231277                    100     5470  1.0  1609391805694128   \n",
      "3231280                    400     5225  1.0  1609395070675744   \n",
      "3231283                    300     5225  1.0  1609396275355390   \n",
      "3231294                    900     5225  1.0  1609397546247104   \n",
      "3231315                    300     6631  1.0  1609397547081765   \n",
      "\n",
      "         cancellationPending           cfe                      clock  \\\n",
      "28232                    0.0  1.080628e+05 2020-12-28 13:16:53.805038   \n",
      "28235                    0.0  1.051962e+05 2020-12-28 13:16:56.301011   \n",
      "59128                    0.0  8.455495e+04 2020-12-28 14:33:39.253149   \n",
      "83398                    0.0  2.390114e+06 2020-12-28 10:24:30.883884   \n",
      "83401                    0.0  2.378293e+06 2020-12-28 10:24:32.895358   \n",
      "...                      ...           ...                        ...   \n",
      "3231277                  0.0  1.141863e+06 2020-12-31 13:16:45.696688   \n",
      "3231280                  0.0  7.504252e+06 2020-12-31 14:11:10.679564   \n",
      "3231283                  0.0  6.457848e+06 2020-12-31 14:31:15.359361   \n",
      "3231294                  0.0  4.242813e+06 2020-12-31 14:52:26.250113   \n",
      "3231315                  0.0  1.267015e+06 2020-12-31 14:52:27.084646   \n",
      "\n",
      "           clockAtArrival  cumSharesBought  cumSharesBuyInserted  \\\n",
      "28232    1609132613805038           2400.0                2400.0   \n",
      "28235    1609132616301011           3400.0                3400.0   \n",
      "59128    1609137219253149            400.0                 400.0   \n",
      "83398    1609122270883884           4300.0                4300.0   \n",
      "83401    1609122272895358           4500.0                5100.0   \n",
      "...                   ...              ...                   ...   \n",
      "3231277  1609391805696688            700.0                 700.0   \n",
      "3231280  1609395070679564           1400.0                1800.0   \n",
      "3231283  1609396275359361           1700.0                2100.0   \n",
      "3231294  1609397546250113           2600.0                3300.0   \n",
      "3231315  1609397547084646           1200.0                1400.0   \n",
      "\n",
      "         cumSharesSellInserted  cumSharesSold      date  finalState  \\\n",
      "28232                      0.0            0.0  20201228         1.0   \n",
      "28235                      0.0            0.0  20201228         1.0   \n",
      "59128                      0.0            0.0  20201228         1.0   \n",
      "83398                      0.0            0.0  20201228         1.0   \n",
      "83401                      0.0            0.0  20201228         0.0   \n",
      "...                        ...            ...       ...         ...   \n",
      "3231277                    0.0            0.0  20201231         1.0   \n",
      "3231280                 3100.0         3100.0  20201231         1.0   \n",
      "3231283                 3100.0         3100.0  20201231         1.0   \n",
      "3231294                 3100.0         3100.0  20201231         0.0   \n",
      "3231315                    0.0            0.0  20201231         1.0   \n",
      "\n",
      "                  gfe  hee  insertedShortOrder  insertionPending  internalId  \\\n",
      "28232    1.080618e+05 -1.0                 0.0               0.0       665.0   \n",
      "28235    1.051952e+05 -1.0                 0.0               0.0       666.0   \n",
      "59128    8.455395e+04 -1.0                 0.0               0.0      1007.0   \n",
      "83398    2.390113e+06 -1.0                 0.0               0.0       730.0   \n",
      "83401    2.378292e+06 -1.0                 0.0               0.0       731.0   \n",
      "...               ...  ...                 ...               ...         ...   \n",
      "3231277  1.141862e+06 -1.0                 0.0               0.0      2220.0   \n",
      "3231280  7.504251e+06 -1.0                 0.0               0.0      4032.0   \n",
      "3231283  6.457847e+06 -1.0                 0.0               0.0      4284.0   \n",
      "3231294  4.242812e+06 -1.0                 0.0               0.0      4561.0   \n",
      "3231315  1.267014e+06 -1.0                 0.0               0.0      3465.0   \n",
      "\n",
      "          inv_L  inv_L0  inv_S  inv_S0 l4algoDebug  l4tr  locateShares  \\\n",
      "28232    2400.0  2400.0    0.0     0.0         NaN   0.0           0.0   \n",
      "28235    3400.0  3400.0    0.0     0.0         NaN   0.0           0.0   \n",
      "59128     400.0   400.0    0.0     0.0         NaN   0.0           0.0   \n",
      "83398    4300.0  4300.0    0.0     0.0         NaN   0.0           0.0   \n",
      "83401    4500.0  4500.0    0.0     0.0         NaN   0.0           0.0   \n",
      "...         ...     ...    ...     ...         ...   ...           ...   \n",
      "3231277   700.0   700.0    0.0     0.0         NaN   0.0           0.0   \n",
      "3231280  1400.0  1400.0    0.0     0.0         NaN   0.0           0.0   \n",
      "3231283  1700.0  1700.0    0.0     0.0         NaN   0.0           0.0   \n",
      "3231294  2600.0  2600.0    0.0     0.0         NaN   0.0           0.0   \n",
      "3231315  1200.0  1200.0    0.0     0.0         NaN   0.0           0.0   \n",
      "\n",
      "         locateSharesTotal  mfe  mra100  mrb100       mrm     mrm25  mrmum  \\\n",
      "28232                  0.0 -1.0   286.0   285.0  0.002023         0    0.0   \n",
      "28235                  0.0 -1.0   286.0   285.0  0.002023         0    0.0   \n",
      "59128                  0.0 -1.0   284.0   283.0  0.000000         0    0.0   \n",
      "83398                  0.0 -1.0  1189.0  1187.0  0.001666  0.006503    0.0   \n",
      "83401                  0.0 -1.0  1190.0  1189.0  0.001666  0.006503    0.0   \n",
      "...                    ...  ...     ...     ...       ...       ...    ...   \n",
      "3231277                0.0 -1.0  6050.0  6047.0  0.000816  0.001397    1.0   \n",
      "3231280                0.0 -1.0  6018.0  6017.0 -0.000092  -5.4e-05    0.0   \n",
      "3231283                0.0 -1.0  6039.0  6021.0  0.000372  -5.5e-05    0.0   \n",
      "3231294                0.0 -1.0  6057.0  6030.0  0.001273   0.00141    0.0   \n",
      "3231315                0.0 -1.0  6030.0  6024.0  0.001583   0.00141    1.0   \n",
      "\n",
      "         mrrlma   mrsb300    mrsb90   mrss300    mrss90  mrstaat  mrstauc  \\\n",
      "28232       0.0        -1  0.001804 -1.000000 -0.005010   1000.0      0.0   \n",
      "28235       0.0        -1  0.002871 -1.000000 -0.006036   1000.0      0.0   \n",
      "59128       0.0        -1  0.003085 -1.000000 -0.006566   1000.0      0.0   \n",
      "83398       0.0        -1  0.001261 -1.000000 -0.002589   3000.0   1000.0   \n",
      "83401       0.0        -1  0.001341 -1.000000 -0.002630   3000.0   1000.0   \n",
      "...         ...       ...       ...       ...       ...      ...      ...   \n",
      "3231277     0.0        -1  0.002240 -1.000000 -0.002725   1000.0      0.0   \n",
      "3231280     0.0        -1 -0.001924 -1.000000  0.001825  13000.0  11000.0   \n",
      "3231283     0.0 -0.000649  0.001633 -0.002333 -0.001736  13000.0  11000.0   \n",
      "3231294     0.0  -0.00178  0.001953 -0.002683 -0.002821  13000.0  11000.0   \n",
      "3231315     0.0        -1  0.001952 -1.000000 -0.002820   3000.0   1000.0   \n",
      "\n",
      "         mrstaum          mrv               ms  mse   mt  ... resa  \\\n",
      "28232        0.0    6816000.0  13:16:53.803580    0 -1.0  ...  1.0   \n",
      "28235        0.0    6908600.0  13:16:56.299580    0 -1.0  ...  1.0   \n",
      "59128        0.0  110947694.0  14:33:39.251684    0 -1.0  ...  1.0   \n",
      "83398        0.0    5771729.0  10:24:30.882421    0 -1.0  ...  1.0   \n",
      "83401        0.0    5778829.0  10:24:32.894421    0 -1.0  ...  1.0   \n",
      "...          ...          ...              ...  ...  ...  ...  ...   \n",
      "3231277      0.0     754724.0  13:16:45.695838    0 -1.0  ...  1.0   \n",
      "3231280      1.0     893924.0  14:11:10.678315    0 -1.0  ...  1.0   \n",
      "3231283      1.0    1010575.0  14:31:15.358359    0 -1.0  ...  1.0   \n",
      "3231294      1.0    1127375.0  14:52:26.249404    0 -1.0  ...  1.0   \n",
      "3231315      0.0    1126475.0  14:52:27.083502    0 -1.0  ...  1.0   \n",
      "\n",
      "                 sdd    secid   sequenceNo  session  threadId totalActions  \\\n",
      "28232    131654000.0  1600186  101659215.0        0   48669.0        664.0   \n",
      "28235    131657000.0  1600186  101679613.0        0   48669.0        665.0   \n",
      "59128    143340000.0  1600396  137185848.0        0   48669.0       1003.0   \n",
      "83398    102429000.0  1600556   59975566.0        0  249361.0        730.0   \n",
      "83401    102432000.0  1600556   59989572.0        0  249361.0        734.0   \n",
      "...              ...      ...          ...      ...       ...          ...   \n",
      "3231277  131635160.0  2300858  113316289.0        0   19973.0       2220.0   \n",
      "3231280  141111540.0  2300858  202789791.0        0  185910.0       4033.0   \n",
      "3231283  143116220.0  2300858  218537351.0        0  185910.0       4285.0   \n",
      "3231294  145227120.0  2300858  238848266.0        0  185910.0       4562.0   \n",
      "3231315  145227120.0  2300858  244192986.0        0  409869.0       3465.0   \n",
      "\n",
      "         totalCanceled     tradeId  tradePrice  underlyingIndex  updateType  \\\n",
      "28232            116.0  18748745.0        2.86              852           4   \n",
      "28235            116.0  18751730.0        2.86              852           4   \n",
      "59128            176.0  24830029.0        2.84              852           4   \n",
      "83398            223.0  11662729.0       11.89              905           4   \n",
      "83401            224.0  11666695.0       11.89              905           4   \n",
      "...                ...         ...         ...              ...         ...   \n",
      "3231277          322.0    1.04e+14       60.50              852           4   \n",
      "3231280          764.0    1.04e+14       60.17              852           4   \n",
      "3231283          838.0    1.04e+14       60.21              852           4   \n",
      "3231294          920.0    1.04e+14       60.30              852           4   \n",
      "3231315          198.0    1.04e+14       60.30              852           4   \n",
      "\n",
      "                 vai                          zipFile      colo caa_orderLog  \\\n",
      "28232      6813600.0  logs_20201228_zt_92_01_day_9248  zt_92_01          NaN   \n",
      "28235      6907000.0  logs_20201228_zt_92_01_day_9248  zt_92_01          NaN   \n",
      "59128    110947694.0  logs_20201228_zt_92_01_day_9248  zt_92_01          NaN   \n",
      "83398      5771729.0  logs_20201228_zt_94_06_day_9441  zt_94_06          NaN   \n",
      "83401      5778029.0  logs_20201228_zt_94_06_day_9441  zt_94_06          NaN   \n",
      "...              ...                              ...       ...          ...   \n",
      "3231277     754724.0  logs_20201231_zs_54_01_day_5470  zs_54_01          NaN   \n",
      "3231280     893524.0  logs_20201231_zs_52_09_day_5225  zs_52_09          NaN   \n",
      "3231283    1010275.0  logs_20201231_zs_52_09_day_5225  zs_52_09          NaN   \n",
      "3231294    1126175.0  logs_20201231_zs_52_09_day_5225  zs_52_09          NaN   \n",
      "3231315    1126175.0  logs_20201231_zs_66_01_day_6631  zs_66_01          NaN   \n",
      "\n",
      "         start_time  Price  OrderQty  Side statusLs TradePriceLs  TradeQtyLs  \\\n",
      "28232           NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "28235           NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "59128           NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "83398           NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "83401           NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "...             ...    ...       ...   ...      ...          ...         ...   \n",
      "3231277         NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "3231280         NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "3231283         NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "3231294         NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "3231315         NaN    NaN       NaN   NaN      NaN          NaN         NaN   \n",
      "\n",
      "         beta_60  adjMid_F30s  adjMid_F90s  adjMid_F300s indexClose  \\\n",
      "28232        NaN          NaN          NaN           NaN        NaN   \n",
      "28235        NaN          NaN          NaN           NaN        NaN   \n",
      "59128        NaN          NaN          NaN           NaN        NaN   \n",
      "83398        NaN          NaN          NaN           NaN        NaN   \n",
      "83401        NaN          NaN          NaN           NaN        NaN   \n",
      "...          ...          ...          ...           ...        ...   \n",
      "3231277      NaN          NaN          NaN           NaN        NaN   \n",
      "3231280      NaN          NaN          NaN           NaN        NaN   \n",
      "3231283      NaN          NaN          NaN           NaN        NaN   \n",
      "3231294      NaN          NaN          NaN           NaN        NaN   \n",
      "3231315      NaN          NaN          NaN           NaN        NaN   \n",
      "\n",
      "        indexClose_F30s indexClose_F90s  indexClose_F300s    test  broker  \\\n",
      "28232               NaN             NaN               NaN  212976      92   \n",
      "28235               NaN             NaN               NaN  212975      92   \n",
      "59128               NaN             NaN               NaN  213088      92   \n",
      "83398               NaN             NaN               NaN  214378      94   \n",
      "83401               NaN             NaN               NaN  214377      94   \n",
      "...                 ...             ...               ...     ...     ...   \n",
      "3231277             NaN             NaN               NaN  659444      54   \n",
      "3231280             NaN             NaN               NaN  634163      52   \n",
      "3231283             NaN             NaN               NaN  634170      52   \n",
      "3231294             NaN             NaN               NaN  634172      52   \n",
      "3231315             NaN             NaN               NaN  673546      66   \n",
      "\n",
      "         colo_broker   order   group        startClock  duration  \\\n",
      "28232          zt_92  213242    3795  1609132613362595    442443   \n",
      "28235          zt_92  213243    3796  1609132615860518    440493   \n",
      "59128          zt_92  213355    7919  1609137218846103    407046   \n",
      "83398          zt_94  214644   11243  1609122270511886    371998   \n",
      "83401          zt_94  214645   11244  1609122271788531   1106827   \n",
      "...              ...     ...     ...               ...       ...   \n",
      "3231277        zs_54  659982  495769  1609391805694165      2523   \n",
      "3231280        zs_52  634707  495770  1609395070675812      3752   \n",
      "3231283        zs_52  634708  495771  1609396275355453      3908   \n",
      "3231294        zs_52  634709  495772  1609397546247179      2934   \n",
      "3231315        zs_66  674084  495772  1609397547081825      2821   \n",
      "\n",
      "         orderDirection1  directNum isMsg  status  orderNtl  exchange  \\\n",
      "28232                  1        1.0   1.0       0    6864.0       SSE   \n",
      "28235                  1        1.0   1.0       0    2860.0       SSE   \n",
      "59128                  1        1.0   1.0       0    1136.0       SSE   \n",
      "83398                  1        1.0   1.0       0   51127.0       SSE   \n",
      "83401                  1        1.0   0.0       1    9512.0       SSE   \n",
      "...                  ...        ...   ...     ...       ...       ...   \n",
      "3231277                1        1.0   1.0       0    6050.0       SZE   \n",
      "3231280               -1        1.0   1.0       0   24068.0       SZE   \n",
      "3231283                1        1.0   1.0       0   18063.0       SZE   \n",
      "3231294                1        1.0   1.0       1   72360.0       SZE   \n",
      "3231315                1        1.0   1.0       0   18090.0       SZE   \n",
      "\n",
      "         tradeNtl      m1      m2        sta       tag  num  \n",
      "28232      6864.0  1000.0     0.0  1. staone  previous    1  \n",
      "28235      2860.0  1000.0     0.0  1. staone  previous    1  \n",
      "59128      1136.0  1000.0     0.0  1. staone  previous    2  \n",
      "83398     51127.0  3000.0  1000.0  2. statwo  previous    1  \n",
      "83401      2378.0  3000.0  1000.0  2. statwo  previous    1  \n",
      "...           ...     ...     ...        ...       ...  ...  \n",
      "3231277    6050.0  1000.0     0.0  1. staone  previous    2  \n",
      "3231280   24068.0  3000.0  1000.0  3. sta300  previous    2  \n",
      "3231283   18063.0  3000.0  1000.0  3. sta300  previous    2  \n",
      "3231294   54270.0  3000.0  1000.0  3. sta300  previous    2  \n",
      "3231315   18090.0  3000.0  1000.0  2. statwo  previous    2  \n",
      "\n",
      "[15295 rows x 107 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:814: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:815: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:816: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:818: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:820: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:821: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:824: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:825: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:827: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:828: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:829: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:830: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:835: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:837: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sellOrderNum</th>\n",
       "      <th>sellRet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <th>date</th>\n",
       "      <th>sta</th>\n",
       "      <th>server_account</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">SSE</th>\n",
       "      <th>20201231</th>\n",
       "      <th>2. statwo</th>\n",
       "      <th>zt8806_8854</th>\n",
       "      <td>131</td>\n",
       "      <td>-40.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210112</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zt5205_527601</th>\n",
       "      <td>148</td>\n",
       "      <td>-26.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"23\" valign=\"top\">SZE</th>\n",
       "      <th>20201228</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zs5209_5291</th>\n",
       "      <td>271</td>\n",
       "      <td>-24.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201229</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zs9608_9741</th>\n",
       "      <td>126</td>\n",
       "      <td>-26.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">20201230</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1. staone</th>\n",
       "      <th>zs5209_5291</th>\n",
       "      <td>237</td>\n",
       "      <td>-34.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zt8806_897102</th>\n",
       "      <td>131</td>\n",
       "      <td>-43.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. statwo</th>\n",
       "      <th>zs9606_9765</th>\n",
       "      <td>139</td>\n",
       "      <td>-34.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20201231</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zs5209_5291</th>\n",
       "      <td>202</td>\n",
       "      <td>-44.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">20210104</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1. staone</th>\n",
       "      <th>zs5209_5291</th>\n",
       "      <td>239</td>\n",
       "      <td>-22.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zs9608_9741</th>\n",
       "      <td>242</td>\n",
       "      <td>-42.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">20210105</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zs5208_5281</th>\n",
       "      <td>217</td>\n",
       "      <td>-25.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2. statwo</th>\n",
       "      <th>zs5209_5264</th>\n",
       "      <td>135</td>\n",
       "      <td>-32.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zs9606_9765</th>\n",
       "      <td>162</td>\n",
       "      <td>-35.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210106</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zs5206_5332</th>\n",
       "      <td>123</td>\n",
       "      <td>-32.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">20210111</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1. staone</th>\n",
       "      <th>zs9608_9741</th>\n",
       "      <td>168</td>\n",
       "      <td>-40.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zt8806_8970</th>\n",
       "      <td>170</td>\n",
       "      <td>-24.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">20210112</th>\n",
       "      <th rowspan=\"9\" valign=\"top\">1. staone</th>\n",
       "      <th>zs5206_5275</th>\n",
       "      <td>775</td>\n",
       "      <td>-20.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zs5206_5384</th>\n",
       "      <td>156</td>\n",
       "      <td>-47.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zs5209_5291</th>\n",
       "      <td>304</td>\n",
       "      <td>-46.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zs5213_5329</th>\n",
       "      <td>123</td>\n",
       "      <td>-29.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zs6601_6631</th>\n",
       "      <td>287</td>\n",
       "      <td>-38.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zs8804_8924</th>\n",
       "      <td>234</td>\n",
       "      <td>-55.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zs9606_9758</th>\n",
       "      <td>370</td>\n",
       "      <td>-39.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zs9608_9754</th>\n",
       "      <td>127</td>\n",
       "      <td>-28.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zt8803_8833</th>\n",
       "      <td>194</td>\n",
       "      <td>-35.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>buyOrderNum</th>\n",
       "      <th>buyRet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <th>date</th>\n",
       "      <th>sta</th>\n",
       "      <th>server_account</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SSE</th>\n",
       "      <th>20210108</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zt5205_527601</th>\n",
       "      <td>264</td>\n",
       "      <td>-33.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">SZE</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">20201229</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zs5206_5275</th>\n",
       "      <td>127</td>\n",
       "      <td>-44.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. statwo</th>\n",
       "      <th>zs5206_5384</th>\n",
       "      <td>148</td>\n",
       "      <td>-23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210111</th>\n",
       "      <th>1. staone</th>\n",
       "      <th>zs5208_5281</th>\n",
       "      <td>115</td>\n",
       "      <td>-47.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210112</th>\n",
       "      <th>2. statwo</th>\n",
       "      <th>zs8804_8924</th>\n",
       "      <td>195</td>\n",
       "      <td>-40.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smtp server connected\n",
      "login\n",
      "send mail\n"
     ]
    }
   ],
   "source": [
    "generate_report('20210112')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "-----------------------------------------------------------------------------------------------\n",
      "load data\n",
      "L:\\orderLog\\data\\20201224.pkl\n",
      "L:\\orderLog\\data\\20201225.pkl\n",
      "L:\\orderLog\\data\\20201228.pkl\n",
      "L:\\orderLog\\data\\20201229.pkl\n",
      "L:\\orderLog\\data\\20201230.pkl\n",
      "L:\\orderLog\\data\\20201231.pkl\n",
      "L:\\orderLog\\data\\20210104.pkl\n",
      "L:\\orderLog\\data\\20210105.pkl\n",
      "L:\\orderLog\\data\\20210106.pkl\n",
      "L:\\orderLog\\data\\20210107.pkl\n",
      "L:\\orderLog\\data\\20210108.pkl\n",
      "There are ticks with orderDirection 0\n",
      "             date      colo  accCode    secid  vai  updateType      sdd  \\\n",
      "356173   20201224  zs_64_01     6480  2000622   -1           1  48064.0   \n",
      "368213   20201224  zs_64_01     6480  2000721   -1           1  52413.0   \n",
      "466764   20201224  zs_54_01     5474  2002282   -1           1  48185.0   \n",
      "480279   20201224  zs_54_01     5474  2002360   -1           1  37983.0   \n",
      "503042   20201224  zs_54_01     5474  2002457   -1           1  48999.0   \n",
      "...           ...       ...      ...      ...  ...         ...      ...   \n",
      "9131621  20210108  zs_66_01     6634  2002909   -1           1  50116.0   \n",
      "9152674  20210108  zs_66_01     6634  2002968   -1           1  50141.0   \n",
      "9185796  20210108  zs_54_01     5474  2300076   -1           1  49178.0   \n",
      "9228809  20210108  zs_52_09     5291  2300223   -1           1  39268.0   \n",
      "9352415  20210108  zs_66_01     6634  2300550   -1           1  52791.0   \n",
      "\n",
      "         orderDirection  absOrderSize  internalId       orderId  \n",
      "356173                0             0        -1.0  1.162010e+05  \n",
      "368213                0             0        -1.0  1.574720e+05  \n",
      "466764                0             0        -1.0  8.689137e+08  \n",
      "480279                0             0        -1.0  8.689086e+08  \n",
      "503042                0             0        -1.0  8.689142e+08  \n",
      "...                 ...           ...         ...           ...  \n",
      "9131621               0             0        -1.0  1.812816e+10  \n",
      "9152674               0             0        -1.0  1.812816e+10  \n",
      "9185796               0             0        -1.0  8.689140e+08  \n",
      "9228809               0             0        -1.0  1.198880e+06  \n",
      "9352415               0             0        -1.0  1.812817e+10  \n",
      "\n",
      "[179 rows x 11 columns]\n",
      "There are orders with all things same except sdd\n",
      "         ApplSeqNum       aaa  absFilledThisUpdate  absOrderSize  \\\n",
      "3108513   1444460.0  0.000687                    0           100   \n",
      "3108573   9795909.0  0.000694                    0           100   \n",
      "\n",
      "         absOrderSizeCumFilled  accCode  ars             caamd  \\\n",
      "3108513                      0   966301  1.0  1609205575014635   \n",
      "3108573                      0   966301  1.0  1609208372125359   \n",
      "\n",
      "         cancellationPending        cfe         clock    clockAtArrival  \\\n",
      "3108513                  0.0  128034.25  1.609206e+15  1609205575014666   \n",
      "3108573                  0.0   52600.79  1.609208e+15  1609208372125429   \n",
      "\n",
      "         cumSharesBought  cumSharesBuyInserted  cumSharesSellInserted  \\\n",
      "3108513              0.0                   0.0                  300.0   \n",
      "3108573              0.0                   0.0                  600.0   \n",
      "\n",
      "         cumSharesSold      date  finalState        gfe       hee  \\\n",
      "3108513          200.0  20201229         0.0  128033.25  0.000504   \n",
      "3108573          400.0  20201229         0.0   52599.79 -0.000102   \n",
      "\n",
      "         insertedShortOrder  insertionPending  internalId  inv_L  inv_L0  \\\n",
      "3108513                 0.0               1.0         9.0  700.0     0.0   \n",
      "3108573                 0.0               1.0         9.0  100.0     0.0   \n",
      "\n",
      "         inv_S  inv_S0            l4algoDebug  l4tr  locateShares  \\\n",
      "3108513    0.0     0.0                    NaN   0.0           0.0   \n",
      "3108573    0.0     0.0  1015.000000|2.000000|   0.0           0.0   \n",
      "\n",
      "         locateSharesTotal  mfe  mra100  mrb100       mrm     mrm25  mrmum  \\\n",
      "3108513                0.0 -1.0  2765.0  2764.0 -0.000118  -0.00139   -1.0   \n",
      "3108573                0.0 -1.0  2774.0  2771.0  0.000174 -0.002573   -1.0   \n",
      "\n",
      "         mrrlma mrsb300    mrsb90  mrss300    mrss90  mrstaat  mrstauc  \\\n",
      "3108513    -1.0      -1 -0.001033     -1.0  0.000687   1000.0      0.0   \n",
      "3108573    -1.0      -1 -0.001770     -1.0  0.000694   1000.0      0.0   \n",
      "\n",
      "         mrstaum        mrv               ms  mse   mt  mta   mv  \\\n",
      "3108513     -1.0   140020.0  09:32:55.014196  100  0.0 -999  0.0   \n",
      "3108573     -1.0  1976879.0  10:19:32.125029  100  0.0 -999  0.0   \n",
      "\n",
      "         orderDirection  orderId  orderOutstanding  orderPrice orderSysId  \\\n",
      "3108513              -1      9.0               0.0       27.64        NaN   \n",
      "3108573              -1     -1.0               0.0       27.71        NaN   \n",
      "\n",
      "         resa          sdd    secid  sequenceNo  session  threadId  \\\n",
      "3108513   2.0   93255060.0  2002595   8271885.0        5   29229.0   \n",
      "3108573   2.0  101932190.0  2002595  56341419.0        1   29306.0   \n",
      "\n",
      "         totalActions  totalCanceled tradeId  tradePrice  underlyingIndex  \\\n",
      "3108513           7.0            0.0     NaN        -1.0              905   \n",
      "3108573          10.0            1.0     NaN        -1.0              905   \n",
      "\n",
      "         updateType      vai                            zipFile      colo  \\\n",
      "3108513           0   140020  logs_20201229_zt_96_01_day_966301  zt_96_01   \n",
      "3108573           0  1976879  logs_20201229_zt_96_01_day_966301  zt_96_01   \n",
      "\n",
      "         caa_orderLog    start_time     Price  OrderQty  Side statusLs  \\\n",
      "3108513  1.609206e+15  1.609206e+15  276400.0     100.0   2.0   (0, 4)   \n",
      "3108573           NaN           NaN       NaN       NaN   NaN      NaN   \n",
      "\n",
      "        TradePriceLs TradeQtyLs   beta_60  adjMid_F30s  adjMid_F90s  \\\n",
      "3108513  (0, 276400)   (0, 100)  0.581536    27.643913    27.627500   \n",
      "3108573          NaN        NaN  0.581536    27.649275    27.629737   \n",
      "\n",
      "         adjMid_F300s  indexClose  indexClose_F30s  indexClose_F90s  \\\n",
      "3108513     27.473333   6239.2341        6241.7701        6238.8906   \n",
      "3108573     27.631667   6235.4744        6234.7954        6229.9708   \n",
      "\n",
      "         indexClose_F300s  \n",
      "3108513         6226.1042  \n",
      "3108573         6237.2874  \n",
      "There are orders with same internalId but different orderId other than accCode 8856 case\n",
      "date      colo      accCode  secid    orderDirection  absOrderSize  internalId\n",
      "20201225  zs_52_09  5291     2002212   0              0             -1.0          2\n",
      "20201228  zs_54_01  5456     2300036  -1              1000           2093.0       2\n",
      "20201229  zt_96_01  966301   1600376   0              0             -1.0          3\n",
      "                             2002595  -1              100            9.0          2\n",
      "Name: orderId, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are orders in 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "     secid  accCode      colo      vai  updateType         sdd  internalId  \\\n",
      "0  2300271     8856  zs_88_04  22400.0           0  93007340.0        30.0   \n",
      "1  2300271     8856  zs_88_04  22400.0           2        -1.0        30.0   \n",
      "2  2300271     8856  zs_88_04  22400.0           1     34209.0        30.0   \n",
      "3  2300271     8856  zs_88_04  22400.0           3        -1.0        30.0   \n",
      "4  2300271     8856  zs_88_04  22400.0           0  93009860.0        34.0   \n",
      "5  2300271     8856  zs_88_04  22400.0           2        -1.0        34.0   \n",
      "6  2300271     8856  zs_88_04  22400.0           2        -1.0        34.0   \n",
      "\n",
      "   orderId  absOrderSize  absFilledThisUpdate  absOrderSizeCumFilled  \\\n",
      "0     30.0           100                    0                      0   \n",
      "1     30.0           100                    0                      0   \n",
      "2     30.0           100                    0                      0   \n",
      "3     30.0           100                    0                      0   \n",
      "4     34.0           100                    0                      0   \n",
      "5     34.0           100                    0                      0   \n",
      "6     34.0           100                  100                    100   \n",
      "\n",
      "   orderPrice  tradePrice  \n",
      "0       24.73        -1.0  \n",
      "1       24.73        -1.0  \n",
      "2       24.73        -1.0  \n",
      "3       24.73        -1.0  \n",
      "4       24.70        -1.0  \n",
      "5       24.70        -1.0  \n",
      "6       24.70        24.7  \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-131d4238ddc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'updateType'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vai'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'orderId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2890\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2943\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2944\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3353\u001b[0m         \"\"\"\n\u001b[1;32m-> 3354\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3355\u001b[0m         \u001b[1;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3341\u001b[0m         new_data = self._mgr.take(\n\u001b[1;32m-> 3342\u001b[1;33m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3343\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1397\u001b[0m         \"\"\"\n\u001b[1;32m-> 1398\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m         indexer = (\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1866\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[1;32m-> 1867\u001b[1;33m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   1891\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1892\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1893\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-131d4238ddc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'updateType'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vai'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'orderId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'updateType'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vai'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'orderId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'updateType'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vai'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'orderId'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'There are orders out of 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vai'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vai'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'secid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'accCode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'colo'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vai'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'updateType'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sdd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'internalId'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'orderId'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'absOrderSize'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'absFilledThisUpdate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'absOrderSizeCumFilled'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'orderPrice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tradePrice'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2888\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2889\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2890\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2943\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2944\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2946\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3352\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3353\u001b[0m         \"\"\"\n\u001b[1;32m-> 3354\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3355\u001b[0m         \u001b[1;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3341\u001b[0m         new_data = self._mgr.take(\n\u001b[1;32m-> 3342\u001b[1;33m             \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3343\u001b[0m         )\n\u001b[0;32m   3344\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"take\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   1396\u001b[0m         \u001b[0mTake\u001b[0m \u001b[0mitems\u001b[0m \u001b[0malong\u001b[0m \u001b[0many\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m         \"\"\"\n\u001b[1;32m-> 1398\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m         indexer = (\n\u001b[0;32m   1400\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"int64\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1865\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[1;32m-> 1867\u001b[1;33m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         )\n\u001b[0;32m   1869\u001b[0m         \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerged_blocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   1887\u001b[0m         \u001b[1;31m# combination of those slices is a slice, too.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1888\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1889\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1891\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \"\"\"\n\u001b[0;32m    282\u001b[0m     \u001b[0m_warn_for_nonsequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "pd.set_option('max_rows', 100)\n",
    "pd.set_option('max_columns', 100)\n",
    "\n",
    "perc = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "dd= ' 20210108'\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "print('start')\n",
    "readPath = 'L:\\\\orderLog\\\\data\\\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "path = np.sort(dataPathLs)[-11:]\n",
    "cur = np.max([int(os.path.basename(i).split('.')[0]) for i in path])\n",
    "assert(cur == int(dd))\n",
    "\n",
    "\n",
    "body = \"<html><body><div>\" + 'Hi all,<p>The following is daily report based on ' + str(cur) + ' data.</p>'\n",
    "body += '<p><b>I. Assertions</p></b>'\n",
    "count = 0\n",
    "\n",
    "# load data\n",
    "print('-----------------------------------------------------------------------------------------------')\n",
    "print('load data')\n",
    "rawOrderLog = []\n",
    "for thisPath in path:\n",
    "    print(thisPath)\n",
    "    data = pickle.load(open(thisPath, 'rb'))\n",
    "    data = data.rename(columns={'mdClockAtArrival': 'caamd'})\n",
    "    rawOrderLog += [data]\n",
    "rawOrderLog = pd.concat(rawOrderLog, sort=False)\n",
    "\n",
    "for col in ['clockAtArrival', 'caamd', 'secid', 'updateType', 'vai', 'absFilledThisUpdate', 'orderDirection', 'absOrderSize',\n",
    "            'absOrderSizeCumFilled', 'date', 'accCode', 'mse']:\n",
    "    rawOrderLog[col] = rawOrderLog[col].fillna(0).astype('int64')   \n",
    "rawOrderLog = rawOrderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "rawOrderLog = rawOrderLog[rawOrderLog[\"secid\"] >= 1000000]\n",
    "\n",
    "if rawOrderLog[rawOrderLog.duplicated(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                    'orderDirection', 'absOrderSize'], keep=False)].shape[0] != 0:\n",
    "    print('There are accounts with duplicated ticks:')\n",
    "    print(rawOrderLog[rawOrderLog.duplicated(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                    'orderDirection', 'absOrderSize'], keep=False)]\\\n",
    ".groupby(['date', 'colo', 'accCode'])['ars'].size())\n",
    "    rawOrderLog = rawOrderLog.drop_duplicates(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                    'orderDirection', 'absOrderSize'], keep='first')\n",
    "\n",
    "\n",
    "print('There are ticks with orderDirection 0')\n",
    "print(rawOrderLog[rawOrderLog['orderDirection'] == 0][['date', 'colo', 'accCode', \\\n",
    "            'secid', 'vai', 'updateType', 'sdd', 'orderDirection', 'absOrderSize', 'internalId', 'orderId']])\n",
    "try:\n",
    "    num1 = rawOrderLog[(rawOrderLog['orderDirection'] == 0) & (rawOrderLog['date'] == cur)].shape[0]\n",
    "    assert(num1 < 30)\n",
    "except:\n",
    "    count += 1\n",
    "    body += str(count) + '. There are in total ' + str(num1) + ' orders with orderDirection 0.<div>'\n",
    "\n",
    "\n",
    "assert(rawOrderLog[rawOrderLog['updateType'] == 0][rawOrderLog[rawOrderLog['updateType'] == 0]\\\n",
    "                                                   .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                'vai', 'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "try:\n",
    "    assert(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                       .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                    'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "except:\n",
    "    print('There are orders with all things same except sdd')\n",
    "    print(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                       .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                    'absOrderSize', 'internalId'], keep=False)])\n",
    "    assert(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                       .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                    'absOrderSize', 'internalId', 'sdd'], keep=False)].shape[0] == 0)\n",
    "try:\n",
    "    assert(sum(rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() != 1) == 0) \n",
    "except:\n",
    "    print('There are orders with same internalId but different orderId other than accCode 8856 case')\n",
    "    print(rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique()[rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() > 1])\n",
    "\n",
    "r2 = rawOrderLog[(rawOrderLog['accCode'] != 8856) & (rawOrderLog['orderDirection'] != 0)]\n",
    "r1 = rawOrderLog[(rawOrderLog['accCode'] == 8856) & (rawOrderLog['orderDirection'] != 0)]\n",
    "r1['test'] = r1.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "            'orderDirection', 'absOrderSize']).grouper.group_info[0]\n",
    "r1 = r1.sort_values(by=['test', 'clockAtArrival'])\n",
    "r1.loc[r1['updateType'] != 0, 'vai'] = np.nan\n",
    "r1['vai'] = r1.groupby('test')['vai'].ffill()\n",
    "r2['test'] = r2.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "            'orderDirection', 'absOrderSize', 'internalId']).grouper.group_info[0]\n",
    "r2 = r2.sort_values(by=['test', 'clockAtArrival'])\n",
    "r2.loc[r2['updateType'] != 0, 'vai'] = np.nan\n",
    "r2['vai'] = r2.groupby('test')['vai'].ffill()\n",
    "try:\n",
    "    assert(sum(r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "except:\n",
    "    a = r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique()[r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1].reset_index()\n",
    "    print('There are orders in 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "    print(pd.merge(r1, a[['test', 'vai']], on=['test', 'vai'], how='inner')[['secid', 'accCode', 'colo', 'vai', 'updateType', 'sdd', 'internalId', 'orderId', 'absOrderSize', 'absFilledThisUpdate', 'absOrderSizeCumFilled', 'orderPrice', 'tradePrice']])\n",
    "\n",
    "try:\n",
    "    assert(sum(r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "except:\n",
    "    a = r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique()[r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1].reset_index()\n",
    "    print('There are orders out of 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "    print(pd.merge(r2, a[['test', 'vai']], on=['test', 'vai'], how='inner')[['secid', 'accCode', 'colo', 'vai', 'updateType', 'sdd', 'internalId', 'orderId', 'absOrderSize', 'absFilledThisUpdate', 'absOrderSizeCumFilled', 'orderPrice', 'tradePrice']])\n",
    "orderLog = pd.concat([r1, r2])\n",
    "del r1\n",
    "del r2    \n",
    "\n",
    "orderLog = orderLog.sort_values(by=['date', 'colo', 'accCode', 'secid', 'vai', 'clockAtArrival']).reset_index(drop=True)\n",
    "orderLog['clock'] = orderLog['clockAtArrival'].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "orderLog[\"broker\"] = np.where(orderLog[\"accCode\"].astype(str).apply(lambda x: len(x) == 6), orderLog['accCode'] // 10000, orderLog['accCode'] // 100)\n",
    "orderLog['colo_broker'] = orderLog['colo'].str[:2] + '_' + orderLog['broker'].astype('str')\n",
    "orderLog['order'] = orderLog.groupby(['date', 'colo', 'accCode', 'secid', 'vai', 'orderDirection', 'absOrderSize', 'internalId']).grouper.group_info[0]\n",
    "orderLog['group'] = orderLog.groupby(['date', 'secid', 'vai']).grouper.group_info[0]\n",
    "orderLog['startClock'] = orderLog.groupby(['order'])['clockAtArrival'].transform('first')\n",
    "orderLog['duration'] = orderLog['clockAtArrival'] - orderLog['startClock']\n",
    "orderLog['orderPrice'] = orderLog['orderPrice'].apply(lambda x: round(x, 2))\n",
    "orderLog['tradePrice'] = orderLog['tradePrice'].apply(lambda x: round(x, 2))\n",
    "orderLog['orderDirection1'] = np.where(orderLog[\"orderDirection\"] == -2, -1, np.where(\n",
    "    orderLog[\"orderDirection\"] == 2, 1, orderLog[\"orderDirection\"]))\n",
    "orderLog[\"ars\"] = orderLog.groupby(['order'])['ars'].transform('first')\n",
    "orderLog['sdd'] = orderLog.groupby('order')['sdd'].transform('first')\n",
    "orderLog['caamd'] = orderLog.groupby('order')['caamd'].transform('first')\n",
    "\n",
    "\n",
    "### Assertion 1:  make sure same direction in same date, secid, vai\n",
    "print('=======================================================================================')\n",
    "print('1. same date, secid, vai: same direction')\n",
    "taking = orderLog[orderLog['ars'].isin([1, 7])]\n",
    "making = orderLog[orderLog['ars'].isin([2, 3])]\n",
    "else_orders = orderLog[~(orderLog['ars'].isin([1, 2, 3, 7]))]\n",
    "if else_orders.shape[0] != 0:\n",
    "    print('orders with abnormal ars values')\n",
    "    print(else_orders[(else_orders['updateType'] == 0) & (else_orders['date'] == cur)].groupby('ars')['date'].size().sort_values(ascending=False))\n",
    "    print(else_orders[(else_orders['updateType'] == 0) & (else_orders['date'] == cur)].groupby('accCode')['date'].size().sort_values(ascending=False))\n",
    "    ars_list = else_orders[(else_orders['updateType'] == 0) & (else_orders['date'] == cur)].groupby('ars')['date'].size().sort_values(ascending=False).reset_index()['ars'].values\n",
    "    ars_list = ', '.join([str(x) for x in ars_list])\n",
    "    count += 1\n",
    "    body += str(count) + '. There are abnormal ars values in data: ' + ars_list + '.<div>'\n",
    "    kk = else_orders[(else_orders['updateType'] == 0) & (else_orders['date'] == cur)].groupby('accCode')['date'].size().sort_values(ascending=False)\n",
    "    if kk[kk > 20].shape[0] > 0:\n",
    "        count += 1\n",
    "        stock_list = ', '.join([str(x) for x in kk[kk > 20].index.values])\n",
    "        body += str(count) + '. These accounts have more than 20 abnormal ars orders: ' + stock_list + '.<div>'\n",
    "taking['directNum'] = taking.groupby(['date', 'secid', 'vai', 'sdd'])['orderDirection1'].transform('nunique')\n",
    "if taking[(taking['directNum'] != 1)].shape[0] > 0:\n",
    "    print('opposite direction for same date, same secid, same vai')\n",
    "    print(taking[(taking['directNum'] != 1) & (taking['updateType'] == 0)].groupby(['accCode'])['orderDirection'].size())\n",
    "    try:\n",
    "        num1 = taking[(taking['directNum'] != 1) & (taking['updateType'] == 0) & (taking['date'] == cur)].shape[0]\n",
    "        assert(num1 < 20)\n",
    "    except:\n",
    "        count += 1\n",
    "        body += str(count) + '. There are in total ' + str(num1) + ' orders with opposite directions under same date, secid, vai.<div>'\n",
    "    try:\n",
    "        num2 = list(taking[(taking['directNum'] != 1) & (taking['updateType'] == 0) & (taking['date'] == \\\n",
    "            cur)].groupby('accCode')['date'].size()[taking[(taking['directNum'] != 1) & \\\n",
    "           (taking['updateType'] == 0) & (taking['date'] == cur)].\\\n",
    "                                                                       groupby('accCode')['date'].size() > 10].index)\n",
    "        assert(len(num2) == 0)\n",
    "    except:\n",
    "        count += 1\n",
    "        num2 = ', '.join([str(x) for x in num2])\n",
    "        body += str(count) + '. ' + num2 + ' has more than 10 orders with opposite directions under same date, secid, vai.<div>'\n",
    "    taking = taking[taking['directNum'] == 1]\n",
    "\n",
    "assert((taking.groupby(['date', 'secid', 'vai', 'sdd'])['orderDirection1'].nunique() == 1).all() == True)\n",
    "orderLog = pd.concat([taking, making]).sort_values(by=['date', 'colo', 'accCode', 'secid', 'vai', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "## Assertion 2:  make sure each account, secid, vai only has one insertion\n",
    "print('=======================================================================================')\n",
    "print('2. same date, secid, vai, accCode: one insertion')\n",
    "a = orderLog[orderLog['updateType'] == 0].groupby(['date', 'colo', 'accCode', 'secid', 'vai', 'sdd'])['clockAtArrival'].count().reset_index()\n",
    "if a[a['clockAtArrival'] > 1].shape[0] > 0:\n",
    "    print('more than one insertion at same time')\n",
    "    a = a[(a['clockAtArrival'] > 1)]\n",
    "    print(a)\n",
    "    print(a.groupby(['date'])['accCode'].size())\n",
    "    print(a.groupby(['date', 'accCode'])['sdd'].size())\n",
    "    try:\n",
    "        num1 = a[a['date'] == cur].shape[0]\n",
    "        assert(num1 < 20)\n",
    "    except:\n",
    "        count += 1\n",
    "        body += str(count) + '. There are in total ' + str(num1) + ' orders with more than one insertion in same order.<div>'\n",
    "    try:\n",
    "        num2 = list(a[a['date'] == cur].groupby(['accCode'])['date'].size()[a[a['date'] == cur].groupby(['accCode'])['date'].size() > 10].index)\n",
    "        assert(len(num2) == 0)\n",
    "    except:\n",
    "        count += 1\n",
    "        num2 = ', '.join([str(x) for x in num2])\n",
    "        body += str(count) + '. ' + num2 + ' has more than 10 orders with more than one insertion in same order.<div>'        \n",
    "    d_el = pd.merge(orderLog, a, on=['date', 'colo', 'accCode', 'secid', 'vai', 'sdd'])['order'].unique()\n",
    "    orderLog = orderLog[~(orderLog['order'].isin(d_el))]\n",
    "\n",
    "orderLog['isMsg'] = np.where(orderLog['updateType'] == 0, \n",
    "                             np.where(orderLog['mse'] == 100, 1, 0), np.nan)\n",
    "orderLog['isMsg'] = orderLog.groupby(['order'])['isMsg'].ffill()\n",
    "\n",
    "\n",
    "### Assertion 3:  check IPO stocks selling status\n",
    "print('=======================================================================================')\n",
    "print('3. IPO stocks selling (ars = 301, 302)')\n",
    "if orderLog[(orderLog['ars'].isin([301, 302])) & (orderLog['date'] == cur)].shape[0] != 0:\n",
    "    kk = orderLog[(orderLog['ars'].isin([301, 302])) & (orderLog['date'] == cur)]\n",
    "    print(kk)\n",
    "    try:\n",
    "        assert(kk[kk['orderDirection1'] == 1].shape[0] == 0)\n",
    "        print('we only sell, never buy')\n",
    "    except:\n",
    "        print('There are IPO buy side orders!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        count += 1\n",
    "        num1 = kk[kk['orderDirection1'] == 1].shape[0]\n",
    "        body += str(count) + '. There are ' + str(num1) + ' IPO buy side orders.<div>'\n",
    "        print(kk[kk['orderDirection1'] == 1])\n",
    "    kk1 = kk[kk['updateType'] == 0]\n",
    "    kk1 = kk1.sort_values(by=['accCode', 'secid','clockAtArrival'])\n",
    "    kk1['diff'] = kk1.groupby(['accCode', 'secid'])['clockAtArrival'].apply(lambda x: x-x.shift(1))\n",
    "    kk1['diff'] = kk1['diff'].fillna(0)\n",
    "    try:\n",
    "        assert(kk1[kk1['diff'] < 10e6].shape[0] == 0)\n",
    "        print('for each stock in the same account, there is no insertion within 10 seconds of the previous insertion')\n",
    "    except:\n",
    "        count += 1\n",
    "        kk1 = kk1.reset_index()\n",
    "        num2 = kk1[kk1['diff'] < 10e6].shape[0]\n",
    "        body += str(count) + '. There are ' + str(num2) + ' over ' + str(kk1.shape[0]) + ' orders with insertion within 10 seconds for orders under same account same stock.<div>'\n",
    "        print('There are insertion within 10 seconds for orders under same account same stock!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')        \n",
    "        print(kk1[kk1['diff'] < 10e6])\n",
    "    kk2 = kk[(kk['updateType'] == 1)]\n",
    "\n",
    "    try:\n",
    "        assert(kk2[kk2['duration'] < 3e6].shape[0] == 0)\n",
    "        print('for each stock in the same account, the cancellation of an order happens more than 3 seconds after the insertion')\n",
    "    except:\n",
    "        count += 1\n",
    "        num2 = kk2[kk2['duration'] < 3e6].shape[0]\n",
    "        body += str(count) + '. There are ' + str(num2) + ' over ' + str(kk1.shape[0]) + ' orders with cancellation within 3 seconds after insertion.<div>'        \n",
    "        print('There are cancellation within 3 seconds for orders under same account same stock!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print(kk2[kk2['duration'] < 3e6])\n",
    "\n",
    "\n",
    "### Assertion 4: check updateType == 7 orders, make sure updateType == 7 orders < 20 per account, < 100 in total\n",
    "print('=======================================================================================')\n",
    "print('4. updateType 7 orders')\n",
    "if orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)].shape[0] != 0:\n",
    "    try:\n",
    "        assert(orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)].groupby(['date', 'accCode'])['order'].nunique().max() < 20)\n",
    "    except:\n",
    "        print('There are more than 20 updateType 7 orders per account')\n",
    "        count += 1\n",
    "        a = list(orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)].groupby(['accCode'])['order'].nunique()[\n",
    "            orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)].groupby(['accCode'])['order'].nunique() >= 20\n",
    "        ].index)\n",
    "        a = ', '.join([str(x) for x in a])\n",
    "        body += str(count) + '. ' + a + ' has more than 20 updateType 7 orders.<div>'\n",
    "    try:      \n",
    "        assert(orderLog[(orderLog['updateType'] == 7) & (orderLog['date'] == cur)]['order'].nunique() < 100)\n",
    "    except:\n",
    "        print('Ther are more than 100 updateType 7 orders in total')\n",
    "        count += 1\n",
    "        body += str(count) + '. There are more than 100 updateType 7 orders in total.<div>'\n",
    "\n",
    "\n",
    "### Assertion 5: check updateType == 6 orders, make sure updateType == 6 orders < 5% per account\n",
    "### order being rejected by broker\n",
    "print('=======================================================================================')\n",
    "print('5. updateType 6 orders')\n",
    "k1 = orderLog[(orderLog['updateType'] == 6) & (orderLog['date'] == cur)].groupby(['accCode'])['order'].nunique().reset_index()\n",
    "k2 = orderLog[(orderLog['date'] == cur)].groupby(['accCode'])['order'].nunique().reset_index()\n",
    "k = pd.merge(k1, k2, on=['accCode'], how='left')\n",
    "k['prob'] = k['order_x']/k['order_y']\n",
    "try:\n",
    "    assert(sum(k['prob'] >= 0.05) == 0)\n",
    "except:\n",
    "    print('There are accounts with more than 5% updateType 6 orders!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "    print(k[k['prob'] >= 0.05])\n",
    "    a = k[k['prob'] >= 0.05]['accCode'].unique()\n",
    "    a = ', '.join([str(x) for x in a])\n",
    "    count += 1\n",
    "    body += str(count) + '. ' + a + ' has more than 5% updateType 6 orders.<div>'\n",
    "\n",
    "### Assertion 6: check CYB orders, make sure all CYB stocks have absOrderSize < 30w\n",
    "print('=======================================================================================')\n",
    "print('6. CYB stocks order size < 30w')\n",
    "try:\n",
    "    cyb = orderLog[(orderLog['secid'] >= 2300000) & (orderLog['updateType'] == 0) & (orderLog['date'] == cur)]\n",
    "    assert(cyb[cyb['absOrderSize'] > 300000].shape[0] == 0)\n",
    "except:\n",
    "    print('CYB stocks total absOrderSize >= 30w!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "    num1 = cyb[cyb['absOrderSize'] > 300000].shape[0]\n",
    "    count += 1\n",
    "    body += str(count) + '. There are ' + str(num1) + ' orders with CYB absOrderSize > 30w.<div>'\n",
    "\n",
    "\n",
    "### Assertion 7:  make sure there is no unexpected updateType \n",
    "print('=======================================================================================')\n",
    "print('7. unexpected updateType')\n",
    "def getTuple(x):\n",
    "    return tuple(i for i in x)\n",
    "\n",
    "checkLog = orderLog[~((orderLog['updateType'] == 4) & (orderLog.groupby(['order'])['updateType'].shift(-1) == 4))]\n",
    "checkLog = checkLog.groupby(['order'])['updateType'].apply(lambda x: getTuple(x)).reset_index()\n",
    "checkLog['status'] = np.where(checkLog['updateType'].isin([(0, 2, 4), (0, 2, 1, 4), (0, 2, 1, 2, 4), (0, 2, 4, 1, 4), (0, 4), (0, 1, 4), (0, 4, 1, 4), (0, 2, 2, 4), (0, 4, 2, 4), (0, 2, 2, 1, 4), (0, 2, 2, 4, 1, 4)]),0,\n",
    "                     np.where(checkLog['updateType'].isin([(0, 2, 4, 1, 3), (0, 2, 4, 1, 4, 3), (0, 2, 1, 4, 3), (0, 4, 1, 3), (0, 1, 4, 3),\n",
    "                                                               (0, 2, 2, 4, 1, 3), (0, 2, 2, 4, 1, 4, 3), (0, 2, 2, 1, 4, 3), (0, 4, 2, 4, 1, 3),\n",
    "                                                               (0, 4, 2, 1, 3), (0, 4, 1, 4, 3), (0, 4, 1)]), 1,\n",
    "                     np.where(checkLog['updateType'].isin([(0, 2, 1, 3), (0, 2, 2, 1, 3), (0, 2, 3), (0, 3), (0, 1, 3), (0, ), (0, 2), (0, 2, 1), (0, 2, 2)]), 2, 3)))\n",
    "print(checkLog[checkLog['status'] == 3].groupby('updateType')['order'].size())\n",
    "orderLog = pd.merge(orderLog, checkLog[['order', 'status']], how='left', on=['order'], validate='many_to_one')\n",
    "orderLog = orderLog[orderLog['status'].isin([0, 1, 2])].reset_index(drop=True)\n",
    "\n",
    "### Assertion 8:  make sure status==0 got all traded\n",
    "print('=======================================================================================')\n",
    "print('8. status == 0: all traded')\n",
    "a = orderLog[(orderLog['status'] == 0)]\n",
    "a = a.groupby(['date', 'order'])[['absOrderSizeCumFilled', 'absOrderSize']].max().reset_index()\n",
    "a.columns = ['date', 'order', 'filled', 'total']\n",
    "print('in total trade, any fill != total cases')\n",
    "print(a[a['filled'] != a['total']])\n",
    "if a[a['filled'] != a['total']].shape[0] > 0:\n",
    "    removeOrderLs = a[a['filled'] != a['total']]['order'].unique()\n",
    "    count += 1\n",
    "    body += str(count) + '. There are ' + str(a[(a['filled'] != a['total']) & (a['date'] == cur)]['order'].nunique()) + \\\n",
    "    ' over ' + str(a[(a['date'] == cur)]['order'].nunique()) + ' orders status == 0 but not all traded.<div>'\n",
    "    orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "### Assertion 9:  make sure status==1 got partial traded\n",
    "print('=======================================================================================')\n",
    "print('9. status == 1: partial traded')\n",
    "a = orderLog[orderLog['status'] == 1]\n",
    "a = a.groupby(['date', 'order'])[['absOrderSizeCumFilled', 'absOrderSize']].max().reset_index()\n",
    "a.columns = ['date', 'order', 'filled', 'total']\n",
    "print('in partial trade, any fill >= total or fill is 0 cases for updateType 4')\n",
    "print(a[(a['filled'] >= a['total']) | (a['filled'] == 0)])\n",
    "if a[(a['filled'] >= a['total']) | (a['filled'] == 0)].shape[0] > 0:\n",
    "    removeOrderLs = a[(a['filled'] >= a['total']) | (a['filled'] == 0)]['order'].unique()\n",
    "    count += 1\n",
    "    body += str(count) + '. There are ' + str(a[((a['filled'] >= a['total']) | (a['filled'] == 0)) & (a['date'] == cur)]['order'].nunique()) + \\\n",
    "    ' over ' + str(a[(a['date'] == cur)]['order'].nunique()) + ' orders status == 1 but not partial traded.<div>'\n",
    "    orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "### Assertion 10: make sure no cancellation within 1 sec\n",
    "print('=======================================================================================')\n",
    "print('10. no cancellation within 1 sec')\n",
    "a = orderLog[(orderLog['updateType'] == 1) & (orderLog['duration'] < 1e6)]\n",
    "print('any cancellation within 1 sec')\n",
    "print(a)\n",
    "if a[a['date'] == cur].shape[0] > 0:\n",
    "    count += 1\n",
    "    body += str(count) + '. There are ' + str(a[(a['date'] == cur)]['order'].nunique()) + ' orders cancel within 1s.<div>'    \n",
    "if a.shape[0] > 0:\n",
    "    removeOrderLs = a['order'].unique()\n",
    "    orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "\n",
    "### Assertion 11: make sure no order has shares > 80w or notional > 800w\n",
    "print('=======================================================================================')\n",
    "print('11. Orders with size > 80w or notional > 800w')\n",
    "orderLog['orderNtl'] = orderLog['absOrderSize'] * orderLog['orderPrice']\n",
    "if orderLog[(orderLog['absOrderSize'] > 800000) & (orderLog['date'] == cur)].shape[0] > 0:\n",
    "    count += 1\n",
    "    body += str(count) + '. There are ' + str(orderLog[(orderLog['absOrderSize'] > 800000) & (orderLog['date'] == cur)]['order'].nunique()) + ' orders shares > 80w.<div>'    \n",
    "if orderLog[orderLog['absOrderSize'] > 800000].shape[0] > 0:\n",
    "    print('some order quantity are > 80w')\n",
    "    print(orderLog[orderLog['absOrderSize'] > 800000].groupby(['colo', 'accCode'])['order'].nunique())\n",
    "    print(orderLog[orderLog['absOrderSize'] > 800000][['date', 'accCode', 'secid', 'vai', 'absOrderSize', 'orderPrice',\n",
    "                                                         'orderNtl', 'orderDirection', 'clock', 'order']])\n",
    "if orderLog[(orderLog['orderNtl'] > 8000000) & (orderLog['date'] == cur)].shape[0] > 0:\n",
    "    count += 1\n",
    "    body += str(count) + '. There are ' + str(orderLog[(orderLog['orderNtl'] > 8000000) & (orderLog['date'] == cur)]['order'].nunique()) + ' orders notional > 800w.<div>'                \n",
    "if orderLog[orderLog['orderNtl'] > 8000000].shape[0] > 0:\n",
    "    print('some order ntl are > 800w')\n",
    "    print(orderLog[orderLog['orderNtl'] > 8000000].groupby(['colo', 'accCode'])['order'].nunique())\n",
    "    print(orderLog[orderLog['orderNtl'] > 8000000][['date', 'accCode', 'secid', 'vai', 'absOrderSize', 'orderPrice',\n",
    "                                                      'orderNtl', 'orderDirection', 'clock', 'order', \"updateType\", \n",
    "                                                      \"tradePrice\", \"absOrderSizeCumFilled\", \"absFilledThisUpdate\"]])\n",
    "\n",
    "removeOrderLs = list(set(orderLog[orderLog['absOrderSize'] > 800000]['order'].unique()) | set(orderLog[orderLog['orderNtl'] > 8000000]['order'].unique()))\n",
    "orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "\n",
    "orderLog = orderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'order', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "orderLog['exchange'] = np.where(orderLog['secid'] >= 2000000, 'SZE', 'SSE')\n",
    "orderLog['orderNtl'] = orderLog['orderPrice'] * orderLog['absOrderSize']\n",
    "orderLog['tradeNtl'] = np.where(orderLog['updateType'] == 4, orderLog['tradePrice']*orderLog['absFilledThisUpdate'], 0)\n",
    "orderLog[\"mrstaat\"] = orderLog.groupby(['order'])['mrstaat'].transform('first')\n",
    "orderLog[\"ars\"] = orderLog.groupby(['order'])['ars'].transform('first')\n",
    "orderLog[\"mrstauc\"] = orderLog.groupby(['order'])['mrstauc'].transform('first')\n",
    "orderLog[\"mrsb90\"] = orderLog.groupby(['order'])['mrsb90'].transform('first')\n",
    "orderLog[\"mrss90\"] = orderLog.groupby(['order'])['mrss90'].transform('first')\n",
    "orderLog[\"aaa\"] = orderLog.groupby(['order'])['aaa'].transform('first')\n",
    "orderLog = orderLog[~orderLog['ars'].isnull()]\n",
    "# orderLog = orderLog[orderLog['ars'] % 10 == 1]\n",
    "\n",
    "\n",
    "orderLog['m1'] = orderLog['mrstaat'].apply(lambda x: x - (x // 10000) * 10000)\n",
    "orderLog['m2'] = orderLog['mrstauc'].apply(lambda x: x - (x // 10000) * 10000)\n",
    "try:\n",
    "    orderLog['mrsb90'] = orderLog['mrsb90'].astype(float)\n",
    "except:\n",
    "    print(orderLog[orderLog['mrsb90'] == '-'])\n",
    "    orderLog = orderLog[orderLog['mrsb90'] != '-']\n",
    "    orderLog['mrsb90'] = orderLog['mrsb90'].astype(float)\n",
    "try:\n",
    "    orderLog['mrss90'] = orderLog['mrss90'].astype(float)\n",
    "except:\n",
    "    print(orderLog[orderLog['mrss90'] == '-'])\n",
    "    orderLog = orderLog[orderLog['mrss90'] != '-']\n",
    "    orderLog['mrss90'] = orderLog['mrss90'].astype(float)\n",
    "try:\n",
    "    orderLog['aaa'] = orderLog['aaa'].astype(float)\n",
    "except:\n",
    "    print(orderLog[orderLog['aaa'] == '-'])\n",
    "    orderLog = orderLog[orderLog['aaa'] != '-']\n",
    "    orderLog['aaa'] = orderLog['aaa'].astype(float)\n",
    "\n",
    "orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "         (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'mrstauc'] = \\\n",
    "orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "         (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'm2']\n",
    "\n",
    "orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "         (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'mrstaat'] = \\\n",
    "orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "         (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'm1']\n",
    "\n",
    "orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "         (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'mrstauc'] = \\\n",
    "orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "         (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'm2']\n",
    "\n",
    "orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "         (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'mrstaat'] = \\\n",
    "orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "         (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'm1']    \n",
    "\n",
    "\n",
    "orderLog['sta'] = np.where(orderLog['mrstaat'] == 1000, '1. staone', np.where(\n",
    "orderLog['mrstaat'] == 3000, '2. statwo', np.where(\n",
    "orderLog['mrstaat'].isin([11000, 13000]), '3. sta300', 'else')))\n",
    "print(orderLog[(orderLog['sta'] == 'else') & (orderLog['updateType'] == 0)].groupby(['date', 'accCode'])['secid'].size())\n",
    "print(orderLog[(orderLog['sta'] == 'else') & (orderLog['date'] == cur) & (orderLog['updateType'] == 0)])\n",
    "m1 = orderLog[(orderLog['sta'] == 'else') & (orderLog['date'] == cur) & (orderLog['updateType'] == 0)].shape[0]\n",
    "if m1 != 0:\n",
    "    count += 1\n",
    "    body += str(count) + '. There are ' + str(m1) + ' orders with invalid strategy.<div>'                    \n",
    "orderLog = orderLog[orderLog['mrstaat'].isin([11000, 13000, 1000, 3000])]\n",
    "print(orderLog[orderLog['updateType'] == 0].groupby(['mrstaat', 'mrstauc'])['date'].size())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "body += '<p><b>II. fill rate</b></p>'\n",
    "\n",
    "import pandas as pd\n",
    "def convertToHtml(result,title):\n",
    "    #将数据转换为html的table\n",
    "    #result是list[list1,list2]这样的结构\n",
    "    #title是list结构；和result一一对应。titleList[0]对应resultList[0]这样的一条数据对应html表格中的一列\n",
    "    d = {}\n",
    "    index = 0\n",
    "    for t in title:\n",
    "        d[t]=result[index]\n",
    "        index = index+1\n",
    "    df = pd.DataFrame(d)\n",
    "    df = df[title]\n",
    "    h = df.to_html(index=False)\n",
    "    return h\n",
    "\n",
    "placeSZE = orderLog[(orderLog['secid'] >= 2000000) & (orderLog['updateType'] == 0)]\n",
    "print('%.2f%% SZE orders triggered by msg data'%(placeSZE[placeSZE['isMsg'] == 1].shape[0]/placeSZE.shape[0]*100))\n",
    "body += '%.2f%% SZE orders triggered by msg data<div>'%(placeSZE[placeSZE['isMsg'] == 1].shape[0]/placeSZE.shape[0]*100)\n",
    "placeSZE = orderLog[(orderLog['secid'] >= 2000000) & (orderLog['updateType'] == 0)]\n",
    "r1 = placeSZE[placeSZE['isMsg'] == 1].groupby(['date', 'accCode'])['secid'].size().reset_index()\n",
    "r2 = placeSZE.groupby(['date', 'accCode'])['secid'].size().reset_index()\n",
    "re = pd.merge(r1, r2, on=['date', 'accCode'], how='inner')\n",
    "re['perc'] = re['secid_x'] / re['secid_y']\n",
    "print(re[re['perc'] < 0.8])\n",
    "for i in re[re['perc'] < 0.8]['accCode'].unique():\n",
    "    a = list(re[(re['perc'] < 0.8) & (re['accCode'] == i)]['date'].unique())\n",
    "    a = ', '.join([str(x) for x in a])\n",
    "    body += 'accCode ' + str(i) + ' SZE orders on ' + a + ' has msg triggered percentage < 80%<div>'\n",
    "\n",
    "orderLog['tag'] = 'previous'\n",
    "orderLog.loc[orderLog['date'] == cur, 'tag'] = 'current'\n",
    "o1 = orderLog[orderLog['updateType'] == 0].groupby(['tag', 'exchange'])['orderNtl'].sum().reset_index()\n",
    "o2 = orderLog[orderLog['updateType'] == 4].groupby(['tag', 'exchange'])['tradeNtl'].sum().reset_index()\n",
    "o = pd.merge(o1, o2, on=['tag', 'exchange'])\n",
    "o['perc'] = o['tradeNtl'] / o['orderNtl']\n",
    "o['perc'] = o['perc'].apply(lambda x: '%.f'%(x*100))\n",
    "o1 = o[o['tag'] == 'current']\n",
    "o1 = o1.rename(columns={'perc':'cur_perc'})\n",
    "o2 = o[o['tag'] == 'previous']\n",
    "o2 = o2.rename(columns={'perc':'prev_perc'})\n",
    "re = pd.merge(o1[['exchange', 'cur_perc']], o2[['exchange', 'prev_perc']], on='exchange')\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(re.groupby(['exchange']).first().to_html()))\n",
    "result = [re['exchange'].values, re['cur_perc'].values, re['prev_perc'].values]\n",
    "title=['exchange', 'cur_perc', 'prev_perc']\n",
    "body += convertToHtml(result,title)\n",
    "\n",
    "orderLog['num'] = orderLog.groupby(['exchange', 'colo', 'accCode', 'secid'])['tag'].transform('nunique')\n",
    "checkLog = orderLog[orderLog['num'] == 2]\n",
    "# checkLog = orderLog\n",
    "d1 = checkLog[(checkLog['updateType'] == 0)].groupby(['tag', 'exchange', 'sta', 'colo', 'accCode'])['orderNtl'].sum().reset_index()\n",
    "d2 = checkLog[(checkLog['updateType'] == 4)].groupby(['tag', 'exchange', 'sta', 'colo', 'accCode'])['tradeNtl'].sum().reset_index()\n",
    "dd = pd.merge(d1, d2, on=['tag', 'exchange', 'sta', 'colo', 'accCode'])\n",
    "dd['fill rate'] = dd['tradeNtl'] / dd['orderNtl']\n",
    "add = checkLog[(checkLog['updateType'] == 0)].groupby(['tag', 'exchange', 'sta', 'colo', 'accCode'])['order'].nunique().reset_index()\n",
    "add = add.rename(columns={'order':'num'})\n",
    "dd = pd.merge(dd, add, on=['tag', 'exchange', 'sta', 'colo', 'accCode'])\n",
    "prev = dd[dd['tag'] == 'previous']\n",
    "prev = prev.rename(columns={'num':'prev_num', 'fill rate':'prev_fillRate'})\n",
    "cur = dd[dd['tag'] == 'current']\n",
    "cur = cur.rename(columns={'num':'cur_num', 'fill rate':'cur_fillRate'})\n",
    "report = pd.merge(prev[['exchange', 'sta', 'colo', 'accCode', 'prev_num', 'prev_fillRate']], \n",
    "              cur[['exchange', 'sta', 'colo', 'accCode', 'cur_num', 'cur_fillRate']], on=['exchange', 'sta', 'colo', 'accCode'], how='inner')\n",
    "\n",
    "a1 = checkLog[(checkLog['updateType'] == 0) & (checkLog['tag'] == 'previous')].groupby(['date', 'exchange', 'sta', 'colo', 'accCode'])['orderNtl'].sum().reset_index()\n",
    "a2 = checkLog[(checkLog['updateType'] == 4) & (checkLog['tag'] == 'previous')].groupby(['date', 'exchange', 'sta', 'colo', 'accCode'])['tradeNtl'].sum().reset_index()\n",
    "aa = pd.merge(a1, a2, on=['date', 'exchange', 'sta', 'colo', 'accCode'])\n",
    "aa['fill rate'] = aa['tradeNtl'] / aa['orderNtl']\n",
    "aa['count'] = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['date'].transform('nunique')\n",
    "aa1 = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['fill rate'].describe()[['mean', 'std']].reset_index()\n",
    "aa1 = aa1.fillna(0)\n",
    "aa = pd.merge(aa, aa1, on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "for j in [1]:\n",
    "    aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "    aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "    aa['count1'] = np.where((aa['fill rate'] <= aa[str(j) + 'std_high']) & (aa['fill rate'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "    re1 = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['count1'].sum().reset_index()\n",
    "    re2 = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['count'].first().reset_index()\n",
    "    re1 = pd.merge(re2, re1, on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "    re1['count1'] = re1['count1'] / re1['count']\n",
    "    re1 = re1.rename(columns={'count1':str(j)+'*std'})\n",
    "for j in [1.5, 2, 2.5, 3, 3.5, 4, 4.5]:\n",
    "    aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "    aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "    aa['count1'] = np.where((aa['fill rate'] <= aa[str(j) + 'std_high']) & (aa['fill rate'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "    re = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['count1'].sum().reset_index()\n",
    "    re2 = aa.groupby(['exchange', 'sta', 'colo', 'accCode'])['count'].first().reset_index()\n",
    "    re = pd.merge(re2, re, on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "    re['count1'] = re['count1'] / re['count']\n",
    "    re = re.rename(columns={'count1':str(j)+'*std'})\n",
    "    re1 = pd.merge(re1, re[['exchange', 'sta', 'colo', 'accCode', str(j)+'*std']], on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "nc = []\n",
    "for i in range(0, re1.shape[0]):\n",
    "    nc.append(np.float(re1.columns[5:][(re1.iloc[i, 5:] == 1)][0].split('*')[0]))\n",
    "re1['n'] = nc\n",
    "print(re1.shape[0])\n",
    "print(aa1.shape[0])\n",
    "aa1 = pd.merge(aa1, re1[['exchange', 'sta', 'colo', 'accCode', 'n']], on=['exchange', 'sta', 'colo', 'accCode'])\n",
    "aa1['min'] = aa1['mean'] - aa1['std'] * aa1['n']\n",
    "aa1['max'] = aa1['mean'] + aa1['std'] * aa1['n']\n",
    "report = pd.merge(report, aa1, on=['exchange', 'sta', 'colo', 'accCode'], how='left')\n",
    "assert(report[report['mean'].isnull()].shape[0] == 0)\n",
    "print(report[((report['cur_fillRate'] > report['max']) | (report['cur_fillRate'] < report['min'])) & (report['cur_num'] > 100) & (abs(report['cur_fillRate'] - report['prev_fillRate']) > 0.15)].groupby(['exchange', 'sta', 'colo', 'accCode'])['prev_fillRate', 'cur_fillRate'].first())\n",
    "report = report[((report['cur_fillRate'] > report['max']) | (report['cur_fillRate'] < report['min'])) & (report['cur_num'] > 100) & (abs(report['cur_fillRate'] - report['prev_fillRate']) > 0.15)].groupby(['exchange', 'sta', 'colo', 'accCode'])['prev_fillRate', 'cur_fillRate'].first().reset_index()\n",
    "\n",
    "for cols in ['prev_fillRate', 'cur_fillRate']:\n",
    "    report[cols] = report[cols].apply(lambda x: '%.f%%'%(100*x))\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(report.groupby(['exchange', 'sta', 'colo', 'accCode']).first().to_html()))\n",
    "body += '<div>In the following cases, fill rate under given accCode pass the hurdle we set:'\n",
    "result = [report['exchange'].values, report['sta'].values, report['colo'].values, report['accCode'].values, \n",
    "         report['prev_fillRate'].values, report['cur_fillRate'].values]\n",
    "title = ['exchange', 'sta', 'colo', 'accCode', 'prev_fillRate', 'cur_fillRate']\n",
    "body += convertToHtml(result,title)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "body += '<p><b>III. internal latency</b></p>'\n",
    "\n",
    "orderLog['num'] = orderLog.groupby(['exchange', 'colo', 'accCode', 'secid'])['tag'].transform('nunique')\n",
    "checkLog = orderLog[(orderLog[\"updateType\"] == 0) & (orderLog['num'] == 2)]\n",
    "checkLog = checkLog[checkLog['caamd'] != 0]\n",
    "checkLog['internal_latency'] = checkLog[\"clockAtArrival\"] - checkLog[\"caamd\"]\n",
    "SZE = checkLog[checkLog['secid'] >= 2000000]\n",
    "SSE = checkLog[checkLog['secid'] < 2000000]\n",
    "SZE = SZE[SZE['isMsg'] == 1]\n",
    "c1 = SZE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].quantile(.95).reset_index()\n",
    "c2 = SZE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].median().reset_index()\n",
    "c3 = SZE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].count().reset_index()\n",
    "\n",
    "re1 = pd.merge(c3, c1, on=['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])\n",
    "re1 = re1.rename(columns = {'internal_latency_x': 'count', 'internal_latency_y': '95 percentile'})\n",
    "re1 = pd.merge(re1, c2, on=['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])\n",
    "re1 = re1.rename(columns = {'internal_latency': 'median'})\n",
    "re1['isMsg'] = 1\n",
    "\n",
    "c1 = SSE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].quantile(.95).reset_index()\n",
    "c2 = SSE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].median().reset_index()\n",
    "c3 = SSE.groupby(['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])[\"internal_latency\"].count().reset_index()\n",
    "\n",
    "re2 = pd.merge(c3, c1, on=['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])\n",
    "re2 = re2.rename(columns = {'internal_latency_x': 'count', 'internal_latency_y': '95 percentile'})\n",
    "re2 = pd.merge(re2, c2, on=['tag', \"exchange\", \"colo\", \"accCode\", \"sta\", \"isMsg\"])\n",
    "re2 = re2.rename(columns = {'internal_latency': 'median'})\n",
    "re2\n",
    "\n",
    "re = pd.concat([re1, re2]).reset_index(drop=True)\n",
    "\n",
    "for col in ['isMsg','median', '95 percentile']:\n",
    "    re[col] = re[col].astype(int)\n",
    "\n",
    "re1 = re[re['tag'] == 'current']\n",
    "re1 = re1.rename(columns={'count':'cur_count', 'median':'cur_med', '95 percentile':'cur_95p'})\n",
    "re2 = re[re['tag'] == 'previous']\n",
    "re2 = re2.rename(columns={'count':'prev_count', 'median':'prev_med', '95 percentile':'prev_95p'})\n",
    "report = pd.merge(re1, re2, on=['exchange', 'colo', 'accCode', 'sta', 'isMsg'])\n",
    "\n",
    "checkLog[(checkLog['exchange'] == 'SSE') | ((checkLog['exchange'] == 'SZE') & (checkLog['isMsg'] == 1))]\n",
    "aa = checkLog[(checkLog['tag'] == 'previous')].groupby(['date', 'exchange', 'sta', 'colo', 'accCode', 'isMsg'])['internal_latency'].median().reset_index()\n",
    "aa['count'] = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['date'].transform('nunique')\n",
    "aa1 = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['internal_latency'].describe()[['mean', 'std']].reset_index()\n",
    "aa1 = aa1.fillna(0)\n",
    "aa = pd.merge(aa, aa1, on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "for j in [1]:\n",
    "    aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "    aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "    aa['count1'] = np.where((aa['internal_latency'] <= aa[str(j) + 'std_high']) & (aa['internal_latency'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "    re1 = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['count1'].sum().reset_index()\n",
    "    re2 = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['count'].first().reset_index()\n",
    "    re1 = pd.merge(re2, re1, on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "    re1['count1'] = re1['count1'] / re1['count']\n",
    "    re1 = re1.rename(columns={'count1':str(j)+'*std'})\n",
    "for j in [1.5, 2, 2.5, 3, 3.5, 4, 4.5]:\n",
    "    aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "    aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "    aa['count1'] = np.where((aa['internal_latency'] <= aa[str(j) + 'std_high']) & (aa['internal_latency'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "    re = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['count1'].sum().reset_index()\n",
    "    re2 = aa.groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['count'].first().reset_index()\n",
    "    re = pd.merge(re2, re, on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "    re['count1'] = re['count1'] / re['count']\n",
    "    re = re.rename(columns={'count1':str(j)+'*std'})\n",
    "    re1 = pd.merge(re1, re[['exchange', 'sta', 'colo', 'accCode', 'isMsg', str(j)+'*std']], on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "nc = []\n",
    "for i in range(0, re1.shape[0]):\n",
    "    nc.append(np.float(re1.columns[6:][(re1.iloc[i, 6:] == 1)][0].split('*')[0]))\n",
    "re1['n'] = nc\n",
    "print(re1.shape[0])\n",
    "print(aa1.shape[0])\n",
    "aa1 = pd.merge(aa1, re1[['exchange', 'sta', 'colo', 'accCode', 'isMsg', 'n']], on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'])\n",
    "aa1['min'] = aa1['mean'] - aa1['std'] * aa1['n']\n",
    "aa1['max'] = aa1['mean'] + aa1['std'] * aa1['n']\n",
    "report = pd.merge(report, aa1, on=['exchange', 'sta', 'colo', 'accCode', 'isMsg'], how='left')\n",
    "assert(report[report['mean'].isnull()].shape[0] == 0)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(report[((report['cur_med'] > report['max']) | (report['cur_med'] < report['min'])) \\\n",
    "                    & (report['cur_count'] > 100) & (abs(report['cur_med'] - report['prev_med']) > 20)] \\\n",
    "             .groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['prev_med', 'cur_med', 'prev_95p', 'cur_95p'].first().to_html()))\n",
    "report = report[((report['cur_med'] > report['max']) | (report['cur_med'] < report['min'])) \\\n",
    "                    & (report['cur_count'] > 100) & (abs(report['cur_med'] - report['prev_med']) > 20)] \\\n",
    "             .groupby(['exchange', 'sta', 'colo', 'accCode', 'isMsg'])['prev_med', 'cur_med', 'prev_95p', 'cur_95p'].first().reset_index()\n",
    "\n",
    "body += '<div>In the following cases, internal latency under given accCode pass the hurdle we set:'\n",
    "result = [report['exchange'].values, report['sta'].values, report['colo'].values, report['accCode'].values, report['isMsg'].values, \n",
    "         report['prev_med'].values, report['prev_95p'].values, report['cur_med'].values, report['cur_95p'].values]\n",
    "title = ['exchange', 'sta', 'colo', 'accCode', 'isMsg', 'prev_med', 'prev_95p', 'cur_med', 'cur_95p']\n",
    "body += convertToHtml(result,title)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "body += '<p><b>IV. tickToMBD</b></p>'\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "checkLog = orderLog[~orderLog['start_time'].isnull()]\n",
    "checkLog = checkLog.drop_duplicates(['date', 'secid', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'], keep=False)\n",
    "checkLog = checkLog[~checkLog['accCode'].isnull()]\n",
    "checkLog['tag'] = 'previous'\n",
    "checkLog.loc[checkLog['date'] == orderLog['date'].max(), 'tag'] = 'current'\n",
    "\n",
    "cc1 = checkLog\n",
    "cc1 = cc1.reset_index(drop=True)\n",
    "cc1['ordering'] = cc1.index\n",
    "cc1['time_diff'] = cc1['caa_orderLog'] - cc1['start_time']\n",
    "cc1['colo1'] = cc1['colo'].str[:2] + cc1['colo'].str[3:5] + cc1['colo'].str[6:8]\n",
    "cc1['colo_broker'] = cc1['colo1'] + '_' + cc1[\"accCode\"].astype(int).astype(str)\n",
    "\n",
    "\n",
    "re1 = cc1.groupby(['tag', 'sta', 'colo', 'accCode'])['time_diff'].describe().fillna(0).astype(int).reset_index()\n",
    "# re1 = re1[re1['count'] > 20].reset_index()\n",
    "c1 = cc1.groupby(['tag', 'sta', 'colo', 'accCode'])['time_diff'].apply(lambda x: x.describe([0.1])['10%']).astype(int).reset_index()\n",
    "c1 = c1.rename(columns={\"time_diff\":\"10%\"})\n",
    "re1 = pd.merge(re1, c1[['tag', 'sta', 'colo', 'accCode', '10%']], on=['tag', 'sta', 'colo', 'accCode'])\n",
    "c1 = cc1.groupby(['tag', 'sta', 'colo', 'accCode'])['time_diff'].apply(lambda x: x.describe([0.9])['90%']).astype(int).reset_index()\n",
    "c1 = c1.rename(columns={\"time_diff\":\"90%\"})\n",
    "re1 = pd.merge(re1, c1[['tag', 'sta', 'colo', 'accCode', '90%']], on=['tag', 'sta', 'colo', 'accCode'])\n",
    "ree1 = re1[re1['tag'] == 'previous']\n",
    "ree1 = ree1.rename(columns={'50%':'prev_med', 'count':'prev_count'})\n",
    "ree2 = re1[re1['tag'] == 'current']\n",
    "ree2 = ree2.rename(columns={'50%':'cur_med', 'count':'cur_count'})\n",
    "report = pd.merge(ree1[['sta', 'colo', 'accCode', 'prev_count', 'prev_med']], \n",
    "               ree2[['sta', 'colo', 'accCode', 'cur_count', 'cur_med']], on=['sta', 'colo', 'accCode'])\n",
    "\n",
    "\n",
    "aa = cc1[(cc1['updateType'] == 0) & (cc1['tag'] == 'previous')].groupby(['date', 'sta', 'colo', 'accCode'])['time_diff'].median().reset_index()\n",
    "aa['count'] = aa.groupby(['sta', 'colo', 'accCode'])['date'].transform('nunique')\n",
    "aa1 = aa.groupby(['sta', 'colo', 'accCode'])['time_diff'].describe()[['mean', 'std']].reset_index()\n",
    "aa1 = aa1.fillna(0)\n",
    "aa = pd.merge(aa, aa1, on=['sta', 'colo', 'accCode'])\n",
    "for j in [1]:\n",
    "    aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "    aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "    aa['count1'] = np.where((aa['time_diff'] <= aa[str(j) + 'std_high']) & (aa['time_diff'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "    re1 = aa.groupby(['sta', 'colo', 'accCode'])['count1'].sum().reset_index()\n",
    "    re2 = aa.groupby(['sta', 'colo', 'accCode'])['count'].first().reset_index()\n",
    "    re1 = pd.merge(re2, re1, on=['sta', 'colo', 'accCode'])\n",
    "    re1['count1'] = re1['count1'] / re1['count']\n",
    "    re1 = re1.rename(columns={'count1':str(j)+'*std'})\n",
    "for j in [1.5, 2, 2.5, 3, 3.5, 4, 4.5]:\n",
    "    aa[str(j) + 'std_low'] = aa['mean'] - j*aa['std']\n",
    "    aa[str(j) + 'std_high'] = aa['mean'] + j*aa['std']\n",
    "    aa['count1'] = np.where((aa['time_diff'] <= aa[str(j) + 'std_high']) & (aa['time_diff'] >= aa[str(j) + 'std_low']), 1, 0)\n",
    "    re = aa.groupby(['sta', 'colo', 'accCode'])['count1'].sum().reset_index()\n",
    "    re2 = aa.groupby(['sta', 'colo', 'accCode'])['count'].first().reset_index()\n",
    "    re = pd.merge(re2, re, on=['sta', 'colo', 'accCode'])\n",
    "    re['count1'] = re['count1'] / re['count']\n",
    "    re = re.rename(columns={'count1':str(j)+'*std'})\n",
    "    re1 = pd.merge(re1, re[['sta', 'colo', 'accCode', str(j)+'*std']], on=['sta', 'colo', 'accCode'])\n",
    "nc = []\n",
    "for i in range(0, re1.shape[0]):\n",
    "    nc.append(np.float(re1.columns[6:][(re1.iloc[i, 6:] == 1)][0].split('*')[0]))\n",
    "re1['n'] = nc\n",
    "print(re1.shape[0])\n",
    "print(aa1.shape[0])\n",
    "aa1 = pd.merge(aa1, re1[['sta', 'colo', 'accCode', 'n']], on=['sta', 'colo', 'accCode'])\n",
    "aa1['min'] = aa1['mean'] - aa1['std'] * aa1['n']\n",
    "aa1['max'] = aa1['mean'] + aa1['std'] * aa1['n']\n",
    "report = pd.merge(report, aa1, on=['sta', 'colo', 'accCode'], how='left')\n",
    "assert(report[report['mean'].isnull()].shape[0] == 0)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(report[((report['cur_med'] > report['max']) | (report['cur_med'] < report['min'])) \\\n",
    "                    & (report['cur_count'] > 100) & (abs(report['cur_med'] - report['prev_med']) > 1000)] \\\n",
    "             .groupby(['sta', 'colo', 'accCode'])['prev_med', 'cur_med'].first().to_html()))\n",
    "report = report[((report['cur_med'] > report['max']) | (report['cur_med'] < report['min'])) \\\n",
    "                    & (report['cur_count'] > 100) & (abs(report['cur_med'] - report['prev_med']) > 1000)] \\\n",
    "             .groupby(['sta', 'colo', 'accCode'])['prev_med', 'cur_med'].first().reset_index()\n",
    "body += '<div>In the following cases, tickToMBD under given accCode pass the hurdle we set:<div>'\n",
    "result = [report['sta'].values, report['colo'].values, report['accCode'].values,\n",
    "         report['prev_med'].values, report['cur_med'].values]\n",
    "title = ['sta', 'colo', 'accCode', 'prev_med', 'cur_med']\n",
    "body += convertToHtml(result,title)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "body += '<p><b>V. order return</b></p>'\n",
    "checkLog = orderLog[orderLog['updateType'] == 4]\n",
    "if checkLog[checkLog['absFilledThisUpdate'] == 0].shape[0] != 0:\n",
    "    print('There are stocks with zero trade size')\n",
    "    print(checkLog[checkLog['absFilledThisUpdate'] == 0].groupby(['colo', 'accCode'])['secid'].size())\n",
    "    checkLog = checkLog[checkLog['absFilledThisUpdate'] != 0]\n",
    "if checkLog[checkLog['beta_60'].isnull()].shape[0] != 0:\n",
    "    print('There are stocks with null beta')\n",
    "    print(checkLog[checkLog['beta_60'].isnull()])\n",
    "    checkLog = checkLog[~checkLog['beta_60'].isnull()]\n",
    "checkLog['max_trade'] = checkLog.groupby('order')['absOrderSizeCumFilled'].transform('max')\n",
    "checkLog['last'] = 0\n",
    "checkLog.loc[checkLog['max_trade'] == checkLog['absOrderSizeCumFilled'], 'last'] = 1\n",
    "checkLog[\"buyRet\"] = np.where(checkLog[\"orderDirection\"].isin([1, 2]), checkLog[\"adjMid_F90s\"] / checkLog[\"tradePrice\"] - 1, np.nan)\n",
    "checkLog[\"buyRet1\"] = np.where(checkLog[\"orderDirection\"].isin([1, 2]), checkLog[\"adjMid_F300s\"] / checkLog[\"tradePrice\"] - 1, np.nan)\n",
    "checkLog[\"sellRet\"] = np.where(checkLog[\"orderDirection\"].isin([-1, -2]), checkLog[\"tradePrice\"] / checkLog[\"adjMid_F90s\"] - 1, np.nan)\n",
    "checkLog[\"sellRet1\"] = np.where(checkLog[\"orderDirection\"].isin([-1, -2]), checkLog[\"tradePrice\"] / checkLog[\"adjMid_F300s\"] - 1, np.nan)\n",
    "checkLog[\"buyNum\"] = np.where((checkLog[\"orderDirection\"].isin([1, 2])) & (checkLog['last'] == 1), 1, 0)\n",
    "checkLog[\"sellNum\"] = np.where((checkLog[\"orderDirection\"].isin([-1, -2])) & (checkLog['last'] == 1), 1, 0)\n",
    "checkLog[\"server\"] = checkLog[\"colo\"].apply(lambda x: x.split(\"_\")[0] + x.split(\"_\")[1] + x.split(\"_\")[2])\n",
    "checkLog[\"server_account\"] = checkLog[\"server\"] + '_' + checkLog['accCode'].astype('str')\n",
    "df = checkLog[(checkLog['ars']%10 == 1)]\n",
    "\n",
    "df['tradeNtl'] = df['tradePrice']*df['absFilledThisUpdate']\n",
    "df[\"buyNtl\"] = np.where(~df[\"buyRet\"].isnull(), df[\"tradeNtl\"], np.nan)\n",
    "df[\"sellNtl\"] = np.where(~df[\"sellRet\"].isnull(), df[\"tradeNtl\"], np.nan)\n",
    "df[\"sumbuyNtl\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"buyNtl\"].transform(sum)\n",
    "df[\"sumsellNtl\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"sellNtl\"].transform(sum)\n",
    "\n",
    "df[\"sumsellRet\"] = df[\"tradeNtl\"] * df[\"sellRet\"]\n",
    "df[\"sumsellRet\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"sumsellRet\"].transform(sum)\n",
    "\n",
    "\n",
    "df[\"sumbuyRet\"] = df[\"tradeNtl\"] * df[\"buyRet\"]\n",
    "df[\"sumbuyRet\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"sumbuyRet\"].transform(sum)\n",
    "\n",
    "df[\"buyRet\"] = df[\"sumbuyRet\"] / df[\"sumbuyNtl\"]\n",
    "df[\"sellRet\"] = df[\"sumsellRet\"] / df[\"sumsellNtl\"]\n",
    "df[\"buyOrderNum\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"buyNum\"].transform(sum)\n",
    "df[\"sellOrderNum\"] = df.groupby([\"date\", \"exchange\", \"sta\", \"server_account\"])[\"sellNum\"].transform(sum)\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "for col in [\"buyRet\", \"sellRet\"]:\n",
    "    df[col] = (df[col] * 10000).round(2)\n",
    "\n",
    "re = df.groupby([\"exchange\", \"date\", \"sta\", \"server_account\"])[\"buyOrderNum\", \"buyRet\", \"sellOrderNum\", \"sellRet\"].first().reset_index()\n",
    "re1 = re[(re['sellRet'] < -20) & (re['sellOrderNum'] > 100)]\n",
    "re2 = re[(re['buyRet'] < -20) & (re['buyOrderNum'] > 100)]\n",
    "\n",
    "display(HTML(re1.groupby([\"exchange\", 'date', 'sta', \"server_account\"])[['sellOrderNum', 'sellRet']].first().to_html()))\n",
    "display(HTML(re2.groupby([\"exchange\", 'date', 'sta', \"server_account\"])[['buyOrderNum', 'buyRet']].first().to_html()))\n",
    "pr1 = re1[re1['date'] == checkLog['date'].max()].groupby([\"exchange\", 'date', 'sta', \"server_account\"])[['sellOrderNum', 'sellRet']].first().reset_index()\n",
    "pr2 = re2[re2['date'] == checkLog['date'].max()].groupby([\"exchange\", 'date', 'sta', \"server_account\"])[['buyOrderNum', 'buyRet']].first().reset_index()\n",
    "\n",
    "if pr1.shape[0] != 0:\n",
    "    body += '<div>In the following cases, sell order return under given strategy and accCode < -20bps:<div>'\n",
    "    result = [pr1['exchange'].values, pr1['date'].values, pr1['sta'].values,\n",
    "             pr1['server_account'].values, pr1['sellOrderNum'].values, pr1['sellRet'].values]\n",
    "    title = ['exchange', 'date', 'sta', 'server_account', 'sellOrderNum', 'sellRet']\n",
    "    body += convertToHtml(result,title)\n",
    "\n",
    "if pr2.shape[0] != 0:\n",
    "    body += '<div>In the following cases, buy order return under given strategy and accCode < -20bps:<div>'\n",
    "    result = [pr2['exchange'].values, pr2['date'].values, pr2['sta'].values,\n",
    "             pr2['server_account'].values, pr2['buyOrderNum'].values, pr2['buyRet'].values]\n",
    "    title = ['exchange', 'date', 'sta', 'server_account', 'buyOrderNum', 'buyRet']\n",
    "    body += convertToHtml(result,title)\n",
    "\n",
    "pr1 = re1.groupby(['exchange', 'sta', 'server_account'])['date'].size()[\n",
    "    re1.groupby(['exchange', 'sta', 'server_account'])['date'].size() >= 2].reset_index().sort_values(by='date', ascending=False)\n",
    "pr2 = re2.groupby(['exchange', 'sta', 'server_account'])['date'].size()[\n",
    "    re2.groupby(['exchange', 'sta', 'server_account'])['date'].size() >= 2].reset_index().sort_values(by='date', ascending=False)\n",
    "add1 = pd.merge(re, pr1, on=['exchange', 'sta', 'server_account'], how='inner')\n",
    "add1 = add1[(add1['sellRet'] < -20) & (add1['sellOrderNum'] > 100)]\n",
    "add2 = pd.merge(re, pr2, on=['exchange', 'sta', 'server_account'], how='inner')\n",
    "add2 = add2[(add2['buyRet'] < -20) & (add2['buyOrderNum'] > 100)]\n",
    "\n",
    "if add1.shape[0] != 0:\n",
    "    add1 = add1.groupby(['exchange', 'sta', 'server_account'])['sellRet'].describe()[['count', 'mean', 'min', 'max']].reset_index().sort_values(by='count', ascending=False)\n",
    "    for i in ['mean', 'min', 'max']:\n",
    "        add1[i] = add1[i].round(2)\n",
    "    for i in ['count']:\n",
    "        add1[i] = add1[i].astype(int)\n",
    "    body += '<div>In the following cases, more than 2 days sell order return under given strategy and accCode < -20bps in previous 10 days (include today):<div>'\n",
    "    result = [add1['exchange'].values, add1['sta'].values,\n",
    "             add1['server_account'].values, add1['count'].values, add1['mean'].values, add1['min'].values, add1['max'].values]\n",
    "    title = ['exchange', 'sta', 'server_account', 'count', 'mean', 'min', 'max']\n",
    "    body += convertToHtml(result,title)\n",
    "\n",
    "if add2.shape[0] != 0:\n",
    "    add2 = add2.groupby(['exchange', 'sta', 'server_account'])['buyRet'].describe()[['count', 'mean', 'min', 'max']].reset_index().sort_values(by='count', ascending=False)\n",
    "    for i in ['mean', 'min', 'max']:\n",
    "        add2[i] = add2[i].round(2)\n",
    "    for i in ['count']:\n",
    "        add2[i] = add2[i].astype(int)\n",
    "    body += '<div>In the following cases, more than 2 days buy order return under given strategy and accCode < -20bps in previous 10 days (include today):<div>'\n",
    "    result = [add2['exchange'].values, add2['sta'].values,\n",
    "             add2['server_account'].values, add2['count'].values, add2['mean'].values, add2['min'].values, add2['max'].values]\n",
    "    title = ['exchange', 'sta', 'server_account', 'count', 'mean', 'min', 'max']\n",
    "    body += convertToHtml(result,title)\n",
    "\n",
    "\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart    \n",
    "\n",
    "title = str(orderLog['date'].max()) + ' daily report'\n",
    "body = body + \"</div></body></html>\"\n",
    "smtp_server = '42.120.226.4' # 'smtp.mxhichina.com'\n",
    "user = 'zhenyu.yin@general-int.com'\n",
    "passwd = 'Yqzy0063!'\n",
    "from_addr = 'zhenyu.yin@general-int.com'\n",
    "to_addr = ['zhenyu.yin@general-int.com', 'kevin.zhang@general-int.com']\n",
    "#     to_addr = ['zhenyu.yin@general-int.com']\n",
    "\n",
    "msg = MIMEMultipart()\n",
    "msg['From'] = from_addr\n",
    "msg['To'] = ', '.join(to_addr)\n",
    "msg['Subject'] = title\n",
    "txt = MIMEText(body, _subtype='html', _charset='UTF-8')\n",
    "msg.attach(txt)\n",
    "\n",
    "smtp = None\n",
    "while True:\n",
    "    try:\n",
    "        smtp = smtplib.SMTP(smtp_server)\n",
    "        print('smtp server connected')\n",
    "        smtp.login(user, passwd)\n",
    "        print('login')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "print('send mail')\n",
    "smtp.sendmail(from_addr, to_addr, msg.as_string())\n",
    "smtp.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exchange</th>\n",
       "      <th>date</th>\n",
       "      <th>sta</th>\n",
       "      <th>server_account</th>\n",
       "      <th>sellOrderNum</th>\n",
       "      <th>sellRet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SSE</td>\n",
       "      <td>20210108</td>\n",
       "      <td>1. staone</td>\n",
       "      <td>zt5205_527601</td>\n",
       "      <td>601</td>\n",
       "      <td>50.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  exchange      date        sta server_account  sellOrderNum  sellRet\n",
       "0      SSE  20210108  1. staone  zt5205_527601           601    50.23"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date      accCode  ars      \n",
       "20201217  8854     1.0           719\n",
       "                   2.0           216\n",
       "                   3.0           246\n",
       "          966301   0.0           137\n",
       "20201218  8854     0.0             2\n",
       "                   1.0           633\n",
       "                   2.0           195\n",
       "                   3.0           249\n",
       "          966301   0.0            65\n",
       "20201221  8854     0.0             4\n",
       "                   1.0           923\n",
       "                   2.0           252\n",
       "                   3.0           311\n",
       "          966301   0.0            41\n",
       "20201222  8854     0.0             2\n",
       "                   1.0          1385\n",
       "                   2.0           307\n",
       "                   3.0           369\n",
       "          966301   0.0            83\n",
       "20201223  8854     0.0             2\n",
       "                   1.0          1256\n",
       "                   2.0           376\n",
       "                   3.0           399\n",
       "          966301   0.0            59\n",
       "20201224  8854     0.0             3\n",
       "                   1.0          1090\n",
       "                   2.0           352\n",
       "                   3.0           170\n",
       "                   3093885.0       1\n",
       "          966301   0.0            46\n",
       "20201225  8854     0.0             2\n",
       "                   1.0          1247\n",
       "                   2.0           423\n",
       "                   3.0           361\n",
       "          966301   0.0            95\n",
       "20201228  8854     0.0          1067\n",
       "          966301   0.0            77\n",
       "20201229  8854     0.0             2\n",
       "                   1.0          1741\n",
       "          966301   1.0            99\n",
       "                   2.0            12\n",
       "                   3.0            35\n",
       "20201230  8854     0.0             3\n",
       "                   1.0           663\n",
       "          966301   1.0            33\n",
       "                   2.0             3\n",
       "                   3.0            16\n",
       "20201231  8854     0.0             1\n",
       "                   1.0          1123\n",
       "          966301   1.0            80\n",
       "                   2.0            16\n",
       "                   3.0            23\n",
       "Name: order, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderLog[orderLog['updateType'] == 0].groupby(['date', 'accCode', 'ars'])['order'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date      accCode  ars      \n",
       "20201217  8854     1.0           719\n",
       "                   2.0           216\n",
       "                   3.0           246\n",
       "          966301   0.0           137\n",
       "20201218  8854     0.0             2\n",
       "                   1.0           633\n",
       "                   2.0           195\n",
       "                   3.0           249\n",
       "          966301   0.0            65\n",
       "20201221  8854     0.0             4\n",
       "                   1.0           923\n",
       "                   2.0           252\n",
       "                   3.0           311\n",
       "          966301   0.0            41\n",
       "20201222  8854     0.0             2\n",
       "                   1.0          1385\n",
       "                   2.0           307\n",
       "                   3.0           369\n",
       "          966301   0.0            83\n",
       "20201223  8854     0.0             2\n",
       "                   1.0          1256\n",
       "                   2.0           376\n",
       "                   3.0           399\n",
       "          966301   0.0            59\n",
       "20201224  8854     0.0             3\n",
       "                   1.0          1090\n",
       "                   2.0           352\n",
       "                   3.0           170\n",
       "                   3093885.0       1\n",
       "          966301   0.0            46\n",
       "20201225  8854     0.0             2\n",
       "                   1.0          1247\n",
       "                   2.0           423\n",
       "                   3.0           361\n",
       "          966301   0.0            95\n",
       "20201228  8854     0.0          1067\n",
       "          966301   0.0            77\n",
       "20201229  8854     0.0             2\n",
       "                   1.0          1741\n",
       "          966301   1.0            99\n",
       "                   2.0            12\n",
       "                   3.0            35\n",
       "20201230  8854     0.0             3\n",
       "                   1.0           663\n",
       "          966301   1.0            33\n",
       "                   2.0             3\n",
       "                   3.0            16\n",
       "20201231  8854     0.0             1\n",
       "                   1.0          1123\n",
       "          966301   1.0            80\n",
       "                   2.0            16\n",
       "                   3.0            23\n",
       "Name: order, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orderLog[orderLog['updateType'] == 0].groupby(['date', 'accCode', 'ars'])['order'].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
