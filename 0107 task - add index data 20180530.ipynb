{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20180530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index finished\n"
     ]
    }
   ],
   "source": [
    "import pymongo \n",
    "import io \n",
    "import pandas as pd \n",
    "import pickle \n",
    "import datetime \n",
    "import time \n",
    "import gzip \n",
    "import lzma \n",
    "import pytz \n",
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq \n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db', version=3): \n",
    "        self.db_name = db_name \n",
    "        self.uri = uri \n",
    "        self.client = pymongo.MongoClient(self.uri) \n",
    "        self.db = self.client[self.db_name] \n",
    "        self.chunk_size = 20000 \n",
    "        self.symbol_column = symbol_column \n",
    "        self.date_column = 'date' \n",
    "        self.version = version\n",
    "\n",
    "    def parse_uri(self, uri): \n",
    "        # mongodb://user:password@example.com \n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"date must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid date type: \" + str(type(x)))\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "        return query\n",
    "\n",
    "    def read_tick(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name] \n",
    "        query = self.build_query(start_date, end_date, symbol) \n",
    "        if not query: \n",
    "            print('cannot read the whole table') \n",
    "            return None  \n",
    "        segs = [] \n",
    "        for x in collection.find(query): \n",
    "            x['data'] = self.deser(x['data'], x['ver']) \n",
    "            segs.append(x) \n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start'])) \n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def read_daily(self, table_name, start_date=None, end_date=None, skey=None, index_id=None, interval=None, index_name=None, col=None, return_sdi=True): \n",
    "        collection = self.db[table_name]\n",
    "        # Build projection \n",
    "        prj = {'_id': 0} \n",
    "        if col is not None: \n",
    "            if return_sdi: \n",
    "                col = ['skey', 'date', 'index_id'] + col \n",
    "            for col_name in col: \n",
    "                prj[col_name] = 1 \n",
    "        # Build query \n",
    "        query = {} \n",
    "        if skey is not None: \n",
    "            query['skey'] = {'$in': skey} \n",
    "        if interval is not None: \n",
    "            query['interval'] = {'$in': interval} \n",
    "        if index_id is not None: \n",
    "            query['index_id'] = {'$in': index_id}    \n",
    "        if index_name is not None:\n",
    "            n = '' \n",
    "            for name in index_name: \n",
    "                try: \n",
    "                    name = re.compile('[\\u4e00-\\u9fff]+').findall(name)[0] \n",
    "                    if len(n) == 0: \n",
    "                        n = n = \"|\".join(name) \n",
    "                    else: \n",
    "                        n = n + '|' + \"|\".join(name) \n",
    "                except: \n",
    "                    if len(n) == 0: \n",
    "                        n = name \n",
    "                    else: \n",
    "                        n = n + '|' + name \n",
    "            query['index_name'] = {'$regex': n}\n",
    "        if start_date is not None: \n",
    "            if end_date is not None: \n",
    "                query['date'] = {'$gte': start_date, '$lte': end_date} \n",
    "            else: \n",
    "                query['date'] = {'$gte': start_date} \n",
    "        elif end_date is not None: \n",
    "            query['date'] = {'$lte': end_date} \n",
    "        # Load data \n",
    "        cur = collection.find(query, prj) \n",
    "        df = pd.DataFrame.from_records(cur) \n",
    "        if df.empty: \n",
    "            df = pd.DataFrame() \n",
    "        else:\n",
    "            if 'index_id' in df.columns:\n",
    "                df = df.sort_values(by=['date', 'index_id', 'skey']).reset_index(drop=True)\n",
    "            else:\n",
    "                df = df.sort_values(by=['date','skey']).reset_index(drop=True)\n",
    "        return df \n",
    " \n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = self.version\n",
    "            ser_data = self.ser(df_seg, version)\n",
    "            seg = {'ver': version, 'data': ser_data, 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        elif version == 3:\n",
    "            # 32-bit number needs more space than 64-bit for parquet\n",
    "            for col_name in s.columns:\n",
    "                col = s[col_name]\n",
    "                if col.dtype == np.int32:\n",
    "                    s[col_name] = s[col_name].astype(np.int64)\n",
    "                elif col.dtype == np.uint32:\n",
    "                    s[col_name] = s[col_name].astype(np.uint64)\n",
    "            tbl = pa.Table.from_pandas(s)\n",
    "            f = io.BytesIO()\n",
    "            pq.write_table(tbl, f, use_dictionary=False, compression='ZSTD', compression_level=0)\n",
    "            f.seek(0)\n",
    "            data = f.read()\n",
    "            return data\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        print(version)\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        elif version == 3:\n",
    "            f = io.BytesIO()\n",
    "            f.write(s)\n",
    "            f.seek(0)\n",
    "            return pq.read_table(f, use_threads=False).to_pandas()\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "pd.set_option(\"max_columns\", 200)\n",
    "\n",
    "\n",
    "    \n",
    "readPath = r'K:\\data\\2018\\201805\\20180530\\SH\\snapshot\\***2\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs == 16) | (dateLs == 300) | (dateLs == 852) | (dateLs == 905)]\n",
    "SH = []\n",
    "ll = []\n",
    "for i in dataPathLs:\n",
    "    try:\n",
    "        df = pd.read_csv(i, usecols = [17,19,20,21,22,34,41,42])\n",
    "    except:\n",
    "        print(\"empty data\")\n",
    "        print(i)\n",
    "        ll.append(int(os.path.basename(i).split('.')[0]))\n",
    "        continue\n",
    "    df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "    SH += [df]\n",
    "del df\n",
    "SH = pd.concat(SH).reset_index(drop=True)\n",
    "\n",
    "SH[\"skey\"] = SH[\"StockID\"] + 1000000\n",
    "SH.drop([\"StockID\"],axis=1,inplace=True)\n",
    "SH[\"date\"] = int(SH[\"SendingTime\"].iloc[0]//1000000000)\n",
    "SH[\"time\"] = (SH['SendingTime'] - int(SH['SendingTime'].iloc[0]//1000000000*1000000000)).astype(np.int64) * 1000\n",
    "SH[\"clockAtArrival\"] = SH[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "SH.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "SH['datetime'] = SH[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "\n",
    "SH.columns = ['cum_volume', 'open','high', 'prev_close', 'low', 'close', 'cum_amount', 'skey', \n",
    "              'date', 'time', 'clockAtArrival', 'datetime']\n",
    "SH = SH.fillna(0)\n",
    "SH = SH.drop_duplicates(['cum_volume', 'open','high', 'prev_close', 'low', 'close', 'cum_amount', 'skey', \n",
    "              'date', 'time', 'clockAtArrival', 'datetime'])\n",
    "assert(sum(SH['time']%1000000) == 0)\n",
    "assert(sum(SH[SH['cum_volume'] == 0].groupby('skey')['time'].max() \n",
    "           < SH[SH['cum_volume'] > 0].groupby('skey')['time'].min()))\n",
    "m_ax = SH[SH['time'] <= 150500000000].groupby('skey').last()['time'].min()\n",
    "assert((SH[SH['time'] >= m_ax].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                           'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0) & \\\n",
    "           (sum(SH[SH['time'] >= m_ax].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "SH = SH[(SH['cum_volume'] > 0) & (SH['time'] <= 150500000000)]\n",
    "\n",
    "k1 = SH.groupby('skey')['datetime'].min().reset_index()\n",
    "k1 = k1.rename(columns={'datetime':'min'})\n",
    "k2 = SH.groupby('skey')['datetime'].max().reset_index()\n",
    "k2 = k2.rename(columns={'datetime':'max'})\n",
    "k = pd.merge(k1, k2, on='skey')\n",
    "k['diff'] = (k['max']-k['min']).apply(lambda x: x.seconds)\n",
    "df = pd.DataFrame()\n",
    "for i in np.arange(k.shape[0]):\n",
    "    df1 = pd.DataFrame()\n",
    "    df1['datetime1'] = [k.loc[i, 'min'] + datetime.timedelta(seconds=int(x)) for x in np.arange(0, k.loc[i, 'diff'] + 1)]\n",
    "    df1['skey'] = k.loc[i, 'skey']\n",
    "    assert(df1['datetime1'].min() == k.loc[i, 'min'])\n",
    "    assert(df1['datetime1'].max() == k.loc[i, 'max'])\n",
    "    df = pd.concat([df, df1])\n",
    "\n",
    "SH = pd.merge(SH, df, left_on=['skey', 'datetime'], right_on=['skey', 'datetime1'], how='outer').sort_values(by=['skey', 'datetime1']).reset_index(drop=True)\n",
    "assert(SH[SH['datetime1'].isnull()].shape[0] == 0)\n",
    "for cols in ['date', 'cum_volume', 'cum_amount', 'prev_close', 'open', 'high', 'low', 'close']:\n",
    "    SH[cols] = SH.groupby('skey')[cols].ffill()\n",
    "SH.drop([\"datetime\"],axis=1,inplace=True)\n",
    "SH = SH.rename(columns={'datetime1':'datetime'})\n",
    "SH['date'] = SH['date'].iloc[0]\n",
    "SH['date'] = SH['date'].astype('int32')\n",
    "SH['skey'] = SH['skey'].astype('int32')\n",
    "SH[\"time\"] = SH['datetime'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(':', \"\"))).astype(np.int64)\n",
    "SH['SendingTime'] = (SH['date'].astype('int64')) * 1000000 + SH['time']\n",
    "SH[\"clockAtArrival\"] = SH[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "SH.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "SH['time'] = SH['time'] * 1000000\n",
    "\n",
    "assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "SH[\"prev_close\"] = np.where(SH[\"time\"] >= 91500000000, SH.groupby(\"skey\")[\"prev_close\"].transform(\"max\"), SH[\"prev_close\"]) \n",
    "SH[\"open\"] = np.where(SH[\"cum_volume\"] > 0, SH.groupby(\"skey\")[\"open\"].transform(\"max\"), SH[\"open\"])\n",
    "assert(sum(SH[SH[\"open\"] != 0].groupby(\"skey\")[\"open\"].nunique() != 1) == 0)\n",
    "assert(sum(SH[SH[\"prev_close\"] != 0].groupby(\"skey\")[\"prev_close\"].nunique() != 1) == 0)\n",
    "assert(SH[SH[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "\n",
    "for cols in ['open', 'high', 'prev_close', 'low', 'close']:\n",
    "    SH[cols] = SH[cols].apply(lambda x: round(x, 4)).astype('float64')\n",
    "\n",
    "SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"cum_volume\", \"cum_amount\", \n",
    "         \"prev_close\", \"open\", \"high\", \"low\", \"close\"]]    \n",
    "m_in = SH[SH['time'] <= 113500000000].groupby('skey').last()['time'].min()\n",
    "m_ax = SH[SH['time'] >= 125500000000].groupby('skey').first()['time'].max()\n",
    "assert((SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open', 'high', 'low', 'prev_close', \n",
    "                                           'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0)\n",
    "      & (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "       (sum(SH[(SH['time'] >= m_in) & (SH['time'] <= m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "SH = pd.concat([SH[SH['time'] <= 113500000000], SH[SH['time'] >= 125500000000]])\n",
    "\n",
    "SH = SH.sort_values(by=['skey', 'time', 'cum_volume'])\n",
    "SH[\"ordering\"] = SH.groupby(\"skey\").cumcount()\n",
    "SH[\"ordering\"] = SH[\"ordering\"] + 1\n",
    "SH['ordering'] = SH['ordering'].astype('int32')\n",
    "SH['cum_volume'] = SH['cum_volume'].astype('int64')\n",
    "\n",
    "SH = SH[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"ordering\", \"cum_volume\", \"cum_amount\", \n",
    "         \"open\", \"close\"]]\n",
    "\n",
    "display(SH[\"date\"].iloc[0])\n",
    "print(\"index finished\")\n",
    "\n",
    "database_name = 'com_md_eq_cn'\n",
    "user = \"zhenyuy\"\n",
    "password = \"bnONBrzSMGoE\"\n",
    "\n",
    "db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "db1.write('md_index', SH)\n",
    "\n",
    "del SH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
