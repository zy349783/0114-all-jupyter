{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_data(dd):\n",
    "    \n",
    "    import os\n",
    "    import glob\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import pymongo\n",
    "    import io\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import datetime\n",
    "    import time\n",
    "    import gzip\n",
    "    import lzma\n",
    "    import pytz\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    import numpy as np\n",
    "\n",
    "    print('start processing raw Data')\n",
    "    startTm = datetime.datetime.now()\n",
    "\n",
    "    startDate = dd\n",
    "    endDate = dd\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    dataPathLs = np.array(glob.glob(os.path.join(readPath, 'speedCompare***.csv')))\n",
    "    dateLs = np.array([os.path.basename(i).split('_')[1].split('.')[0] for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "    dateLs = dateLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "    date = dateLs[0]\n",
    "    assert(date == dd)\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    orderLog = pd.read_csv(os.path.join(readPath, 'speedCompare_%s.csv'%date))\n",
    "\n",
    "    for col in ['clockAtArrival', 'secid', 'updateType', 'vai', 'absFilledThisUpdate', 'orderDirection', 'absOrderSize',\n",
    "            'absOrderSizeCumFilled', 'date', 'accCode', 'mse']:\n",
    "        orderLog[col] = orderLog[col].fillna(0)\n",
    "        orderLog[col] = orderLog[col].astype('int64')\n",
    "\n",
    "    orderLog = orderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'clockAtArrival']).reset_index(drop=True)\n",
    "    orderLog = orderLog[orderLog[\"secid\"] >= 1000000]\n",
    "\n",
    "    targetStock = orderLog['secid'].unique()\n",
    "    targetStock = np.array([int(str(i)[1:]) for i in targetStock])\n",
    "    targetStockSZ = sorted(targetStock[targetStock < 600000])\n",
    "    targetStockSH = sorted(targetStock[targetStock >= 600000])\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.34\\\\random_backup\\\\Kevin_zhenyu\\\\rawData'\n",
    "    mdOrderLogPath = glob.glob(os.path.join(readPath, 'logs_%s_zs_92_01***'%date, 'mdOrderLog***.csv'))[-1]\n",
    "    mdTradeLogPath = glob.glob(os.path.join(readPath, 'logs_%s_zs_92_01***'%date, 'mdTradeLog***.csv'))[-1]\n",
    "\n",
    "    mdOrderLog = pd.read_csv(mdOrderLogPath)\n",
    "    mdOrderLog = mdOrderLog[mdOrderLog['SecurityID'].isin(targetStockSZ)]\n",
    "    mdOrderLog['OrderType'] = mdOrderLog['OrderType'].astype(str)\n",
    "    mdOrderLog = mdOrderLog[['clockAtArrival', 'sequenceNo', 'TransactTime', 'SecurityID', 'ApplSeqNum', 'Side',\n",
    "                         'OrderType', 'Price', 'OrderQty']]\n",
    "\n",
    "    mdTradeLog = pd.read_csv(mdTradeLogPath, encoding='utf-8')\n",
    "    mdTradeLog['ExecType'] = mdTradeLog['ExecType'].astype(str)\n",
    "    mdTradeLog = mdTradeLog[mdTradeLog['SecurityID'].isin(targetStockSZ)]\n",
    "    mdTradeLog['volumeThisUpdate'] = np.where(mdTradeLog['ExecType'] == 'F', mdTradeLog['TradeQty'], 0)\n",
    "    mdTradeLog['cum_volume'] = mdTradeLog.groupby(['SecurityID'])['volumeThisUpdate'].cumsum()\n",
    "    mdTradeLog = mdTradeLog[['clockAtArrival', 'sequenceNo', 'TransactTime', 'SecurityID', 'ApplSeqNum', 'cum_volume',\n",
    "                         'ExecType', 'TradePrice', 'TradeQty', 'TradeMoney', 'BidApplSeqNum', 'OfferApplSeqNum']]\n",
    "\n",
    "    rawMsgDataSZ = pd.concat([mdOrderLog, mdTradeLog], sort=False)\n",
    "    del mdOrderLog\n",
    "    del mdTradeLog\n",
    "\n",
    "    rawMsgDataSZ = rawMsgDataSZ.sort_values(by=['sequenceNo']).reset_index(drop=True)\n",
    "\n",
    "    rawMsgDataSZ['cum_volume'] = rawMsgDataSZ.groupby(['SecurityID'])['cum_volume'].ffill().bfill()\n",
    "    rawMsgDataSZ['ExecType'] = rawMsgDataSZ['ExecType'].fillna('2')\n",
    "    rawMsgDataSZ['TradeQty'] = rawMsgDataSZ['TradeQty'].fillna(0)\n",
    "\n",
    "    saveCols = ['clockAtArrival', 'sequenceNo', 'TransactTime', 'SecurityID', 'cum_volume', 'ApplSeqNum', \n",
    "            'Side', 'OrderType', 'Price', 'OrderQty', 'ExecType', 'TradePrice', 'TradeQty', 'TradeMoney',\n",
    "            'BidApplSeqNum', 'OfferApplSeqNum']\n",
    "    rawMsgDataSZ = rawMsgDataSZ[saveCols]\n",
    "    savePath = 'L:\\\\orderLog\\\\mdData'\n",
    "    rawMsgDataSZ.to_pickle(os.path.join(savePath, 'mdLog_msg_%s.pkl'%startDate))\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    print('start tickToMBD')\n",
    "\n",
    "    pd.set_option('max_rows', 200)\n",
    "    pd.set_option('max_columns', 200)\n",
    "\n",
    "    perc = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "    startTm = datetime.datetime.now()\n",
    "\n",
    "    startDate = dd\n",
    "    endDate = dd\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    dataPathLs = np.array(glob.glob(os.path.join(readPath, 'speedCompare***.csv')))\n",
    "    dateLs = np.array([os.path.basename(i).split('_')[1].split('.')[0] for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "    dateLs = dateLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "    thisDate = dateLs[0]\n",
    "    assert(thisDate == dd)\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    rawOrderLog = pd.read_csv(os.path.join(readPath, 'speedCompare_%s.csv'%thisDate))\n",
    "    for col in ['clockAtArrival', 'caamd', 'secid', 'updateType', 'vai', 'absFilledThisUpdate', 'orderDirection', 'absOrderSize',\n",
    "                'absOrderSizeCumFilled', 'date', 'accCode', 'mse']:\n",
    "        rawOrderLog[col] = rawOrderLog[col].fillna(0)\n",
    "        rawOrderLog[col] = rawOrderLog[col].astype('int64')   \n",
    "    rawOrderLog = rawOrderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'clockAtArrival']).reset_index(drop=True)\n",
    "    original_data = rawOrderLog.copy()\n",
    "\n",
    "    rawOrderLog = rawOrderLog[rawOrderLog[\"secid\"] >= 1000000]\n",
    "\n",
    "    display('There are accounts with duplicated ticks:')\n",
    "    display(rawOrderLog[rawOrderLog.duplicated(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep=False)]\\\n",
    "    .groupby(['date', 'colo', 'accCode'])['ars'].size())\n",
    "    rawOrderLog = rawOrderLog.drop_duplicates(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep='first')\n",
    "\n",
    "    display('There are ticks with orderDirection 0')\n",
    "    display(rawOrderLog[rawOrderLog['orderDirection'] == 0][['date', 'colo', 'accCode', \\\n",
    "                'secid', 'vai', 'updateType', 'sdd', 'orderDirection', 'absOrderSize', 'internalId', 'orderId']])\n",
    "\n",
    "    assert(rawOrderLog[rawOrderLog['updateType'] == 0][rawOrderLog[rawOrderLog['updateType'] == 0]\\\n",
    "                                                       .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                    'vai', 'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "    try:\n",
    "        assert(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "    except:\n",
    "        print('There are orders with all things same except sdd')\n",
    "        print(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId'], keep=False)])\n",
    "        assert(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId', 'sdd'], keep=False)].shape[0] == 0)\n",
    "    try:\n",
    "        assert(sum(rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() != 1) == 0) \n",
    "    except:\n",
    "        print('There are orders with same internalId but different orderId other than accCode 8856 case')\n",
    "        print(rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique()[rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() > 1])\n",
    "\n",
    "    r2 = rawOrderLog[(rawOrderLog['accCode'] != 8856) & (rawOrderLog['orderDirection'] != 0)]\n",
    "    r1 = rawOrderLog[(rawOrderLog['accCode'] == 8856) & (rawOrderLog['orderDirection'] != 0)]\n",
    "    r1['test'] = r1.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize']).grouper.group_info[0]\n",
    "    r1 = r1.sort_values(by=['test', 'clockAtArrival'])\n",
    "    r1.loc[r1['updateType'] != 0, 'vai'] = np.nan\n",
    "    r1['vai'] = r1.groupby('test')['vai'].ffill()\n",
    "    r2['test'] = r2.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize', 'internalId']).grouper.group_info[0]\n",
    "    r2 = r2.sort_values(by=['test', 'clockAtArrival'])\n",
    "    r2.loc[r2['updateType'] != 0, 'vai'] = np.nan\n",
    "    r2['vai'] = r2.groupby('test')['vai'].ffill()\n",
    "    try:\n",
    "        assert(sum(r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "    except:\n",
    "        print('There are orders in 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        a = r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique()[r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1].reset_index()\n",
    "        print(pd.merge(r1, a[['test', 'vai']], on=['test', 'vai'], how='inner')[['secid', 'accCode', 'colo', 'vai', 'updateType', 'sdd', 'internalId', 'orderId', 'absOrderSize', 'absFilledThisUpdate', 'absOrderSizeCumFilled', 'orderPrice', 'tradePrice']])\n",
    "    try:\n",
    "        assert(sum(r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "    except:\n",
    "        print('There are orders out of 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        a = r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique()[r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1].reset_index()\n",
    "        print(pd.merge(r2, a[['test', 'vai']], on=['test', 'vai'], how='inner')[['secid', 'accCode', 'colo', 'vai', 'updateType', 'sdd', 'internalId', 'orderId', 'absOrderSize', 'absFilledThisUpdate', 'absOrderSizeCumFilled', 'orderPrice', 'tradePrice']])\n",
    "    rawOrderLog = pd.concat([r1, r2])\n",
    "    del r1\n",
    "    del r2  \n",
    "\n",
    "    rawOrderLog = rawOrderLog.sort_values(by=['date', 'colo', 'accCode', 'secid', 'vai', 'clockAtArrival']).reset_index(drop=True)\n",
    "    rawOrderLog['clock'] = rawOrderLog['clockAtArrival'].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    rawOrderLog[\"broker\"] = np.where(rawOrderLog[\"accCode\"].astype(str).apply(lambda x: len(x) == 6), rawOrderLog['accCode'] // 10000, rawOrderLog['accCode'] // 100)\n",
    "    rawOrderLog['colo_broker'] = rawOrderLog['colo'].str[:2] + '_' + rawOrderLog['broker'].astype('str')\n",
    "    rawOrderLog['colo_account'] = rawOrderLog['colo'].str[:2] + '_' + rawOrderLog['accCode'].astype('str')\n",
    "    rawOrderLog['order'] = rawOrderLog.groupby(['date', 'colo', 'accCode', 'secid', 'vai', 'orderDirection', 'absOrderSize', 'internalId']).grouper.group_info[0]\n",
    "    rawOrderLog['group'] = rawOrderLog.groupby(['date', 'secid', 'vai']).grouper.group_info[0]\n",
    "    rawOrderLog['startClock'] = rawOrderLog.groupby(['order'])['clockAtArrival'].transform('first')\n",
    "    rawOrderLog['duration'] = rawOrderLog['clockAtArrival'] - rawOrderLog['startClock']\n",
    "    rawOrderLog['orderPrice'] = rawOrderLog['orderPrice'].apply(lambda x: round(x, 2))\n",
    "    rawOrderLog['tradePrice'] = rawOrderLog['tradePrice'].apply(lambda x: round(x, 2))\n",
    "    rawOrderLog['orderDirection1'] = np.where(rawOrderLog[\"orderDirection\"] == -2, -1, np.where(\n",
    "        rawOrderLog[\"orderDirection\"] == 2, 1, rawOrderLog[\"orderDirection\"]))\n",
    "    rawOrderLog['sdd'] = rawOrderLog.groupby('order')['sdd'].transform('first')\n",
    "    rawOrderLog['caamd'] = rawOrderLog.groupby('order')['caamd'].transform('first')\n",
    "    rawOrderLog[\"ars\"] = rawOrderLog.groupby(['order'])['ars'].transform('first')\n",
    "    orderLog = rawOrderLog.copy()\n",
    "\n",
    "    orderLog['exchange'] = np.where(orderLog['secid'] >= 2000000, 'SZE', 'SSE')\n",
    "    orderLog['orderNtl'] = orderLog['orderPrice'] * orderLog['absOrderSize']\n",
    "    orderLog['tradeNtl'] = np.where(orderLog['updateType'] == 4, orderLog['tradePrice']*orderLog['absFilledThisUpdate'], 0)\n",
    "    orderLog['isMsg'] = np.where(orderLog['updateType'] == 0, \n",
    "                                 np.where(orderLog['mse'] == 100, 1, 0), np.nan)\n",
    "    orderLog['isMsg'] = orderLog.groupby(['order'])['isMsg'].ffill()\n",
    "    orderLog['firstUpdateType'] = orderLog.groupby(['order'])['updateType'].transform('first')\n",
    "    orderLog = orderLog[orderLog['firstUpdateType'] == 0]\n",
    "    orderLog['insertNum'] = np.where(orderLog['updateType'] == 0, 1, 0)\n",
    "    orderLog['insertNum'] = orderLog.groupby(['order'])['insertNum'].transform('sum')\n",
    "    orderLog = orderLog[orderLog['insertNum'] == 1]\n",
    "    orderLog = orderLog[orderLog['secid'] >= 2000000].reset_index(drop=True)\n",
    "    def getTuple(x):\n",
    "        return tuple(i for i in x)\n",
    "\n",
    "    # 1. market orders\n",
    "    orderDataSZ = rawMsgDataSZ[rawMsgDataSZ['ExecType'] == '2'][['SecurityID', 'ApplSeqNum', 'clockAtArrival', 'sequenceNo', 'Side', 'OrderQty', 'Price', 'cum_volume', \"TransactTime\"]].reset_index(drop=True)\n",
    "    orderDataSZ['updateType'] = 0\n",
    "    tradeDataSZ = pd.concat([rawMsgDataSZ[rawMsgDataSZ['ExecType'] == 'F'][['SecurityID', 'BidApplSeqNum', 'clockAtArrival', 'sequenceNo', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]],\n",
    "                             rawMsgDataSZ[rawMsgDataSZ['ExecType'] == 'F'][['SecurityID', 'OfferApplSeqNum', 'clockAtArrival', 'sequenceNo', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]]], sort=False)\n",
    "    tradeDataSZ['ApplSeqNum'] = np.where(tradeDataSZ['BidApplSeqNum'].isnull(), tradeDataSZ['OfferApplSeqNum'], tradeDataSZ['BidApplSeqNum'])\n",
    "    tradeDataSZ['Side'] = np.where(tradeDataSZ['BidApplSeqNum'].isnull(), 2, 1)\n",
    "    tradeDataSZ = tradeDataSZ[['SecurityID', 'ApplSeqNum', 'clockAtArrival', 'sequenceNo', 'Side', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]]\n",
    "    tradeDataSZ['updateType'] = 4\n",
    "    cancelDataSZ = rawMsgDataSZ[rawMsgDataSZ['ExecType'] == '4'][['SecurityID', 'BidApplSeqNum', 'OfferApplSeqNum', 'clockAtArrival', 'sequenceNo', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]].reset_index(drop=True)\n",
    "    cancelDataSZ['ApplSeqNum'] = np.where(cancelDataSZ['BidApplSeqNum'] == 0, cancelDataSZ['OfferApplSeqNum'], cancelDataSZ['BidApplSeqNum'])\n",
    "    cancelDataSZ['Side'] = np.where(cancelDataSZ['BidApplSeqNum'] == 0, 2, 1)\n",
    "    cancelDataSZ = cancelDataSZ[['SecurityID', 'ApplSeqNum', 'clockAtArrival', 'sequenceNo', 'Side', 'TradeQty', 'cum_volume', \"TransactTime\"]]\n",
    "    cancelDataSZ['updateType'] = 3\n",
    "\n",
    "    msgDataSZ = pd.concat([orderDataSZ, tradeDataSZ, cancelDataSZ], sort=False)\n",
    "    del orderDataSZ\n",
    "    del tradeDataSZ\n",
    "    del cancelDataSZ\n",
    "    msgDataSZ = msgDataSZ.sort_values(by=['SecurityID', 'ApplSeqNum', 'sequenceNo']).reset_index(drop=True)\n",
    "    msgDataSZ['TradePrice'] = np.where(msgDataSZ['updateType'] == 4, msgDataSZ['TradePrice'], 0)\n",
    "    msgDataSZ['TradePrice'] = msgDataSZ['TradePrice'].astype('int64')\n",
    "    msgDataSZ['TradeQty'] = np.where(msgDataSZ['updateType'] == 4, msgDataSZ['TradeQty'], 0)\n",
    "    msgDataSZ['TradeQty'] = msgDataSZ['TradeQty'].astype('int64')\n",
    "    msgDataSZ['secid'] = msgDataSZ['SecurityID'] + 2000000\n",
    "    assert(msgDataSZ['ApplSeqNum'].max() < 1e8)\n",
    "    msgDataSZ['StockSeqNum'] = msgDataSZ['SecurityID']*1e8 + msgDataSZ['ApplSeqNum']\n",
    "    msgDataSZ['date'] = int(thisDate) \n",
    "    print('finish market orders')\n",
    "\n",
    "\n",
    "    # 2. orderLog\n",
    "    infoData = orderLog[(orderLog['date'] == int(thisDate)) & (orderLog[\"isMsg\"] == 1) \n",
    "                        & (orderLog['updateType'].isin([0, 3, 4]))].reset_index(drop=True)\n",
    "    del orderLog\n",
    "    infoData['Price'] = infoData['orderPrice'].apply(lambda x: round(x*100, 0))\n",
    "    infoData['Price'] = infoData['Price'].astype('int64')*100\n",
    "    infoData['OrderQty'] = infoData['absOrderSize']\n",
    "    infoData['Side'] = np.where(infoData['orderDirection1'] == 1, 1, 2)\n",
    "    infoData['TradePrice'] = np.where(infoData['updateType'] == 4, round(infoData['tradePrice']*100, 0), 0)\n",
    "    infoData['TradePrice'] = infoData['TradePrice'].astype('int64')*100\n",
    "    statusInfo = infoData.groupby(['order'])['updateType'].apply(lambda x: tuple(x)).reset_index()\n",
    "    statusInfo.columns = ['order', 'statusLs']\n",
    "    tradePriceInfo = infoData.groupby(['order'])['TradePrice'].apply(lambda x: tuple(x)).reset_index()\n",
    "    tradePriceInfo.columns = ['order', 'TradePriceLs']\n",
    "    tradeQtyInfo = infoData.groupby(['order'])['absFilledThisUpdate'].apply(lambda x: tuple(x)).reset_index()\n",
    "    tradeQtyInfo.columns = ['order', 'TradeQtyLs']\n",
    "    infoData = infoData[infoData['updateType'] == 0]\n",
    "    infoData = pd.merge(infoData, statusInfo, how='left', on=['order'], validate='one_to_one')\n",
    "    infoData = pd.merge(infoData, tradePriceInfo, how='left', on=['order'], validate='one_to_one')\n",
    "    infoData = pd.merge(infoData, tradeQtyInfo, how='left', on=['order'], validate='one_to_one')\n",
    "    infoData['brokerNum'] = infoData.groupby(['date', 'secid', 'vai', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'])['colo_account'].transform('count')\n",
    "    display(infoData[infoData['brokerNum'] >= 2].groupby(['colo', 'accCode'])['date'].count())\n",
    "    display('%.2f%%'%(infoData[infoData['brokerNum'] >= 2].shape[0] / infoData.shape[0]*100))\n",
    "    display(infoData[infoData['brokerNum'] >= 2].shape[0])\n",
    "    display(infoData.shape[0])\n",
    "    infoData = infoData[infoData['brokerNum'] == 1]\n",
    "    infoData = infoData[['date', 'secid', 'vai', 'ars', \"mrstaat\", \"mrstauc\", 'exchange', 'group', 'Price', 'OrderQty', \n",
    "                         'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum', 'order', 'colo', 'accCode', \n",
    "                         'clockAtArrival', 'orderDirection', 'absOrderSize']]   \n",
    "\n",
    "    print('finish our orders')\n",
    "\n",
    "    # 3. find the position of our orders\n",
    "    checkLog = msgDataSZ[msgDataSZ['updateType'] == 0]\n",
    "    checkLog = pd.merge(checkLog, infoData.drop_duplicates(subset=['secid', 'ApplSeqNum'])[['secid', 'ApplSeqNum', 'ars']], \n",
    "                    on=['secid', 'ApplSeqNum'], how='left')\n",
    "    checkLog['ApplSeqNum'] = np.where(~checkLog['ars'].isnull(), checkLog['ApplSeqNum'], checkLog['ars'])\n",
    "    checkLog['start_time'] = np.where(~checkLog['ars'].isnull(), checkLog['clockAtArrival'], checkLog['ars'])\n",
    "    checkLog.drop([\"ars\"],axis=1,inplace=True)\n",
    "    checkLog['ApplSeqNum'] = checkLog.groupby(['secid'])['ApplSeqNum'].ffill()\n",
    "    checkLog['start_time'] = checkLog.groupby(['secid'])['start_time'].ffill()\n",
    "    checkLog['end_time'] = checkLog['start_time'] + 100*1e3\n",
    "    checkLog = checkLog[(~checkLog['ApplSeqNum'].isnull()) & (checkLog['clockAtArrival'] > checkLog['start_time']) & \n",
    "                       (checkLog['clockAtArrival'] < checkLog['end_time'])]\n",
    "    msgDataSZ = msgDataSZ[msgDataSZ['StockSeqNum'].isin(checkLog['StockSeqNum'].values)]\n",
    "    print('finish get the interval')\n",
    "\n",
    "    statusInfo = msgDataSZ.groupby(['StockSeqNum'])['updateType'].apply(lambda x: getTuple(x)).reset_index()\n",
    "    statusInfo.columns = ['StockSeqNum', 'statusLs']\n",
    "    tradePriceInfo = msgDataSZ.groupby(['StockSeqNum'])['TradePrice'].apply(lambda x: tuple(x)).reset_index()\n",
    "    tradePriceInfo.columns = ['StockSeqNum', 'TradePriceLs']\n",
    "    tradeQtyInfo = msgDataSZ.groupby(['StockSeqNum'])['TradeQty'].apply(lambda x: tuple(x)).reset_index()\n",
    "    tradeQtyInfo.columns = ['StockSeqNum', 'TradeQtyLs']\n",
    "    del msgDataSZ\n",
    "    checkLog = pd.merge(checkLog, statusInfo, how='left', on=['StockSeqNum'], validate='one_to_one')\n",
    "    checkLog = pd.merge(checkLog, tradePriceInfo, how='left', on=['StockSeqNum'], validate='one_to_one')\n",
    "    checkLog = pd.merge(checkLog, tradeQtyInfo, how='left', on=['StockSeqNum'], validate='one_to_one')  \n",
    "    del statusInfo\n",
    "    del tradePriceInfo\n",
    "    del tradeQtyInfo\n",
    "    checkLog = checkLog.rename(columns={'clockAtArrival':'caa_orderLog'})\n",
    "\n",
    "    try:\n",
    "        checkLog = pd.merge(checkLog, infoData, how='left', on=['date', 'secid', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'], validate='many_to_one')\n",
    "    except:\n",
    "        print('There are orders with same pattern but different vai, same ApplSeqNum')\n",
    "        display([infoData[infoData.duplicated(['date', 'secid', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'], keep=False)]])\n",
    "        checkLog = pd.merge(checkLog, infoData, how='left', on=['date', 'secid', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'])\n",
    "    del infoData\n",
    "    checkLog = checkLog.drop_duplicates(['date', 'secid', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'], keep=False)\n",
    "    checkLog = checkLog[~checkLog['accCode'].isnull()]\n",
    "    checkLog = checkLog.reset_index(drop=True)\n",
    "    if original_data[original_data.duplicated(['date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep=False)].shape[0] == 0:\n",
    "        original_data = pd.merge(original_data, checkLog[['caa_orderLog', 'start_time', 'date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs']], validate='one_to_one',\n",
    "                                on=['date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], how='left')\n",
    "    else:\n",
    "        original_data = pd.merge(original_data, checkLog[['caa_orderLog', 'start_time', 'date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs']], validate='many_to_one',\n",
    "                                on=['date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], how='left')    \n",
    "\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "\n",
    "\n",
    "    def DB(host, db_name, user, passwd):\n",
    "        auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "        url = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "        client = pymongo.MongoClient(url, maxPoolSize=None)\n",
    "        db = client[db_name]\n",
    "        return db\n",
    "\n",
    "    def read_memb_daily(db, name, start_date=None, end_date=None, skey=None, index_id=None, interval=None, col=None, return_sdi=True):\n",
    "        collection = db[name]\n",
    "        # Build projection\n",
    "        prj = {'_id': 0}\n",
    "        if col is not None:\n",
    "            if return_sdi:\n",
    "                col = ['skey', 'date', 'interval'] + col\n",
    "            for col_name in col:\n",
    "                prj[col_name] = 1\n",
    "\n",
    "        # Build query\n",
    "        query = {}\n",
    "        if skey is not None:\n",
    "            query['skey'] = {'$in': skey}\n",
    "        if index_id is not None:\n",
    "            query['index_id'] = {'$in': index_id}\n",
    "        if interval is not None:\n",
    "            query['interval'] = {'$in': interval}\n",
    "        if start_date is not None:\n",
    "            if end_date is not None:\n",
    "                query['date'] = {'$gte': start_date, '$lte': end_date}\n",
    "            else:\n",
    "                query['date'] = {'$gte': start_date}\n",
    "        elif end_date is not None:\n",
    "            query['date'] = {'$lte': end_date}\n",
    "\n",
    "        # Load data\n",
    "        cur = collection.find(query, prj)\n",
    "        df = pd.DataFrame.from_records(cur)\n",
    "        if df.empty:\n",
    "            df = pd.DataFrame()\n",
    "        else:\n",
    "            df = df.sort_values(by=['date', 'index_id', 'skey'])\n",
    "        return df    \n",
    "\n",
    "    def read_beta_daily(db, name, start_date=None, end_date=None, skey=None, interval=None, col=None, return_sdi=True): \n",
    "        collection = db[name] \n",
    "        # Build projection \n",
    "        prj = {'_id': 0} \n",
    "        if col is not None: \n",
    "            if return_sdi: \n",
    "                col = ['skey', 'date'] + col \n",
    "            for col_name in col: \n",
    "                prj[col_name] = 1 \n",
    "\n",
    "        # Build query \n",
    "        query = {} \n",
    "        if skey is not None: \n",
    "            query['skey'] = {'$in': skey} \n",
    "        if interval is not None: \n",
    "            query['interval'] = {'$in': interval} \n",
    "        if start_date is not None: \n",
    "            if end_date is not None: \n",
    "                query['date'] = {'$gte': start_date, '$lte': end_date} \n",
    "            else: \n",
    "                query['date'] = {'$gte': start_date} \n",
    "        elif end_date is not None: \n",
    "            query['date'] = {'$lte': end_date} \n",
    "\n",
    "        # Load data \n",
    "        cur = collection.find(query, prj) \n",
    "        df = pd.DataFrame.from_records(cur) \n",
    "        if df.empty: \n",
    "            df = pd.DataFrame() \n",
    "        else: \n",
    "            df = df.sort_values(by=['date','skey']) \n",
    "        return df  \n",
    "\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    pd.set_option('max_columns', 200)\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    pd.set_option('max_rows', 100)\n",
    "    pd.set_option('max_columns', 100)\n",
    "\n",
    "    perc = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "    print('start generate return')\n",
    "    startTm = datetime.datetime.now()\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    dataPathLs = np.array(glob.glob(os.path.join(readPath, 'speedCompare***.csv')))\n",
    "    dateLs = np.array([os.path.basename(i).split('_')[1].split('.')[0] for i in dataPathLs])\n",
    "    dateLs = dateLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "    date = dateLs[0]\n",
    "    assert(date == dd)\n",
    "\n",
    "    index = read_memb_daily(db1, 'index_memb', int(startDate), int(endDate), index_id=[1000300, 1000905, 1000852, 1000985])\n",
    "    index = index[['index_id', 'skey', 'date']].reset_index(drop=True)\n",
    "    index = index.rename(columns={'index_id':'indexCat', 'skey':'secid'})\n",
    "    index.loc[index['indexCat'] == 1000985, 'indexCat'] = 1000852\n",
    "    index['ID'] = index['secid']\n",
    "\n",
    "    d = read_beta_daily(db1, 'mktbeta', int(startDate), int(endDate))\n",
    "    d1 = d[['skey', 'beta_60d_IF', 'date']]\n",
    "    d1 = d1.rename(columns={'beta_60d_IF':\"beta_60\"})\n",
    "    d1['indexCat'] = 1000300\n",
    "    d2 = d[['skey', 'beta_60d_IC', 'date']]\n",
    "    d2 = d2.rename(columns={'beta_60d_IC':\"beta_60\"})\n",
    "    d2['indexCat'] = 1000905\n",
    "    d3 = d[['skey', 'beta_60d_CSI1000', 'date']]\n",
    "    d3 = d3.rename(columns={'beta_60d_CSI1000':\"beta_60\"})\n",
    "    d3['indexCat'] = 1000852\n",
    "    betaData = pd.concat([d1, d2, d3]).reset_index(drop=True)\n",
    "    betaData['date'] = betaData['date'].apply(lambda x: datetime.datetime.strptime(str(x), '%Y%m%d').date())\n",
    "    betaData = betaData.rename(columns={'skey':'secid'})\n",
    "\n",
    "\n",
    "    dateDate = datetime.datetime.strptime(date, '%Y%m%d').date()\n",
    "    dateBetaData = betaData[betaData['date'] == dateDate]\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    orderLog = pd.read_csv(os.path.join(readPath, 'speedCompare_%s.csv'%date))\n",
    "    for col in ['clockAtArrival', 'caamd', 'secid', 'updateType', 'vai', 'absFilledThisUpdate', 'orderDirection', 'absOrderSize',\n",
    "                'absOrderSizeCumFilled', 'date', 'accCode', 'mse']:\n",
    "        orderLog[col] = orderLog[col].fillna(0)\n",
    "        orderLog[col] = orderLog[col].astype('int64') \n",
    "#     orderLog = orderLog[~orderLog['vai'].isnull()]\n",
    "    orderLog = orderLog.rename(columns={'mdClockAtArrival': 'caamd'})\n",
    "    \n",
    "    display('There are accounts with duplicated ticks:')\n",
    "    display(orderLog[orderLog.duplicated(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep=False)]\\\n",
    "    .groupby(['date', 'colo', 'accCode'])['ars'].size())\n",
    "    orderLog = orderLog.drop_duplicates(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep='first')\n",
    "    \n",
    "    display('There are ticks with orderDirection 0')\n",
    "    display(orderLog[orderLog['orderDirection'] == 0][['date', 'colo', 'accCode', \\\n",
    "                'secid', 'vai', 'updateType', 'sdd', 'orderDirection', 'absOrderSize', 'internalId', 'orderId']])\n",
    "\n",
    "    assert(orderLog[orderLog['updateType'] == 0][orderLog[orderLog['updateType'] == 0]\\\n",
    "                                                       .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                    'vai', 'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "    try:\n",
    "        assert(orderLog[(orderLog['updateType'] == 0) & (orderLog['accCode'] != 8856)][orderLog[(orderLog['updateType'] == 0) & (orderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "    except:\n",
    "        print('There are orders with all things same except sdd')\n",
    "        print(orderLog[(orderLog['updateType'] == 0) & (orderLog['accCode'] != 8856)][orderLog[(orderLog['updateType'] == 0) & (orderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId'], keep=False)])\n",
    "        assert(orderLog[(orderLog['updateType'] == 0) & (orderLog['accCode'] != 8856)][orderLog[(orderLog['updateType'] == 0) & (orderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId', 'sdd'], keep=False)].shape[0] == 0)\n",
    "    try:\n",
    "        assert(sum(orderLog[(orderLog['updateType'] != 0) & (orderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() != 1) == 0) \n",
    "    except:\n",
    "        print('There are orders with same internalId but different orderId other than accCode 8856 case')\n",
    "        print(orderLog[(orderLog['updateType'] != 0) & (orderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique()[orderLog[(orderLog['updateType'] != 0) & (orderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() > 1])\n",
    "\n",
    "    r2 = orderLog[(orderLog['accCode'] != 8856) & (orderLog['orderDirection'] != 0)]\n",
    "    r1 = orderLog[(orderLog['accCode'] == 8856) & (orderLog['orderDirection'] != 0)]\n",
    "    r1['test'] = r1.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize']).grouper.group_info[0]\n",
    "    r1 = r1.sort_values(by=['test', 'clockAtArrival'])\n",
    "    r1.loc[r1['updateType'] != 0, 'vai'] = np.nan\n",
    "    r1['vai'] = r1.groupby('test')['vai'].ffill()\n",
    "    r2['test'] = r2.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize', 'internalId']).grouper.group_info[0]\n",
    "    r2 = r2.sort_values(by=['test', 'clockAtArrival'])\n",
    "    r2.loc[r2['updateType'] != 0, 'vai'] = np.nan\n",
    "    r2['vai'] = r2.groupby('test')['vai'].ffill()\n",
    "    try:\n",
    "        assert(sum(r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "    except:\n",
    "        print('There are orders in 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        a = r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique()[r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1].reset_index()\n",
    "        print(pd.merge(r1, a[['test', 'vai']], on=['test', 'vai'], how='inner')[['secid', 'accCode', 'colo', 'vai', 'updateType', 'sdd', 'internalId', 'orderId', 'absOrderSize', 'absFilledThisUpdate', 'absOrderSizeCumFilled', 'orderPrice', 'tradePrice']])\n",
    "    try:\n",
    "        assert(sum(r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "    except:\n",
    "        a = r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique()[r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1].reset_index()\n",
    "        print('There are orders out of 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print(pd.merge(r2, a[['test', 'vai']], on=['test', 'vai'], how='inner')[['secid', 'accCode', 'colo', 'vai', 'updateType', 'sdd', 'internalId', 'orderId', 'absOrderSize', 'absFilledThisUpdate', 'absOrderSizeCumFilled', 'orderPrice', 'tradePrice']])\n",
    "    orderLog = pd.concat([r1, r2])\n",
    "    del r1\n",
    "    del r2  \n",
    "    \n",
    "    orderLog = orderLog.sort_values(by=['date', 'colo', 'accCode', 'secid', 'vai', 'clockAtArrival']).reset_index(drop=True)\n",
    "    orderLog['order'] = orderLog.groupby(['date', 'colo', 'accCode', 'secid', 'vai', 'orderDirection', 'absOrderSize', 'internalId']).grouper.group_info[0]\n",
    "    orderLog['firstUpdateType'] = orderLog.groupby(['order'])['updateType'].transform('first')\n",
    "    orderLog = orderLog[orderLog['firstUpdateType'] == 0]\n",
    "\n",
    "    orderLog['insertNum'] = np.where(orderLog['updateType'] == 0, 1, 0)\n",
    "    orderLog['insertNum'] = orderLog.groupby(['order'])['insertNum'].transform('sum')\n",
    "    orderLog = orderLog[orderLog['insertNum'] == 1]\n",
    "\n",
    "    orderLog['innerSeq'] = orderLog.index.values\n",
    "    targetStockLs = orderLog['secid'].unique()\n",
    "    orderLog['firstUpdateType'] = orderLog.groupby(['order'])['updateType'].transform('first')\n",
    "    orderLog['firstClock'] = orderLog.groupby(['order'])['clockAtArrival'].transform('first')\n",
    "    orderLog['caamd'] = orderLog.groupby(['order'])['caamd'].transform('first')\n",
    "\n",
    "    assert(orderLog[orderLog['firstUpdateType'] != 0].shape[0] == 0)\n",
    "    indexCatData = index[index['date'] == int(date)]\n",
    "    orderLog = pd.merge(orderLog, indexCatData[['secid', 'indexCat']], how='left', on=['secid'], validate='many_to_one')\n",
    "    orderLog = pd.merge(orderLog, dateBetaData[['secid', 'indexCat', 'beta_60']], how='left', on=['secid', 'indexCat'], validate='many_to_one')\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.34\\\\random_backup\\\\Kevin_zhenyu\\\\rawData\\\\logs_%s_***'%date\n",
    "    mdDataSHPath = glob.glob(os.path.join(readPath, 'mdLog_SH***.csv'))[-1]\n",
    "    mdDataSH = pd.read_csv(mdDataSHPath)\n",
    "    mdDataSH['ID'] = mdDataSH['StockID'] + 1000000\n",
    "    mdDataSH['time'] = mdDataSH.time.str.slice(0, 2) + mdDataSH.time.str.slice(3, 5) + mdDataSH.time.str.slice(6, 8) + '000'\n",
    "    mdDataSH['time'] = mdDataSH['time'].astype('int64')\n",
    "    mdDataSH['time'] = mdDataSH.groupby(['ID'])['time'].cummax()\n",
    "    mdDataSH['max_cum_volume'] = mdDataSH.groupby(['StockID'])['cum_volume'].cummax()\n",
    "    indexData = mdDataSH[mdDataSH['StockID'].isin([300, 852, 905])][['ID', 'sequenceNo', 'close']].reset_index(drop=True)\n",
    "    mdDataSH = mdDataSH[mdDataSH['StockID'] >= 600000]\n",
    "    mdDataSH = mdDataSH[(mdDataSH['cum_volume'] > 0) & (mdDataSH['time'] >= 93000000) &\\\n",
    "                        (mdDataSH['cum_volume'] == mdDataSH['max_cum_volume'])]\n",
    "    mdDataSH = mdDataSH[['ID', 'clockAtArrival', 'sequenceNo', 'time', 'cum_volume', 'bid1p', 'ask1p', 'bid1q', 'ask1q', 'bid5q', 'ask5q']]\n",
    "\n",
    "    mdDataSZPath = glob.glob(os.path.join(readPath, 'mdLog_SZ***.csv'))[-1]\n",
    "    mdDataSZ = pd.read_csv(mdDataSZPath)\n",
    "    mdDataSZ['ID'] = mdDataSZ['StockID'] + 2000000\n",
    "    mdDataSZ['time'] = mdDataSZ.time.str.slice(0, 2) + mdDataSZ.time.str.slice(3, 5) + mdDataSZ.time.str.slice(6, 8) + '000'\n",
    "    mdDataSZ['time'] = mdDataSZ['time'].astype('int64')\n",
    "    mdDataSZ['time'] = mdDataSZ.groupby(['ID'])['time'].cummax()\n",
    "    mdDataSZ['max_cum_volume'] = mdDataSZ.groupby(['StockID'])['cum_volume'].cummax()\n",
    "    mdDataSZ = mdDataSZ[(mdDataSZ['cum_volume'] > 0) & (mdDataSZ['time'] >= 93000000) &\\\n",
    "                        (mdDataSZ['cum_volume'] == mdDataSZ['max_cum_volume'])]\n",
    "    mdDataSZ = mdDataSZ[['ID', 'clockAtArrival', 'sequenceNo', 'time', 'cum_volume', 'bid1p', 'ask1p', 'bid1q', 'ask1q', 'bid5q', 'ask5q']]\n",
    "\n",
    "    mdData = pd.concat([mdDataSH, mdDataSZ, indexData]).reset_index(drop=True)\n",
    "    mdData = mdData.sort_values(by=['sequenceNo']).reset_index(drop=True)\n",
    "\n",
    "    addData = pd.DataFrame({'indexCat': [1000300, 1000905, 1000852], 'ID': [1000300, 1000905, 1000852], \n",
    "                            'secid':[1000300, 1000905, 1000852]})\n",
    "    indexCatData = pd.concat([indexCatData, addData], sort=False).reset_index(drop=True)\n",
    "    mdData = pd.merge(mdData, indexCatData, how='left', on=['ID'], validate='many_to_one')\n",
    "    mdData = mdData[~mdData['indexCat'].isnull()].reset_index(drop=True)\n",
    "    mdData['indexClose'] = np.where(mdData['ID'].isin([1000300, 1000852, 1000905]), mdData['close'], np.nan)\n",
    "    mdData['indexClose'] = mdData.groupby(['indexCat'])['indexClose'].ffill()\n",
    "    mdData = mdData[~mdData['ID'].isin([1000300, 1000852, 1000905])].reset_index(drop=True)\n",
    "\n",
    "    mdData = mdData.sort_values(by=['ID', 'sequenceNo']).reset_index(drop=True)\n",
    "    mdData['safeBid1p'] = np.where(mdData['bid1p'] == 0, mdData['ask1p'], mdData['bid1p'])\n",
    "    mdData['safeAsk1p'] = np.where(mdData['ask1p'] == 0, mdData['bid1p'], mdData['ask1p'])\n",
    "    mdData['adjMid'] = (mdData['safeBid1p']*mdData['ask1q'] + mdData['safeAsk1p']*mdData['bid1q'])/(mdData['bid1q'] + mdData['ask1q'])\n",
    "\n",
    "    mdData['session'] = np.where(mdData['time'] >= 130000000, 1, 0)\n",
    "    def findTmValue(clockLs, tm, method='L', buffer=0):\n",
    "        maxIx = len(clockLs)\n",
    "        orignIx = np.arange(maxIx)\n",
    "        if method == 'F':\n",
    "            ix = np.searchsorted(clockLs, clockLs+(tm-buffer))\n",
    "            ## if target future index is next tick, mask\n",
    "            mask = (orignIx == (ix - 1))|(orignIx == ix)|(ix == maxIx)\n",
    "        elif method == 'L':\n",
    "            ## if target future index is last tick, mask\n",
    "            ix = np.searchsorted(clockLs, clockLs-(tm-buffer))\n",
    "            ix = ix - 1\n",
    "            ix[ix<0] = 0\n",
    "            ## !!!ATTENTION: model3 change\n",
    "            mask = (orignIx == ix) | ((clockLs-(tm-buffer)).values < clockLs.values[0])\n",
    "        ix[mask] = -1\n",
    "        return ix\n",
    "\n",
    "    mdData = mdData.reset_index(drop=True)\n",
    "    groupAllData = mdData.groupby(['ID', 'session'])\n",
    "    mdData['sessionStartCLA'] = groupAllData['clockAtArrival'].transform('min')\n",
    "    mdData['relativeClock'] = mdData['clockAtArrival'] - mdData['sessionStartCLA']\n",
    "    mdData['trainFlag'] = np.where(mdData['relativeClock'] > 179.5*1e6, 1, 0)\n",
    "    mdData['index'] = mdData.index.values\n",
    "    mdData['sessionStartIx'] = groupAllData['index'].transform('min')\n",
    "    for tm in [30, 90, 300]:\n",
    "        tmCol = 'F{}s_ix'.format(tm)\n",
    "        mdData[tmCol] = groupAllData['relativeClock'].transform(lambda x: findTmValue(x, tm*1e6, 'F', 5*1e5)).astype(int)\n",
    "    nearLimit = ((mdData.ask5q.values == 0) | (mdData.bid5q.values == 0))\n",
    "\n",
    "    for tm in [30, 90, 300]:\n",
    "        tmIx = mdData['F{}s_ix'.format(tm)].values + mdData['sessionStartIx'].values\n",
    "        adjMid_tm = mdData['adjMid'].values[tmIx]\n",
    "        adjMid_tm[mdData['F{}s_ix'.format(tm)].values == -1] = np.nan\n",
    "        mdData['adjMid_F{}s'.format(tm)] = adjMid_tm\n",
    "\n",
    "    for tm in [30, 90, 300]:\n",
    "        tmIx = mdData['F{}s_ix'.format(tm)].values + mdData['sessionStartIx'].values\n",
    "        adjMid_tm = mdData['indexClose'].values[tmIx]\n",
    "        adjMid_tm[mdData['F{}s_ix'.format(tm)].values == -1] = np.nan\n",
    "        mdData['indexClose_F{}s'.format(tm)] = adjMid_tm\n",
    "\n",
    "    mdData = mdData[mdData['ID'].isin(targetStockLs)]\n",
    "    mdStartPos = mdData.drop_duplicates(subset=['ID', 'cum_volume'], keep='last')\n",
    "    mdStartPos = mdStartPos[['ID', 'cum_volume', 'clockAtArrival']].reset_index(drop=True)\n",
    "    mdStartPos.columns = ['secid', 'vai', 'mdStartClock']\n",
    "    mdStartPos['isOrder'] = 0\n",
    "    tradeStartPos = orderLog[orderLog['updateType'] == 0][['secid', 'vai', 'order']].reset_index(drop=True)\n",
    "    tradeStartPos['isOrder'] = 1\n",
    "    tradeStartPos = pd.concat([mdStartPos, tradeStartPos], sort=False)\n",
    "    tradeStartPos = tradeStartPos.sort_values(by=['secid', 'vai', 'isOrder'])\n",
    "    tradeStartPos['mdStartClock'] = tradeStartPos.groupby(['secid'])['mdStartClock'].ffill()\n",
    "    tradeStartPos['mdStartClock'] = tradeStartPos.groupby(['secid'])['mdStartClock'].backfill()\n",
    "    tradeStartPos = tradeStartPos[tradeStartPos['isOrder'] == 1][['secid', 'vai', 'order', 'mdStartClock']]\n",
    "\n",
    "    orderLog = pd.merge(orderLog, tradeStartPos[['order', 'mdStartClock']], how='left', on=['order'], validate='many_to_one')\n",
    "    orderLog['mdClockAtArrival'] = orderLog['clockAtArrival'] - orderLog['caamd'] + orderLog['mdStartClock']\n",
    "\n",
    "    tradeData = orderLog[['secid', 'mdClockAtArrival', 'innerSeq']].reset_index(drop=True)\n",
    "    tradeData.columns = ['ID', 'clockAtArrival', 'innerSeq']\n",
    "    tradeData['isOrder'] = 1\n",
    "\n",
    "    mdData = pd.concat([mdData, tradeData], sort=False)\n",
    "    mdData = mdData.sort_values(by=['ID', 'clockAtArrival', 'isOrder', 'innerSeq']).reset_index(drop=True)\n",
    "    for col in ['indexClose', 'adjMid_F30s', 'indexClose_F30s', 'adjMid_F90s', 'indexClose_F90s',\n",
    "                'adjMid_F300s', 'indexClose_F300s']:\n",
    "        mdData[col] = mdData.groupby(['ID'])[col].backfill()\n",
    "        mdData[col] = mdData.groupby(['ID'])[col].ffill()\n",
    "\n",
    "    tradeData = mdData[mdData['isOrder'] == 1][['ID', 'innerSeq', 'adjMid_F30s', 'adjMid_F90s', 'adjMid_F300s',\n",
    "                                                'indexClose', 'indexClose_F30s', 'indexClose_F90s', 'indexClose_F300s']].reset_index(drop=True)\n",
    "    tradeData = tradeData.rename(columns={'ID': 'secid'})\n",
    "    orderLog = pd.merge(orderLog, tradeData, how='left', on=['secid', 'innerSeq'], validate='one_to_one')\n",
    "\n",
    "    if original_data[original_data.duplicated(['date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep=False)].shape[0] == 0:\n",
    "        original_data = pd.merge(original_data, orderLog[['beta_60', 'adjMid_F30s', 'adjMid_F90s', 'adjMid_F300s', 'indexClose', \\\n",
    "        'indexClose_F30s', 'indexClose_F90s', 'indexClose_F300s', 'date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "        'orderDirection', 'absOrderSize']], validate='one_to_one', on=['date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], how='left')\n",
    "    else:\n",
    "        original_data = pd.merge(original_data, orderLog[['beta_60', 'adjMid_F30s', 'adjMid_F90s', 'adjMid_F300s', 'indexClose', \\\n",
    "        'indexClose_F30s', 'indexClose_F90s', 'indexClose_F300s', 'date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "        'orderDirection', 'absOrderSize']], validate='many_to_one', on=['date', 'secid', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], how='left')  \n",
    "    del orderLog\n",
    "    original_data.to_pickle('L:\\\\orderLog\\\\data\\\\' + startDate + '.pkl')\n",
    "    del original_data\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     orderLog = rawOrderLog.copy()\n",
    "#     del rawOrderLog\n",
    "#     startTm = datetime.datetime.now()\n",
    "#     print('start generate speed compared with market orders')\n",
    "    \n",
    "#     ### Assertion 1:  make sure same direction in same date, secid, vai\n",
    "#     print('=======================================================================================')\n",
    "#     print('1. same date, secid, vai: same direction')\n",
    "#     taking = orderLog[orderLog['ars'].isin([1, 7])]\n",
    "#     making = orderLog[orderLog['ars'].isin([2, 3])]\n",
    "#     else_orders = orderLog[~(orderLog['ars'].isin([1, 2, 3, 7]))]\n",
    "#     display('orders with abnormal ars values')\n",
    "#     display(else_orders[else_orders['updateType'] == 0].groupby('ars')['date'].size().sort_values(ascending=False))\n",
    "#     display(else_orders[else_orders['updateType'] == 0].groupby('accCode')['date'].size().sort_values(ascending=False))\n",
    "#     taking['directNum'] = taking.groupby(['date', 'secid', 'vai', 'sdd'])['orderDirection1'].transform('nunique')\n",
    "#     if len(taking[taking['directNum'] != 1]) > 0:\n",
    "#         print('opposite direction for same date, same secid, same vai')\n",
    "#         display(taking[(taking['directNum'] != 1) & (taking['updateType'] == 0)].groupby(['accCode'])['orderDirection'].size())\n",
    "#         taking = taking[taking['directNum'] == 1]\n",
    "\n",
    "#     assert((taking.groupby(['date', 'secid', 'vai', 'sdd'])['orderDirection1'].nunique() == 1).all() == True)\n",
    "#     orderLog = pd.concat([taking, making]).sort_values(by=['date', 'colo', 'accCode', 'secid', 'vai', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "#     ## Assertion 2:  make sure each account, secid, vai only has one insertion\n",
    "#     print('=======================================================================================')\n",
    "#     print('2. same date, secid, vai, accCode: one insertion')\n",
    "#     a = orderLog[orderLog['updateType'] == 0].groupby(['date', 'accCode', 'secid', 'vai', 'sdd'])['clockAtArrival'].count()\n",
    "#     if len(a[a > 1]) > 0:\n",
    "#         print('more than one insertion at same time')\n",
    "#         a = a[a>1].reset_index()\n",
    "#         display(a)\n",
    "#         orderLog = orderLog[~(orderLog['order'].isin(a['order'].unique()))]\n",
    "\n",
    "#     orderLog['isMsg'] = np.where(orderLog['updateType'] == 0, \n",
    "#                                  np.where(orderLog['mse'] == 100, 1, 0), np.nan)\n",
    "#     orderLog['isMsg'] = orderLog.groupby(['order'])['isMsg'].ffill()\n",
    "\n",
    "#     placeSZE = orderLog[(orderLog['secid'] >= 2000000) & (orderLog['updateType'] == 0)]\n",
    "#     print('%.2f%% SZE orders triggered by msg data'%(placeSZE[placeSZE['isMsg'] == 1].shape[0]/placeSZE.shape[0]*100))\n",
    "\n",
    "\n",
    "#     ### Assertion 3:  check IPO stocks selling status\n",
    "#     print('=======================================================================================')\n",
    "#     print('3. IPO stocks selling (ars = 301, 302)')\n",
    "#     if orderLog[orderLog['ars'].isin([301, 302])].shape[0] != 0:\n",
    "#         kk = orderLog[orderLog['ars'].isin([301, 302])]\n",
    "#         print(kk)\n",
    "#         try:\n",
    "#             assert(kk[kk['orderDirection1'] == 1].shape[0] == 0)\n",
    "#             print('we only sell, never buy')\n",
    "#         except:\n",
    "#             print('There are IPO buy side orders!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "#             print(kk[kk['orderDirection1'] == 1])\n",
    "#         kk1 = kk[kk['updateType'] == 0]\n",
    "#         kk1 = kk1.sort_values(by=['accCode', 'secid','clockAtArrival'])\n",
    "#         kk1['diff'] = kk1.groupby(['accCode', 'secid'])['clockAtArrival'].apply(lambda x: x-x.shift(1))\n",
    "#         kk1['diff'] = kk1['diff'].fillna(0)\n",
    "#         try:\n",
    "#             assert(kk1[kk1['diff'] < 10e6].shape[0] == 0)\n",
    "#             print('for each stock in the same account, there is no insertion within 10 seconds of the previous insertion')\n",
    "#         except:\n",
    "#             print('There are insertion within 10 seconds for orders under same account same stock!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "#             print(kk1[kk1['diff'] < 10e6])\n",
    "#         kk2 = kk[(kk['updateType'] == 1)]\n",
    "#         try:\n",
    "#             assert(kk2[kk2['duration'] < 3e6].shape[0] == 0)\n",
    "#             print('for each stock in the same account, the cancellation of an order happens more than 3 seconds after the insertion')\n",
    "#         except:\n",
    "#             print('There are cancellation within 3 seconds for orders under same account same stock!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "#             print(kk2[kk2['duration'] < 3e6])\n",
    "\n",
    "\n",
    "#     ### Assertion 4: check updateType == 7 orders, make sure updateType == 7 orders < 20 per account, < 100 in total\n",
    "#     print('=======================================================================================')\n",
    "#     print('4. updateType 7 orders')\n",
    "#     if orderLog[orderLog['updateType'] == 7].shape[0] != 0:\n",
    "#         assert(orderLog[orderLog['updateType'] == 7].groupby('accCode')['order'].nunique().max() < 20)\n",
    "#         assert(orderLog[orderLog['updateType'] == 7].groupby('accCode')['order'].nunique().sum() < 100)\n",
    "\n",
    "#     ### Assertion 5: check updateType == 6 orders, make sure updateType == 6 orders < 5% per account\n",
    "#     print('=======================================================================================')\n",
    "#     print('5. updateType 6 orders')\n",
    "#     k1 = orderLog[orderLog['updateType'] == 6].groupby('accCode')['order'].nunique().reset_index()\n",
    "#     k2 = orderLog.groupby('accCode')['order'].nunique().reset_index()\n",
    "#     k = pd.merge(k1, k2, on='accCode', how='left')\n",
    "#     k['prob'] = k['order_x']/k['order_y']\n",
    "#     try:\n",
    "#         assert(sum(k['prob'] >= 0.05) == 0)\n",
    "#     except:\n",
    "#         print('There are accounts with more than 5% updateType 6 orders!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "#         print(k[k['prob'] >= 0.05])\n",
    "\n",
    "#     ### Assertion 6: check CYB orders, make sure CYB stocks total absOrderSize < 30w\n",
    "#     print('=======================================================================================')\n",
    "#     print('6. CYB stocks total order size < 30w')\n",
    "#     try:\n",
    "#         assert(orderLog[(orderLog['secid'] >= 2300000) & (orderLog['updateType'] == 0)]['absOrderSize'].max() <= 300000)\n",
    "#     except:\n",
    "#         print('CYB stocks total absOrderSize >= 30w!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "\n",
    "\n",
    "#     ### Assertion 7:  make sure there is no unexpected updateType \n",
    "#     print('=======================================================================================')\n",
    "#     print('7. unexpected updateType')\n",
    "#     def getTuple(x):\n",
    "#         return tuple(i for i in x)\n",
    "\n",
    "#     checkLog = orderLog[~((orderLog['updateType'] == 4) & (orderLog.groupby(['order'])['updateType'].shift(-1) == 4))]\n",
    "#     checkLog = checkLog.groupby(['order'])['updateType'].apply(lambda x: getTuple(x)).reset_index()\n",
    "#     checkLog['status'] = np.where(checkLog['updateType'].isin([(0, 2, 4), (0, 2, 1, 4), (0, 2, 1, 2, 4), (0, 2, 4, 1, 4), (0, 4), (0, 1, 4), (0, 4, 1, 4), (0, 2, 2, 4), (0, 4, 2, 4), (0, 2, 2, 1, 4), (0, 2, 2, 4, 1, 4)]),0,\n",
    "#                          np.where(checkLog['updateType'].isin([(0, 2, 4, 1, 3), (0, 2, 4, 1, 4, 3), (0, 2, 1, 4, 3), (0, 4, 1, 3), (0, 1, 4, 3),\n",
    "#                                                                    (0, 2, 2, 4, 1, 3), (0, 2, 2, 4, 1, 4, 3), (0, 2, 2, 1, 4, 3), (0, 4, 2, 4, 1, 3),\n",
    "#                                                                    (0, 4, 2, 1, 3), (0, 4, 1, 4, 3), (0, 4, 1)]), 1,\n",
    "#                          np.where(checkLog['updateType'].isin([(0, 2, 1, 3), (0, 2, 2, 1, 3), (0, 2, 3), (0, 3), (0, 1, 3), (0, ), (0, 2), (0, 2, 1), (0, 2, 2)]), 2, 3)))\n",
    "#     display(checkLog[checkLog['status'] == 3].groupby('updateType')['order'].size())\n",
    "#     orderLog = pd.merge(orderLog, checkLog[['order', 'status']], how='left', on=['order'], validate='many_to_one')\n",
    "#     orderLog = orderLog[orderLog['status'].isin([0, 1, 2])].reset_index(drop=True)\n",
    "\n",
    "#     ### Assertion 8:  make sure status==0 got all traded\n",
    "#     print('=======================================================================================')\n",
    "#     print('8. status == 0: all traded')\n",
    "#     a = orderLog[orderLog['status'] == 0]\n",
    "#     a = a.groupby(['order'])[['absOrderSizeCumFilled', 'absOrderSize']].max().reset_index()\n",
    "#     a.columns = ['order', 'filled', 'total']\n",
    "#     print('in total trade, any fill != total cases')\n",
    "#     display(a[a['filled'] != a['total']])\n",
    "#     if a[a['filled'] != a['total']].shape[0] > 0:\n",
    "#         removeOrderLs = a[a['filled'] != a['total']]['order'].unique()\n",
    "#         orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "#     ### Assertion 9:  make sure status==1 got partial traded\n",
    "#     print('=======================================================================================')\n",
    "#     print('9. status == 1: partial traded')\n",
    "#     a = orderLog[orderLog['status'] == 1]\n",
    "#     a = a.groupby(['order'])[['absOrderSizeCumFilled', 'absOrderSize']].max().reset_index()\n",
    "#     a.columns = ['order', 'filled', 'total']\n",
    "#     print('in partial trade, any fill >= total or fill is 0 cases for updateType 4')\n",
    "#     display(a[(a['filled'] >= a['total']) | (a['filled'] == 0)])\n",
    "#     if a[(a['filled'] >= a['total']) | (a['filled'] == 0)].shape[0] > 0:\n",
    "#         removeOrderLs = a[(a['filled'] >= a['total']) | (a['filled'] == 0)]['order'].unique()\n",
    "#         orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "#     ### Assertion 10: make sure no cancellation within 1 sec\n",
    "#     print('=======================================================================================')\n",
    "#     print('10. no cancellation within 1 sec')\n",
    "#     a = orderLog[(orderLog['updateType'] == 1) & (orderLog['duration'] < 1e6)]\n",
    "#     print('any cancellation within 1 sec')\n",
    "#     display(a)\n",
    "#     if a.shape[0] > 0:\n",
    "#         removeOrderLs = a['order'].unique()\n",
    "#         orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "\n",
    "#     ### Assertion 11: make sure no order has shares > 80w or notional > 800w\n",
    "#     print('=======================================================================================')\n",
    "#     print('11. Orders with size > 80w or notional > 800w')\n",
    "#     orderLog['orderNtl'] = orderLog['absOrderSize'] * orderLog['orderPrice']\n",
    "#     if orderLog[orderLog['absOrderSize'] > 800000].shape[0] > 0:\n",
    "#         print('some order quantity are > 80w')\n",
    "#         print(orderLog[orderLog['absOrderSize'] > 800000].groupby(['colo', 'accCode'])['order'].nunique())\n",
    "#         display(orderLog[orderLog['absOrderSize'] > 800000][['date', 'accCode', 'secid', 'vai', 'absOrderSize', 'orderPrice',\n",
    "#                                                              'orderNtl', 'orderDirection', 'clock', 'order']])\n",
    "\n",
    "#     if orderLog[orderLog['orderNtl'] > 8000000].shape[0] > 0:\n",
    "#         print('some order ntl are > 800w')\n",
    "#         print(orderLog[orderLog['orderNtl'] > 8000000].groupby(['colo', 'accCode'])['order'].nunique())\n",
    "#         display(orderLog[orderLog['orderNtl'] > 8000000][['date', 'accCode', 'secid', 'vai', 'absOrderSize', 'orderPrice',\n",
    "#                                                           'orderNtl', 'orderDirection', 'clock', 'order', \"updateType\", \n",
    "#                                                           \"tradePrice\", \"absOrderSizeCumFilled\", \"absFilledThisUpdate\"]])\n",
    "\n",
    "#     removeOrderLs = list(set(orderLog[orderLog['absOrderSize'] > 800000]['order'].unique()) | set(orderLog[orderLog['orderNtl'] > 8000000]['order'].unique()))\n",
    "#     orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "#     orderLog = orderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'clockAtArrival']).reset_index(drop=True)\n",
    "#     orderLog[\"mrsb90\"] = orderLog.groupby(['order'])['mrsb90'].transform('first')\n",
    "#     orderLog[\"mrss90\"] = orderLog.groupby(['order'])['mrss90'].transform('first')\n",
    "#     orderLog[\"mrstauc\"] = orderLog.groupby(['order'])['mrstauc'].transform('first')\n",
    "#     orderLog[\"mrstaat\"] = orderLog.groupby(['order'])['mrstaat'].transform('first')\n",
    "#     orderLog[\"aaa\"] = orderLog.groupby(['order'])['aaa'].transform('first')\n",
    "\n",
    "#     orderLog['m1'] = orderLog['mrstaat'].apply(lambda x: x - (x // 10000) * 10000)\n",
    "#     orderLog['m2'] = orderLog['mrstauc'].apply(lambda x: x - (x // 10000) * 10000)\n",
    "# #     if orderLog[orderLog['mrsb90'] == '-'].shape[0] != 0:\n",
    "# #         display(orderLog[orderLog['mrsb90'] == '-'])\n",
    "# #     orderLog = orderLog[orderLog['mrsb90'] != '-']\n",
    "#     orderLog['mrsb90'] = orderLog['mrsb90'].astype(float)\n",
    "# #     if orderLog[orderLog['aaa'] == '-'].shape[0] != 0:\n",
    "# #         display(orderLog[orderLog['aaa'] == '-'])\n",
    "# #     orderLog = orderLog[orderLog['aaa'] != '-']\n",
    "#     orderLog['aaa'] = orderLog['aaa'].astype(float)\n",
    "#     orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "#              (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'mrstauc'] = \\\n",
    "#     orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "#              (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'm2']\n",
    "\n",
    "#     orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "#              (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'mrstaat'] = \\\n",
    "#     orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "#              (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'm1']\n",
    "\n",
    "#     orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "#              (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'mrstauc'] = \\\n",
    "#     orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "#              (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'm2']\n",
    "\n",
    "#     orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "#              (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'mrstaat'] = \\\n",
    "#     orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "#              (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'm1']  \n",
    "\n",
    "#     orderLog = orderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#     orderLog['exchange'] = np.where(orderLog['secid'] >= 2000000, 'SZE', 'SSE')\n",
    "#     orderLog['orderNtl'] = orderLog['orderPrice'] * orderLog['absOrderSize']\n",
    "#     orderLog['tradeNtl'] = np.where(orderLog['updateType'] == 4, orderLog['tradePrice']*orderLog['absFilledThisUpdate'], 0)\n",
    "#     orderLog = orderLog[orderLog['secid'] >= 2000000].reset_index(drop=True)\n",
    "#     orderDataSZ = rawMsgDataSZ[rawMsgDataSZ['ExecType'] == '2'][['SecurityID', 'ApplSeqNum', 'clockAtArrival', 'sequenceNo', 'Side', 'OrderQty', 'Price', 'cum_volume', \"TransactTime\"]].reset_index(drop=True)\n",
    "#     orderDataSZ['updateType'] = 0\n",
    "#     tradeDataSZ = pd.concat([rawMsgDataSZ[rawMsgDataSZ['ExecType'] == 'F'][['SecurityID', 'BidApplSeqNum', 'clockAtArrival', 'sequenceNo', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]],\n",
    "#                              rawMsgDataSZ[rawMsgDataSZ['ExecType'] == 'F'][['SecurityID', 'OfferApplSeqNum', 'clockAtArrival', 'sequenceNo', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]]], sort=False)\n",
    "#     tradeDataSZ['ApplSeqNum'] = np.where(tradeDataSZ['BidApplSeqNum'].isnull(), tradeDataSZ['OfferApplSeqNum'], tradeDataSZ['BidApplSeqNum'])\n",
    "#     tradeDataSZ['Side'] = np.where(tradeDataSZ['BidApplSeqNum'].isnull(), 2, 1)\n",
    "#     tradeDataSZ = tradeDataSZ[['SecurityID', 'ApplSeqNum', 'clockAtArrival', 'sequenceNo', 'Side', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]]\n",
    "#     tradeDataSZ['updateType'] = 4\n",
    "#     cancelDataSZ = rawMsgDataSZ[rawMsgDataSZ['ExecType'] == '4'][['SecurityID', 'BidApplSeqNum', 'OfferApplSeqNum', 'clockAtArrival', 'sequenceNo', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]].reset_index(drop=True)\n",
    "#     cancelDataSZ['ApplSeqNum'] = np.where(cancelDataSZ['BidApplSeqNum'] == 0, cancelDataSZ['OfferApplSeqNum'], cancelDataSZ['BidApplSeqNum'])\n",
    "#     cancelDataSZ['Side'] = np.where(cancelDataSZ['BidApplSeqNum'] == 0, 2, 1)\n",
    "#     cancelDataSZ = cancelDataSZ[['SecurityID', 'ApplSeqNum', 'clockAtArrival', 'sequenceNo', 'Side', 'TradeQty', 'cum_volume', \"TransactTime\"]]\n",
    "#     cancelDataSZ['updateType'] = 3\n",
    "\n",
    "#     msgDataSZ = pd.concat([orderDataSZ, tradeDataSZ, cancelDataSZ], sort=False)\n",
    "#     del orderDataSZ\n",
    "#     del tradeDataSZ\n",
    "#     del cancelDataSZ\n",
    "#     msgDataSZ = msgDataSZ.sort_values(by=['SecurityID', 'ApplSeqNum', 'sequenceNo']).reset_index(drop=True)\n",
    "    \n",
    "#     msgDataSZ['TradePrice'] = np.where(msgDataSZ['updateType'] == 4, msgDataSZ['TradePrice'], 0)\n",
    "#     msgDataSZ['TradePrice'] = msgDataSZ['TradePrice'].astype('int64')\n",
    "#     msgDataSZ['TradeQty'] = np.where(msgDataSZ['updateType'] == 4, msgDataSZ['TradeQty'], 0)\n",
    "#     msgDataSZ['TradeQty'] = msgDataSZ['TradeQty'].astype('int64')\n",
    "#     msgDataSZ['secid'] = msgDataSZ['SecurityID'] + 2000000\n",
    "#     assert(msgDataSZ['ApplSeqNum'].max() < 1e8)\n",
    "#     msgDataSZ['StockSeqNum'] = msgDataSZ['SecurityID']*1e8 + msgDataSZ['ApplSeqNum']\n",
    "#     msgDataSZ['date'] = int(thisDate)\n",
    "#     msgDataSZ['startVolume'] = msgDataSZ.groupby(['StockSeqNum'])['cum_volume'].transform('first')\n",
    "            \n",
    "#     ### order insertion position\n",
    "#     startPos = orderLog[(orderLog['date'] == int(thisDate)) & (orderLog['updateType'] == 0) & (orderLog['secid'] >= 2000000) & (orderLog[\"isMsg\"] == 1)]\n",
    "#     # here!!!!!!!!!! drop duplicates\n",
    "#     startPos = startPos.drop_duplicates(subset=['date', 'secid', 'vai'])\n",
    "    \n",
    "#     startPos = startPos[((startPos.clock.dt.time >= datetime.time(9, 33)) & (startPos.clock.dt.time <= datetime.time(11, 30))) |\\\n",
    "#                         ((startPos.clock.dt.time >= datetime.time(13, 3)) & (startPos.clock.dt.time <= datetime.time(14, 55)))]\n",
    "# #     startPos = startPos[startPos.clock.dt.time < datetime.time(9, 33)]\n",
    "    \n",
    "#     startPos['SecurityID'] = startPos['secid']-2000000\n",
    "#     startPos['orderDirection1'] = np.where(startPos['orderDirection1'] == 1, 1, 2)\n",
    "#     startPos['cum_volume'] = startPos['vai']\n",
    "#     startPos = startPos[['SecurityID', 'cum_volume', 'orderDirection1', 'accCode', 'absOrderSize', 'vai', 'group']]\n",
    "#     startPos['stockGroup'] = startPos.groupby(['accCode', 'SecurityID']).grouper.group_info[0]\n",
    "#     startPos = startPos.sort_values(by=['SecurityID', 'cum_volume']).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "#     ### generate order status change data\n",
    "#     infoData = orderLog[(orderLog['date'] == int(thisDate)) & (orderLog['group'].isin(startPos['group'].unique())) & (orderLog['updateType'].isin([0, 3, 4]))].reset_index(drop=True)\n",
    "#     infoData['Price'] = infoData['orderPrice'].apply(lambda x: round(x*100, 0))\n",
    "#     infoData['Price'] = infoData['Price'].astype('int64')*100\n",
    "#     infoData['OrderQty'] = infoData['absOrderSize']\n",
    "#     infoData['Side'] = np.where(infoData['orderDirection1'] == 1, 1, 2)\n",
    "#     infoData['TradePrice'] = np.where(infoData['updateType'] == 4, round(infoData['tradePrice']*100, 0), 0)\n",
    "#     infoData['TradePrice'] = infoData['TradePrice'].astype('int64')*100\n",
    "#     statusInfo = infoData.groupby(['order'])['updateType'].apply(lambda x: tuple(x)).reset_index()\n",
    "#     statusInfo.columns = ['order', 'statusLs']\n",
    "#     tradePriceInfo = infoData.groupby(['order'])['TradePrice'].apply(lambda x: tuple(x)).reset_index()\n",
    "#     tradePriceInfo.columns = ['order', 'TradePriceLs']\n",
    "#     tradeQtyInfo = infoData.groupby(['order'])['absFilledThisUpdate'].apply(lambda x: tuple(x)).reset_index()\n",
    "#     tradeQtyInfo.columns = ['order', 'TradeQtyLs']\n",
    "#     infoData[\"abstradeNtl\"] = infoData[\"absFilledThisUpdate\"] * infoData[\"TradePrice\"]\n",
    "#     infoData[\"TradeNtl\"] = infoData.groupby(['order'])['abstradeNtl'].transform('sum')\n",
    "#     infoData = infoData[infoData['updateType'] == 0]\n",
    "#     infoData = pd.merge(infoData, statusInfo, how='left', on=['order'], validate='one_to_one')\n",
    "#     infoData = pd.merge(infoData, tradePriceInfo, how='left', on=['order'], validate='one_to_one')\n",
    "#     infoData = pd.merge(infoData, tradeQtyInfo, how='left', on=['order'], validate='one_to_one')\n",
    "#     infoData['brokerNum'] = infoData.groupby(['date', 'secid', 'vai', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs'])['colo_account'].transform('count')\n",
    "#     infoData['brokerLs'] = infoData.groupby(['date', 'secid', 'vai', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs'])['colo_account'].transform(lambda x: ','.join(sorted(x.unique())))\n",
    "#     display(infoData[(infoData[\"brokerNum\"] >= 2) & (infoData[\"orderDirection\"].isin([-2, 2]))][\"group\"].nunique())\n",
    "#     display(infoData[(infoData[\"brokerNum\"] < 2) & (infoData[\"orderDirection\"].isin([-2, 2]))][\"group\"].nunique())\n",
    "#     display(infoData[(infoData[\"brokerNum\"] >= 2) & (infoData[\"orderDirection\"].isin([-2, 2]))][\"group\"].unique())\n",
    "#     gl = infoData[(infoData[\"brokerNum\"] >= 2) & (infoData[\"orderDirection\"].isin([-2, 2]))][\"group\"].unique()\n",
    "#     gl1 = infoData[\"group\"].unique()\n",
    "#     print(len(gl1))\n",
    "#     print(len(startPos[\"group\"].unique()))\n",
    "#     infoData = infoData.drop_duplicates(subset=['date', 'secid', 'vai', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs']).reset_index(drop=True)\n",
    "#     infoData = infoData[['date', 'secid', 'vai', 'ars', \"mrstaat\", \"mrstauc\", 'isMsg', 'TradeNtl', 'exchange', 'group', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'brokerNum', 'brokerLs', 'order']]\n",
    "    \n",
    "#     ### find all orders in market that inserted with us\n",
    "#     checkLog = msgDataSZ[msgDataSZ['updateType'] == 0][['StockSeqNum', 'SecurityID', 'cum_volume', 'sequenceNo', 'clockAtArrival', 'Side', 'OrderQty', 'Price', \"TransactTime\"]].reset_index(drop=True)\n",
    "#     checkLog = checkLog.sort_values(by=['SecurityID', 'sequenceNo'])\n",
    "#     checkLog = pd.merge(checkLog, startPos, how='outer', on=['SecurityID', 'cum_volume'], validate='many_to_one')\n",
    "# #     del startPos\n",
    "#     if checkLog[(~checkLog['vai'].isnull()) & (checkLog['clockAtArrival'].isnull())].shape[0] > 0:\n",
    "#         print('some order vai are wrong')\n",
    "#         removeGroup = checkLog[(~checkLog['vai'].isnull()) & (checkLog['clockAtArrival'].isnull())]['stockGroup'].unique()\n",
    "#         print(removeGroup)\n",
    "#         print(len(gl1))\n",
    "#         print(len(set(checkLog[\"group\"].unique())))\n",
    "#         checkLog = checkLog[~checkLog['stockGroup'].isin(removeGroup)].reset_index(drop=True)\n",
    "#         display(len(set(gl1) - set(checkLog[\"group\"].unique())))\n",
    "    \n",
    "#     assert(checkLog[(~checkLog['vai'].isnull()) & (checkLog['clockAtArrival'].isnull())].shape[0] == 0)\n",
    "#     checkLog['lastClockInVol'] = checkLog.groupby(['SecurityID', 'cum_volume'])['clockAtArrival'].transform('last')\n",
    "#     checkLog['startClock'] = np.where((~checkLog['group'].isnull()) & (checkLog['clockAtArrival'] == checkLog['lastClockInVol']),\n",
    "#                                       checkLog['clockAtArrival'], np.nan)\n",
    "#     checkLog['group'] = checkLog['group'].ffill()\n",
    "#     checkLog['startClock'] = checkLog.groupby(['SecurityID', 'group'])['startClock'].transform('max')\n",
    "#     checkLog['endClock'] = checkLog['startClock'] + 20*1e3\n",
    "#     checkLog = pd.concat([checkLog[checkLog['clockAtArrival'] == checkLog['startClock']], \n",
    "#                           checkLog[(checkLog['clockAtArrival'] > checkLog['startClock'] + 10) & \n",
    "#                                    (checkLog['clockAtArrival'] <= checkLog['endClock'])]])\n",
    "#     checkLog = checkLog.sort_values(by=['SecurityID', 'group'])\n",
    "#     checkLog['vai'] = checkLog.groupby(['SecurityID', 'group'])['vai'].ffill()\n",
    "#     checkLog['orderDirection1'] = checkLog.groupby(['SecurityID', 'group'])['orderDirection1'].ffill()\n",
    "#     checkLog = pd.concat([checkLog[checkLog['clockAtArrival'] == checkLog['startClock']],\n",
    "#                          checkLog[(checkLog['Side'] == checkLog['orderDirection1']) & (checkLog['clockAtArrival'] != checkLog['startClock'])]]).sort_values(by=['SecurityID', 'group'])\n",
    "     \n",
    "#     group_list = checkLog[\"group\"].unique()\n",
    "\n",
    "    \n",
    "#     checkLog = pd.merge(msgDataSZ, checkLog[['StockSeqNum', 'vai', 'group']], how='left', on=['StockSeqNum'], validate='many_to_one')\n",
    "#     del msgDataSZ\n",
    "#     checkLog = checkLog[~checkLog['group'].isnull()]\n",
    "#     # inner merge!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#     statusInfo = checkLog.groupby(['StockSeqNum'])['updateType'].apply(lambda x: getTuple(x)).reset_index()\n",
    "#     statusInfo.columns = ['StockSeqNum', 'statusLs']\n",
    "#     tradePriceInfo = checkLog.groupby(['StockSeqNum'])['TradePrice'].apply(lambda x: tuple(x)).reset_index()\n",
    "#     tradePriceInfo.columns = ['StockSeqNum', 'TradePriceLs']\n",
    "#     tradeQtyInfo = checkLog.groupby(['StockSeqNum'])['TradeQty'].apply(lambda x: tuple(x)).reset_index()\n",
    "#     tradeQtyInfo.columns = ['StockSeqNum', 'TradeQtyLs']\n",
    "#     checkLog = checkLog[checkLog['updateType'] == 0]\n",
    "#     checkLog = pd.merge(checkLog, statusInfo, how='left', on=['StockSeqNum'], validate='one_to_one')\n",
    "#     checkLog = pd.merge(checkLog, tradePriceInfo, how='left', on=['StockSeqNum'], validate='one_to_one')\n",
    "#     checkLog = pd.merge(checkLog, tradeQtyInfo, how='left', on=['StockSeqNum'], validate='one_to_one')\n",
    "    \n",
    "#     infoData = infoData[infoData[\"group\"].isin(group_list)]\n",
    "\n",
    "    \n",
    "#     checkLog = pd.merge(checkLog, infoData, how='outer', on=['date', 'secid', 'group', 'vai', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs'], validate='many_to_one')\n",
    "#     display(set(gl) & set(checkLog[\"group\"].unique()))\n",
    "#     display(len(set(gl1) & set(checkLog[\"group\"].unique())))\n",
    "#     checkLog = checkLog.sort_values(by=['date', 'secid', 'vai', 'sequenceNo']).reset_index(drop=True)\n",
    "#     ### orderType 1 orders have 0 order price, replace 0 with group price\n",
    "#     checkLog['groupPrice'] = checkLog.groupby(['group'])['Price'].transform('median')\n",
    "#     checkLog['Price'] = np.where(checkLog['Price'] == 0, checkLog['groupPrice'], checkLog['Price'])\n",
    "#     checkLog['OrderNtl'] = checkLog['Price'] * checkLog['OrderQty'] / 10000\n",
    "    \n",
    "#     savePath = 'L:\\\\orderLog\\\\result\\\\marketPos'\n",
    "#     checkLog.reset_index(drop=True).to_pickle(os.path.join(savePath, 'marketPos1_%s.pkl'%thisDate))\n",
    "#     del checkLog\n",
    "    \n",
    "#     print(datetime.datetime.now() - startTm)\n",
    "\n",
    "\n",
    "from twisted.internet import task, reactor\n",
    "import schedule\n",
    "\n",
    "\n",
    "def sleeptime(hour,min,sec):\n",
    "    return hour*3600 + min*60 + sec\n",
    "\n",
    "import pymongo\n",
    "import io\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db', version=3): \n",
    "        self.db_name = db_name \n",
    "        self.uri = uri \n",
    "        self.client = pymongo.MongoClient(self.uri) \n",
    "        self.db = self.client[self.db_name] \n",
    "        self.chunk_size = 20000 \n",
    "        self.symbol_column = symbol_column \n",
    "        self.date_column = 'date' \n",
    "        self.version = version\n",
    "\n",
    "    def parse_uri(self, uri): \n",
    "        # mongodb://user:password@example.com \n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"date must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid date type: \" + str(type(x)))\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "        return query\n",
    "\n",
    "    def read_tick(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name] \n",
    "        query = self.build_query(start_date, end_date, symbol) \n",
    "        if not query: \n",
    "            print('cannot read the whole table') \n",
    "            return None  \n",
    "        segs = [] \n",
    "        for x in collection.find(query): \n",
    "            x['data'] = self.deser(x['data'], x['ver']) \n",
    "            segs.append(x) \n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start'])) \n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def read_daily(self, table_name, start_date=None, end_date=None, skey=None, index_id=None, interval=None, index_name=None, col=None, return_sdi=True): \n",
    "        collection = self.db[table_name]\n",
    "        # Build projection \n",
    "        prj = {'_id': 0} \n",
    "        if col is not None: \n",
    "            if return_sdi: \n",
    "                col = ['skey', 'date', 'index_id'] + col \n",
    "            for col_name in col: \n",
    "                prj[col_name] = 1 \n",
    "        # Build query \n",
    "        query = {} \n",
    "        if skey is not None: \n",
    "            query['skey'] = {'$in': skey} \n",
    "        if interval is not None: \n",
    "            query['interval'] = {'$in': interval} \n",
    "        if index_id is not None: \n",
    "            query['index_id'] = {'$in': index_id}    \n",
    "        if index_name is not None:\n",
    "            n = '' \n",
    "            for name in index_name: \n",
    "                try: \n",
    "                    name = re.compile('[\\u4e00-\\u9fff]+').findall(name)[0] \n",
    "                    if len(n) == 0: \n",
    "                        n = n = \"|\".join(name) \n",
    "                    else: \n",
    "                        n = n + '|' + \"|\".join(name) \n",
    "                except: \n",
    "                    if len(n) == 0: \n",
    "                        n = name \n",
    "                    else: \n",
    "                        n = n + '|' + name \n",
    "            query['index_name'] = {'$regex': n}\n",
    "        if start_date is not None: \n",
    "            if end_date is not None: \n",
    "                query['date'] = {'$gte': start_date, '$lte': end_date} \n",
    "            else: \n",
    "                query['date'] = {'$gte': start_date} \n",
    "        elif end_date is not None: \n",
    "            query['date'] = {'$lte': end_date} \n",
    "        # Load data \n",
    "        cur = collection.find(query, prj) \n",
    "        df = pd.DataFrame.from_records(cur) \n",
    "        if df.empty: \n",
    "            df = pd.DataFrame() \n",
    "        else:\n",
    "            if 'index_id' in df.columns:\n",
    "                df = df.sort_values(by=['date', 'index_id', 'skey']).reset_index(drop=True)\n",
    "            else:\n",
    "                df = df.sort_values(by=['date','skey']).reset_index(drop=True)\n",
    "        return df \n",
    " \n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def deser(self, s, version): \n",
    "        def unpickle(s): \n",
    "            return pickle.loads(s) \n",
    "        if version == 1: \n",
    "            return unpickle(gzip.decompress(s)) \n",
    "        elif version == 2: \n",
    "            return unpickle(lzma.decompress(s)) \n",
    "        elif version == 3: \n",
    "            f = io.BytesIO() \n",
    "            f.write(s) \n",
    "            f.seek(0) \n",
    "            return pq.read_table(f, use_threads=False).to_pandas() \n",
    "        else: \n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import shutil\n",
    "import datetime\n",
    "def un_gz(file_name, dirs):\n",
    "    t = tarfile.open(file_name)\n",
    "    dirs = dirs + '\\\\' + os.path.basename(file_name).split('.')[0]\n",
    "    try:\n",
    "        os.mkdir(dirs)\n",
    "    except:\n",
    "        'The directory has already been created'\n",
    "    t.extractall(path=dirs)\n",
    "    da_te = os.path.basename(file_name).split('_')[1]\n",
    "    readPath = dirs + '\\\\md***' + da_te + '***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    readPath = dirs + '\\\\***'\n",
    "    dataPathLs1 = np.array(glob.glob(readPath))\n",
    "    for i in list(set(dataPathLs1) - set(dataPathLs)):\n",
    "        if os.path.exists(i):\n",
    "            if os.path.isfile(i):\n",
    "                os.remove(i)\n",
    "            if os.path.isdir(i):\n",
    "                shutil.rmtree(i) \n",
    "    for i in dataPathLs:\n",
    "        if os.path.getsize(i) < 1000000000:\n",
    "            os.remove(i)\n",
    "    print('finish ' + da_te)\n",
    "    \n",
    "\n",
    "class Test(object):\n",
    "    def __init__(self):\n",
    "        self.status = True        \n",
    "    def test(self):\n",
    "        while self.status == True:\n",
    "            try:\n",
    "                print(datetime.datetime.now())\n",
    "                date = (datetime.datetime.today()).strftime('%Y%m%d')\n",
    "                readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "                dataPathLs = np.array(glob.glob(os.path.join(readPath, 'speedCompare***.csv')))\n",
    "                dateLs = np.array([int(os.path.basename(i).split('_')[1].split('.')[0]) for i in dataPathLs])\n",
    "                assert(np.max(dateLs) == int(date))\n",
    "                print('orderLog is ready')\n",
    "                \n",
    "                readPath = '\\\\\\\\192.168.10.34\\\\trading\\\\dailyRawData\\\\' + date + '\\\\***zs_92_01***'\n",
    "                dataPathLs = np.array(glob.glob(readPath))\n",
    "                assert(len(dataPathLs) == 1)\n",
    "                dateLs = np.array(glob.glob('\\\\\\\\192.168.10.34\\\\random_backup\\\\Kevin_zhenyu\\\\rawData\\\\***'))\n",
    "                dateLs = [int(os.path.basename(i).split('_')[1]) for i in dateLs]\n",
    "                if np.max(dateLs) < int(date):\n",
    "                    un_gz(dataPathLs[0], '\\\\\\\\192.168.10.34\\\\random_backup\\\\Kevin_zhenyu\\\\rawData')\n",
    "                print('market data is ready')\n",
    "                \n",
    "                database_name = 'com_md_eq_cn'\n",
    "                user = 'zhenyuy'\n",
    "                password = 'bnONBrzSMGoE'\n",
    "                db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "                beta = db1.read_daily('mktbeta', start_date=int(date), end_date=int(date))\n",
    "                assert(beta.shape[0] != 0)\n",
    "                print('beta data is ready')\n",
    "                \n",
    "                print('We start to generate data now')  \n",
    "                download_data(date)\n",
    "                self.status = False\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('still wait for data coming')\n",
    "                second = sleeptime(0,5,0)\n",
    "                time.sleep(second)\n",
    "\n",
    "test1 = Test()\n",
    "schedule.every().day.at(\"21:19\").do(test1.test)\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing raw Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3249: DtypeWarning: Columns (27,55,64) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3249: DtypeWarning: Columns (12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3249: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:13:16.436434\n",
      "start tickToMBD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3249: DtypeWarning: Columns (27,55,64) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are accounts with duplicated ticks:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Series([], Name: ars, dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'There are ticks with orderDirection 0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>colo</th>\n",
       "      <th>accCode</th>\n",
       "      <th>secid</th>\n",
       "      <th>vai</th>\n",
       "      <th>updateType</th>\n",
       "      <th>sdd</th>\n",
       "      <th>orderDirection</th>\n",
       "      <th>absOrderSize</th>\n",
       "      <th>internalId</th>\n",
       "      <th>orderId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31619</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_96_08</td>\n",
       "      <td>6237</td>\n",
       "      <td>1600167</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447180</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_94_05</td>\n",
       "      <td>9471</td>\n",
       "      <td>2000038</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>47153.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.364407e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538360</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_54_01</td>\n",
       "      <td>5474</td>\n",
       "      <td>2002009</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>53653.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545465</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_94_03</td>\n",
       "      <td>9448</td>\n",
       "      <td>2002030</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>38319.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565381</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_64_01</td>\n",
       "      <td>6480</td>\n",
       "      <td>2002112</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>49434.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.645100e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590349</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_64_01</td>\n",
       "      <td>6480</td>\n",
       "      <td>2002208</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>40347.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.164390e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616976</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_52_09</td>\n",
       "      <td>5291</td>\n",
       "      <td>2002299</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>34220.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.624500e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693058</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_66_01</td>\n",
       "      <td>6634</td>\n",
       "      <td>2002576</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>46962.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.812811e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695422</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_64_01</td>\n",
       "      <td>6480</td>\n",
       "      <td>2002584</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>34975.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3.422300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695730</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_66_01</td>\n",
       "      <td>6634</td>\n",
       "      <td>2002587</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>47520.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.812812e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713233</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_64_01</td>\n",
       "      <td>6480</td>\n",
       "      <td>2002637</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>47174.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.309590e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749468</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_94_05</td>\n",
       "      <td>9454</td>\n",
       "      <td>2002795</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>49114.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.386163e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756455</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_66_01</td>\n",
       "      <td>6634</td>\n",
       "      <td>2002823</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>39778.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765370</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_54_01</td>\n",
       "      <td>5474</td>\n",
       "      <td>2002855</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>48213.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.689140e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765411</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_66_01</td>\n",
       "      <td>6634</td>\n",
       "      <td>2002855</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>48213.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.812812e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779550</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_52_09</td>\n",
       "      <td>5289</td>\n",
       "      <td>2002920</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>52249.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.514744e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785925</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_54_01</td>\n",
       "      <td>5474</td>\n",
       "      <td>2002932</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>35909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.689052e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797654</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_96_08</td>\n",
       "      <td>9685</td>\n",
       "      <td>2002970</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>49953.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7.221449e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850311</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_64_01</td>\n",
       "      <td>6480</td>\n",
       "      <td>2300151</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>52851.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.060000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881528</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_52_09</td>\n",
       "      <td>5289</td>\n",
       "      <td>2300271</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>53138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.590729e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923598</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_94_05</td>\n",
       "      <td>9454</td>\n",
       "      <td>2300407</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>35412.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975363</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_64_01</td>\n",
       "      <td>6480</td>\n",
       "      <td>2300540</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>35269.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.425300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008314</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_54_01</td>\n",
       "      <td>5474</td>\n",
       "      <td>2300621</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>47918.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>8.689137e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011027</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_66_01</td>\n",
       "      <td>6634</td>\n",
       "      <td>2300629</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>39656.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051746</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_64_01</td>\n",
       "      <td>6480</td>\n",
       "      <td>2300745</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>51277.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.892020e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072883</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_94_05</td>\n",
       "      <td>9471</td>\n",
       "      <td>2300793</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>52736.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.415301e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082882</th>\n",
       "      <td>20210107</td>\n",
       "      <td>zs_94_05</td>\n",
       "      <td>9454</td>\n",
       "      <td>2300825</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>40994.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             date      colo  accCode    secid  vai  updateType      sdd  \\\n",
       "31619    20210107  zs_96_08     6237  1600167   -1           7     -1.0   \n",
       "447180   20210107  zs_94_05     9471  2000038   -1           1  47153.0   \n",
       "538360   20210107  zs_54_01     5474  2002009   -1           7  53653.0   \n",
       "545465   20210107  zs_94_03     9448  2002030   -1           7  38319.0   \n",
       "565381   20210107  zs_64_01     6480  2002112   -1           1  49434.0   \n",
       "590349   20210107  zs_64_01     6480  2002208   -1           1  40347.0   \n",
       "616976   20210107  zs_52_09     5291  2002299   -1           1  34220.0   \n",
       "693058   20210107  zs_66_01     6634  2002576   -1           1  46962.0   \n",
       "695422   20210107  zs_64_01     6480  2002584   -1           1  34975.0   \n",
       "695730   20210107  zs_66_01     6634  2002587   -1           1  47520.0   \n",
       "713233   20210107  zs_64_01     6480  2002637   -1           1  47174.0   \n",
       "749468   20210107  zs_94_05     9454  2002795   -1           1  49114.0   \n",
       "756455   20210107  zs_66_01     6634  2002823   -1           7  39778.0   \n",
       "765370   20210107  zs_54_01     5474  2002855   -1           1  48213.0   \n",
       "765411   20210107  zs_66_01     6634  2002855   -1           1  48213.0   \n",
       "779550   20210107  zs_52_09     5289  2002920   -1           1  52249.0   \n",
       "785925   20210107  zs_54_01     5474  2002932   -1           1  35909.0   \n",
       "797654   20210107  zs_96_08     9685  2002970   -1           1  49953.0   \n",
       "850311   20210107  zs_64_01     6480  2300151   -1           1  52851.0   \n",
       "881528   20210107  zs_52_09     5289  2300271   -1           1  53138.0   \n",
       "923598   20210107  zs_94_05     9454  2300407   -1           7  35412.0   \n",
       "975363   20210107  zs_64_01     6480  2300540   -1           1  35269.0   \n",
       "1008314  20210107  zs_54_01     5474  2300621   -1           1  47918.0   \n",
       "1011027  20210107  zs_66_01     6634  2300629   -1           7  39656.0   \n",
       "1051746  20210107  zs_64_01     6480  2300745   -1           1  51277.0   \n",
       "1072883  20210107  zs_94_05     9471  2300793   -1           1  52736.0   \n",
       "1082882  20210107  zs_94_05     9454  2300825   -1           7  40994.0   \n",
       "\n",
       "         orderDirection  absOrderSize  internalId       orderId  \n",
       "31619                 0             0        -1.0 -1.000000e+00  \n",
       "447180                0             0        -1.0  2.364407e+06  \n",
       "538360                0             0        -1.0 -1.000000e+00  \n",
       "545465                0             0        -1.0 -1.000000e+00  \n",
       "565381                0             0        -1.0  1.645100e+05  \n",
       "590349                0             0        -1.0  1.164390e+05  \n",
       "616976                0             0        -1.0  2.624500e+04  \n",
       "693058                0             0        -1.0  1.812811e+10  \n",
       "695422                0             0        -1.0  3.422300e+04  \n",
       "695730                0             0        -1.0  1.812812e+10  \n",
       "713233                0             0        -1.0  1.309590e+05  \n",
       "749468                0             0        -1.0  2.386163e+06  \n",
       "756455                0             0        -1.0 -1.000000e+00  \n",
       "765370                0             0        -1.0  8.689140e+08  \n",
       "765411                0             0        -1.0  1.812812e+10  \n",
       "779550                0             0        -1.0  1.514744e+06  \n",
       "785925                0             0        -1.0  8.689052e+08  \n",
       "797654                0             0        -1.0  7.221449e+17  \n",
       "850311                0             0        -1.0  2.060000e+05  \n",
       "881528                0             0        -1.0  1.590729e+06  \n",
       "923598                0             0        -1.0 -1.000000e+00  \n",
       "975363                0             0        -1.0  4.425300e+04  \n",
       "1008314               0             0        -1.0  8.689137e+08  \n",
       "1011027               0             0        -1.0 -1.000000e+00  \n",
       "1051746               0             0        -1.0  1.892020e+05  \n",
       "1072883               0             0        -1.0  2.415301e+06  \n",
       "1082882               0             0        -1.0 -1.000000e+00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:169: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are orders in 8856 with same internalId and various orderId!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "     secid  accCode      colo      vai  updateType         sdd  internalId  \\\n",
      "0  2300271     8856  zs_88_04  22400.0           0  93007340.0        30.0   \n",
      "1  2300271     8856  zs_88_04  22400.0           2        -1.0        30.0   \n",
      "2  2300271     8856  zs_88_04  22400.0           1     34209.0        30.0   \n",
      "3  2300271     8856  zs_88_04  22400.0           3        -1.0        30.0   \n",
      "4  2300271     8856  zs_88_04  22400.0           0  93009860.0        34.0   \n",
      "5  2300271     8856  zs_88_04  22400.0           2        -1.0        34.0   \n",
      "6  2300271     8856  zs_88_04  22400.0           2        -1.0        34.0   \n",
      "\n",
      "   orderId  absOrderSize  absFilledThisUpdate  absOrderSizeCumFilled  \\\n",
      "0     30.0           100                    0                      0   \n",
      "1     30.0           100                    0                      0   \n",
      "2     30.0           100                    0                      0   \n",
      "3     30.0           100                    0                      0   \n",
      "4     34.0           100                    0                      0   \n",
      "5     34.0           100                    0                      0   \n",
      "6     34.0           100                  100                    100   \n",
      "\n",
      "   orderPrice  tradePrice  \n",
      "0       24.73        -1.0  \n",
      "1       24.73        -1.0  \n",
      "2       24.73        -1.0  \n",
      "3       24.73        -1.0  \n",
      "4       24.70        -1.0  \n",
      "5       24.70        -1.0  \n",
      "6       24.70        24.7  \n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-855efff11c1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# for d in dateLs[2:]k:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     download_data(d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdownload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'20210107'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-115494a9db25>\u001b[0m in \u001b[0;36mdownload_data\u001b[1;34m(dd)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mtradeDataSZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mcancelDataSZ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[0mmsgDataSZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsgDataSZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SecurityID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ApplSeqNum'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sequenceNo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[0mmsgDataSZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TradePrice'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsgDataSZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'updateType'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsgDataSZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TradePrice'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[0mmsgDataSZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TradePrice'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsgDataSZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TradePrice'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreset_index\u001b[1;34m(self, level, drop, inplace, col_level, col_fill)\u001b[0m\n\u001b[0;32m   4762\u001b[0m             \u001b[0mnew_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4763\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4764\u001b[1;33m             \u001b[0mnew_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4766\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_casted_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   5652\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5653\u001b[0m         \"\"\"\n\u001b[1;32m-> 5654\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5655\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5656\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    773\u001b[0m             \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "# dataLs = np.array(glob.glob(r'L:\\orderLog\\data\\***'))\n",
    "# dateLs = [os.path.basename(i).split('.')[0] for i in dataLs]\n",
    "# for d in dateLs[2:]k:\n",
    "#     download_data(d)\n",
    "download_data('20210107')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
