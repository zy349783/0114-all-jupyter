{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "\n",
    "def DB(host, db_name, user, passwd):\n",
    "    auth_db = db_name if user not in ('admin', 'root') else 'admin'\n",
    "    uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "    return DBObj(uri, db_name=db_name)\n",
    "\n",
    "\n",
    "class DBObj(object):\n",
    "    def __init__(self, uri, symbol_column='skey', db_name='white_db'):\n",
    "        self.db_name = db_name\n",
    "        self.uri = uri\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def rename_table(self, old_table, new_table):\n",
    "        self.db[old_table].rename(new_table)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        multi_date = False\n",
    "\n",
    "        if self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "            multi_date = len(df[self.date_column].unique()) > 1\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain date column')\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        if multi_date:\n",
    "            for (date, symbol), sub_df in df.groupby([self.date_column, self.symbol_column]):\n",
    "                date = str(date)\n",
    "                symbol = int(symbol)\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "        else:\n",
    "            for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "                collection.delete_many({'date': date, 'symbol': symbol})\n",
    "                self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date is not None or end_date is not None:\n",
    "            query['date'] = {}\n",
    "            if start_date is not None:\n",
    "                query['date']['$gte'] = parse_date(start_date)\n",
    "            if end_date is not None:\n",
    "                query['date']['$lte'] = parse_date(end_date)\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat([x['data'] for x in segs], ignore_index=True) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        pickle_protocol = 4\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s, protocol=pickle_protocol), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s, protocol=pickle_protocol), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "\n",
    "def patch_pandas_pickle():\n",
    "    if pd.__version__ < '0.24':\n",
    "        import sys\n",
    "        from types import ModuleType\n",
    "        from pandas.core.internals import BlockManager\n",
    "        pkg_name = 'pandas.core.internals.managers'\n",
    "        if pkg_name not in sys.modules:\n",
    "            m = ModuleType(pkg_name)\n",
    "            m.BlockManager = BlockManager\n",
    "            sys.modules[pkg_name] = m\n",
    "patch_pandas_pickle()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import TSLPy3\n",
    "\n",
    "\n",
    "ul = pd.read_csv(r'D:\\work\\project 17 AMAC\\tickStockList_AMAC.csv')\n",
    "ul = ul['StockID'].values\n",
    "\n",
    "startDate = '20201123T'\n",
    "endDate = '20201123T'\n",
    "for num in range(len(ul)):\n",
    "    stock=ul[num]\n",
    "    tickname = 'Tick_'+ stock\n",
    "    if num%10 == 0: print('Processing ' + str(num) +' AMAC '+stock)\n",
    "    tsstr=\"\"\"\n",
    "           BegT :=%s;\n",
    "           EndT :=%s + 0.99;\n",
    "           setSysParam(pn_stock(),'%s');\n",
    "           returnData := select ['date'],['close'],['sectional_open'],['sectional_vol'],['sectional_amount']\n",
    "                         from tradetable datekey BegT to EndT of DefaultStockID() end;\n",
    "           return returnData;\n",
    "           \"\"\"%(startDate,endDate,stock)\n",
    "    Tick_Stock = pd.DataFrame(TSLPy3.RemoteExecute(tsstr,{})[1])\n",
    "    Tick_Stock.columns = list(pd.Series(Tick_Stock.columns).str.decode('GBK'))\n",
    "    Tick_Stock['intdate'] = Tick_Stock.date.astype(int)\n",
    "    Tick_Stock['time'] = Tick_Stock.date.map(lambda x: datetime.datetime.utcfromtimestamp(round((x - 25569) * 86400.0)))\n",
    "    Tick_Stock['adjTime'] = Tick_Stock.date.map(lambda x: datetime.datetime.utcfromtimestamp(round((x - 25569) * 86400.0) - 1))\n",
    "    Tick_Stock['minute'] = Tick_Stock.adjTime.map(lambda x: (x.hour*60 + x.minute + 1))\n",
    "    assert (Tick_Stock.minute.max() >= 900) & (Tick_Stock.minute.min() <= 570)\n",
    "    Tick_Stock['morning'] = np.where(Tick_Stock.minute <= 690, 1, 0)          \n",
    "    Tick_Stock.rename(columns = {'sectional_open':'industry_open','sectional_vol':'cum_volume','sectional_amount':'cum_amount'}, inplace=True)            \n",
    "    Tick_Stock = Tick_Stock[['intdate','minute','morning','time','close','industry_open','cum_volume','cum_amount']].reset_index(drop = True)\n",
    "    Tick_Stock['ID'] = stock\n",
    "    ## ordering per day per stock\n",
    "    for intD in Tick_Stock.intdate.unique():\n",
    "        Tick_Stock.loc[Tick_Stock.intdate == intD, 'ordering'] = range(0, len(Tick_Stock.loc[Tick_Stock.intdate == intD, 'ID']))\n",
    "    Tick_Stock['month'] = Tick_Stock.time.dt.month + Tick_Stock.time.dt.year * 100\n",
    "    test = Tick_Stock\n",
    "\n",
    "    test['date'] = test['time'].astype(str).apply(lambda x: int(x.split(' ')[0].replace(\"-\", \"\")))\n",
    "    test['time'] = test['time'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(\":\", \"\")))\n",
    "    test['datetime'] = test['date'] * 1000000 + test['time']\n",
    "    test[\"clockAtArrival\"] = test[\"datetime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "    test['datetime'] = test[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    test['time'] = test['time'].astype('int64') * 1000000\n",
    "    test['skey'] = test['ID'].str[4:].astype(int) + 3000000\n",
    "    test = test.rename(columns={'industry_open':\"open\"})\n",
    "    test['open1'] = test.groupby(['skey', 'date'])['open'].transform('max')\n",
    "    test = test.sort_values(by=['skey', 'date', 'ordering'])\n",
    "    test['close1'] = test.groupby(['skey'])['close'].shift(1)\n",
    "    # 每天只有一条tick cum_volume==0, 且当时open==today's open, close==yesterday's close\n",
    "    # 20180215, 20180220, 20180221不满足条件，这几天无交易，close=0；20180215, 20180222 close close1也无法对上\n",
    "    assert(sum(test[test['cum_volume'] == 0].groupby(['skey', 'date'])['ordering'].size() != 1) == 0)\n",
    "    assert(sum(test[test['cum_volume'] == 0].groupby(['skey', 'date'])['ordering'].unique() != 0) == 0)\n",
    "    try:\n",
    "        assert((test[test['cum_volume'] == 0]['open'].min() > 0) & (test[test['open'] != test['open1']].shape[0] == 0))\n",
    "        assert(sum(test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close'] != \n",
    "        test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close1']) == 0)\n",
    "    except:\n",
    "        print(test[(test['cum_volume'] == 0) & (test['open'] == 0)]['datetime'].unique())\n",
    "        print(test[(test['cum_volume'] == 0) & (~test['close1'].isnull())][test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close'] != \n",
    "        test[(test['cum_volume'] == 0) & (~test['close1'].isnull())]['close1']]['datetime'].unique())\n",
    "    test = test[test['cum_volume'] != 0]\n",
    "    test = test.sort_values(by=['skey', 'date', 'ordering'])\n",
    "    test = test[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"cum_volume\", \"cum_amount\", \n",
    "                 \"open\", \"close\"]]\n",
    "    # change to second level tick data\n",
    "    k1 = test.groupby(['date', 'skey'])['datetime'].min().reset_index()\n",
    "    k1 = k1.rename(columns={'datetime':'min'})\n",
    "    k2 = test.groupby(['date', 'skey'])['datetime'].max().reset_index()\n",
    "    k2 = k2.rename(columns={'datetime':'max'})\n",
    "    k = pd.merge(k1, k2, on=['date', 'skey'])\n",
    "    k['diff'] = (k['max']-k['min']).apply(lambda x: x.seconds)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for i in np.arange(k.shape[0]):\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['datetime1'] = [k.loc[i, 'min'] + datetime.timedelta(seconds=int(x)) for x in np.arange(0, k.loc[i, 'diff'] + 1)]\n",
    "        df1['skey'] = k.loc[i, 'skey']\n",
    "        df1['date'] = k.loc[i, 'date']\n",
    "        assert(df1['datetime1'].min() == k.loc[i, 'min'])\n",
    "        assert(df1['datetime1'].max() == k.loc[i, 'max'])\n",
    "        df = pd.concat([df, df1])\n",
    "    test = pd.merge(test, df, left_on=['skey', 'datetime', 'date'], right_on=['skey', 'datetime1', 'date'], how='outer').sort_values(by=['skey', 'date', 'datetime1']).reset_index(drop=True)\n",
    "    assert(test[test['datetime1'].isnull()].shape[0] == 0)\n",
    "    for cols in ['cum_volume', 'cum_amount', 'open', 'close']:\n",
    "        test[cols] = test.groupby(['skey', 'date'])[cols].ffill()\n",
    "    test.drop([\"datetime\"],axis=1,inplace=True)\n",
    "    test = test.rename(columns={'datetime1':'datetime'})\n",
    "    test['skey'] = test['skey'].astype('int32')\n",
    "    test[\"time\"] = test['datetime'].astype(str).apply(lambda x: int(x.split(' ')[1].replace(':', \"\"))).astype(np.int64)\n",
    "    test['SendingTime'] = test['date'] * 1000000 + test['time']\n",
    "    test[\"clockAtArrival\"] = test[\"SendingTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S').timestamp()*1e6))\n",
    "    test.drop([\"SendingTime\"],axis=1,inplace=True)\n",
    "    test['time'] = test['time'] * 1000000\n",
    "\n",
    "    assert(sum(test[test[\"open\"] != 0].groupby([\"skey\", 'date'])[\"open\"].nunique() != 1) == 0)\n",
    "    test[\"open\"] = np.where(test[\"cum_volume\"] > 0, test.groupby([\"skey\", 'date'])[\"open\"].transform(\"max\"), test[\"open\"])\n",
    "    assert(sum(test[test[\"open\"] != 0].groupby([\"skey\", 'date'])[\"open\"].nunique() != 1) == 0)\n",
    "    assert(test[test[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "\n",
    "    test['date'] = test['date'].astype('int32')\n",
    "    test['cum_volume'] = test['cum_volume'].astype('int64')\n",
    "\n",
    "    m_in = test[test['time'] <= 113500000000].groupby('skey').last()['time'].min()\n",
    "    m_ax = test[test['time'] >= 125500000000].groupby('skey').first()['time'].max()\n",
    "    assert(test[(test['time'] >= m_in) & (test['time'] <= m_ax)].drop_duplicates(['cum_volume', 'open',  \n",
    "                                               'close', 'cum_amount', 'skey', 'date'], keep=False).shape[0] == 0\n",
    "           & (sum(test[(test['time'] >= m_in) & (test['time'] <= m_ax)].groupby('skey')['cum_volume'].nunique() != 1) == 0) & \n",
    "           (sum(test[(test['time'] >= m_in) & (test['time'] <= m_ax)].groupby('skey')['close'].nunique() != 1) == 0))\n",
    "    test = pd.concat([test[test['time'] <= 113500000000], test[test['time'] >= 125500000000]])\n",
    "\n",
    "    test = test.sort_values(by=[\"skey\", 'date', 'time'])\n",
    "    test[\"ordering\"] = test.groupby([\"skey\", 'date']).cumcount() + 1\n",
    "    test['ordering'] = test['ordering'].astype('int32')\n",
    "\n",
    "    for cols in ['open', 'cum_amount', 'close']:\n",
    "        test[cols] = test[cols].apply(lambda x: round(x, 4)).astype('float64')\n",
    "    assert(test['time'].max() < 150500000000)\n",
    "\n",
    "    test = test[[\"skey\", \"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ordering\", \"cum_volume\", \"cum_amount\", \n",
    "             \"open\", \"close\"]]\n",
    "\n",
    "    print(\"index finished\")\n",
    "\n",
    "    database_name = 'com_md_eq_cn'\n",
    "    user = \"zhenyuy\"\n",
    "    password = \"bnONBrzSMGoE\"\n",
    "\n",
    "    db1 = DB(\"192.168.10.178\", database_name, user, password)\n",
    "    db1.write('md_index', test)\n",
    "\n",
    "    del test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
