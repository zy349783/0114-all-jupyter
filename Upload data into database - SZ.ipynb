{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import gzip\n",
    "import lzma\n",
    "import pytz\n",
    "\n",
    "class DB(object):\n",
    "    def __init__(self, uri, symbol_column='ID', clock_column='clockAtArrival'):\n",
    "        self.db_name = 'white_db'\n",
    "        user, passwd, host = self.parse_uri(uri)\n",
    "        auth_db = 'admin' if user in ('admin', 'root') else self.db_name\n",
    "        self.uri = 'mongodb://%s:%s@%s/?authSource=%s' % (user, passwd, host, auth_db)\n",
    "\n",
    "        self.client = pymongo.MongoClient(self.uri)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.chunk_size = 20000\n",
    "        self.symbol_column = symbol_column\n",
    "        self.clock_column = clock_column\n",
    "        self.date_column = 'date'\n",
    "\n",
    "    def parse_uri(self, uri):\n",
    "        # mongodb://user:password@example.com\n",
    "        return uri.strip().replace('mongodb://', '').strip('/').replace(':', ' ').replace('@', ' ').split(' ')\n",
    "\n",
    "    def drop_table(self, table_name):\n",
    "        self.db.drop_collection(table_name)\n",
    "\n",
    "    def write(self, table_name, df):\n",
    "        if len(df) == 0: return\n",
    "\n",
    "        if self.clock_column in df.columns:\n",
    "            date = datetime.datetime.fromtimestamp(df.head(1)[self.clock_column].iloc[0] / 1e6, pytz.timezone('Asia/Shanghai')).strftime('%Y%m%d')\n",
    "        elif self.date_column in df.columns:\n",
    "            date = str(df.head(1)[self.date_column].iloc[0])\n",
    "        else:\n",
    "            raise Exception('DataFrame should contain either one of columns: `%s`, `%s`' % (self.clock_column, self.date_column))\n",
    "\n",
    "        collection = self.db[table_name]\n",
    "        collection.create_index([('date', pymongo.ASCENDING), ('symbol', pymongo.ASCENDING)], background=True)\n",
    "        collection.create_index([('symbol', pymongo.ASCENDING), ('date', pymongo.ASCENDING)], background=True)\n",
    "\n",
    "        for symbol, sub_df in df.groupby([self.symbol_column]):\n",
    "            collection.delete_many({'date': date, 'symbol': symbol})\n",
    "            self.write_single(collection, date, symbol, sub_df)\n",
    "\n",
    "    def write_single(self, collection, date, symbol, df):\n",
    "        for start in range(0, len(df), self.chunk_size):\n",
    "            end = min(start + self.chunk_size, len(df))\n",
    "            df_seg = df[start:end]\n",
    "            version = 1\n",
    "            seg = {'ver': version, 'data': self.ser(df_seg, version), 'date': date, 'symbol': symbol, 'start': start}\n",
    "            collection.insert_one(seg)\n",
    "\n",
    "    def build_query(self, start_date=None, end_date=None, symbol=None):\n",
    "        query = {}\n",
    "\n",
    "        def parse_date(x):\n",
    "            if type(x) == str:\n",
    "                if len(x) != 8:\n",
    "                    raise Exception(\"`date` must be YYYYMMDD format\")\n",
    "                return x\n",
    "            elif type(x) == datetime.datetime or type(x) == datetime.date:\n",
    "                return x.strftime(\"%Y%m%d\")\n",
    "            elif type(x) == int:\n",
    "                return parse_date(str(x))\n",
    "            else:\n",
    "                raise Exception(\"invalid `date` type: \" + str(type(x)))\n",
    "\n",
    "        if start_date and end_date:\n",
    "            query['date'] = {'$gte': parse_date(start_date), '$lte': parse_date(end_date)}\n",
    "\n",
    "        def parse_symbol(x):\n",
    "            if type(x) == int:\n",
    "                return x\n",
    "            else:\n",
    "                return int(x)\n",
    "\n",
    "        if symbol:\n",
    "            if type(symbol) == list or type(symbol) == tuple:\n",
    "                query['symbol'] = {'$in': [parse_symbol(x) for x in symbol]}\n",
    "            else:\n",
    "                query['symbol'] = parse_symbol(symbol)\n",
    "\n",
    "        return query\n",
    "\n",
    "    def delete(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot delete the whole table')\n",
    "            return None\n",
    "\n",
    "        collection.delete_many(query)\n",
    "\n",
    "    def read(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "\n",
    "        query = self.build_query(start_date, end_date, symbol)\n",
    "        if not query:\n",
    "            print('cannot read the whole table')\n",
    "            return None\n",
    "\n",
    "        segs = []\n",
    "        for x in collection.find(query):\n",
    "            x['data'] = self.deser(x['data'], x['ver'])\n",
    "            segs.append(x)\n",
    "        segs.sort(key=lambda x: (x['symbol'], x['date'], x['start']))\n",
    "        return pd.concat(x['data'] for x in segs) if segs else None\n",
    "\n",
    "    def list_tables(self):\n",
    "        return self.db.collection_names()\n",
    "\n",
    "    def list_dates(self, table_name, start_date=None, end_date=None, symbol=None):\n",
    "        collection = self.db[table_name]\n",
    "        dates = set()\n",
    "        if start_date is None:\n",
    "            start_date = '00000000'\n",
    "        if end_date is None:\n",
    "            end_date = '99999999'\n",
    "        for x in collection.find(self.build_query(start_date, end_date, symbol), {\"date\": 1, '_id': 0}):\n",
    "            dates.add(x['date'])\n",
    "        return sorted(list(dates))\n",
    "\n",
    "    def ser(self, s, version):\n",
    "        if version == 1:\n",
    "            return gzip.compress(pickle.dumps(s), compresslevel=2)\n",
    "        elif version == 2:\n",
    "            return lzma.compress(pickle.dumps(s), preset=1)\n",
    "        else:\n",
    "            raise Exception('unknown version')\n",
    "\n",
    "    def deser(self, s, version):\n",
    "        def unpickle(s):\n",
    "            return pickle.loads(s)\n",
    "\n",
    "        if version == 1:\n",
    "            return unpickle(gzip.decompress(s))\n",
    "        elif version == 2:\n",
    "            return unpickle(lzma.decompress(s))\n",
    "        else:\n",
    "            raise Exception('unknown version')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. upload SZ 2018 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am & pm data 未分卷\n",
      "20181228\n",
      "90003000000\n",
      "0:07:39.709732\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from unrar import rarfile\n",
    "import py7zr\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "columns1 = [\"Date\",\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",\"PreClosePx\",\n",
    "                   \"PxChnage1\",\"PXChange2\",\"openPrice\",\"HighPx\",\"LowPx\",\"close\",\"NumTrades\",\"cum_volume\",\"cum_amount\",\"PE1\",\"PE2\",\"TradingPhase\",\n",
    "                   \"totalofferqty\", \"vwapAsk\", \"totalbidqty\", \"vwapBid\", \"PreNAV\", \"RealTimeNAV\", \"WarrantPremiumRate\", \"UpLimitPx\",\n",
    "                   \"DownLimitPx\", \"TotalLongPosition\", \"unknown1\", \"unknown2\", \"unknown3\"]\n",
    "columns2 = ['Date',\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",'ask1p','bid1p',\n",
    "                   \"ask1q\",\"bid1q\", 'ask2p','bid2p',\"ask2q\",\"bid2q\",'ask3p','bid3p',\"ask3q\",\"bid3q\",'ask4p','bid4p',\"ask4q\",\"bid4q\",'ask5p',\n",
    "                    'bid5p',\"ask5q\",\"bid5q\",'ask6p','bid6p',\"ask6q\",\"bid6q\",'ask7p','bid7p',\"ask7q\",\"bid7q\",'ask8p','bid8p',\"ask8q\",\"bid8q\",\n",
    "                   'ask9p','bid9p',\"ask9q\",\"bid9q\",'ask10p','bid10p',\"ask10q\",\"bid10q\",\"bid1n\",\"NOORDERS_B1\",\"ORDERQTY_B1\",\n",
    "                    \"ask1n\",\"NOORDERS_S1\",\"ORDERQTY_S1\"]\n",
    "columns3 = [\"Date\", \"SENDTIME\", \"SecurityID\", \"DATATIMESTAMP\", \"PreClosePx\", \"openPrice\", \"HighPx\", \"LowPx\", \"close\", \"NumTrades\", \"cum_volume\",\n",
    "           \"cum_amount\", \"TOTALLONGPOSITION\", \"PERATIO1\", \"PERATIO2\", \"ENDOFDAYMAKER\", \"totalofferqty\", \"vwapAsk\", \"totalbidqty\", \"vwapBid\"]\n",
    "columns4 = [\"Date\", \"SENDTIME\", \"SecurityID\", \"DATATIMESTAMP\", \"ask10p\", \"ask9p\", \"ask8p\", \"ask7p\", \"ask6p\", \"ask5p\", \"ask4p\", \"ask3p\", \"ask2p\",\n",
    "           \"ask1p\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid6p\", \"bid7p\", \"bid8p\", \"bid9p\", \"bid10p\", \"ask10q\", \"ask9q\", \"ask8q\", \"ask7q\",\n",
    "           \"ask6q\", \"ask5q\", \"ask4q\", \"ask3q\", \"ask2q\", \"ask1q\", \"bid1q\", \"bid2q\", \"bid3q\", \"bid4q\", \"bid5q\", \"bid6q\", \"bid7q\", \"bid8q\", \"bid9q\", \n",
    "           \"bid10q\", \"bid1n\", \"NOORDERS _B1\", \"ORDERQTY_B1\", \"ask1n\", \"NOORDERS _S1\", \"ORDERQTY_S1\"]\n",
    "columns5 =  [\"Date\",\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",\"PreClosePx\",\n",
    "                   \"PxChnage1\",\"PXChange2\",\"openPrice\",\"HighPx\",\"LowPx\",\"close\",\"NumTrades\",\"cum_volume\",\"cum_amount\",\"PE1\",\"PE2\",\"TradingPhase\",\n",
    "                   \"totalofferqty\", \"vwapAsk\", \"totalbidqty\", \"vwapBid\", \"PreNAV\", \"RealTimeNAV\", \"WarrantPremiumRate\", \"UpLimitPx\",\n",
    "                   \"DownLimitPx\", \"TotalLongPosition\"]\n",
    "\n",
    "\n",
    "year = \"2018\"\n",
    "df = []\n",
    "bad = []\n",
    "readPath = 'J:\\\\LEVEL2_shenzhen\\\\' + year + '\\\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "for data in dataPathLs[-1:]:\n",
    "    if len(np.array(glob.glob(data +'\\\\***'))) == 0:\n",
    "        continue\n",
    "        \n",
    "    # am & pm  \n",
    "    startTm = datetime.datetime.now()\n",
    "    if len(np.array(glob.glob(data +'\\\\am_snap_level_spot.7z'))) == 1:\n",
    "        date = os.path.basename(data)\n",
    "        path = r'F:\\SZ\\2018' \n",
    "        os.chdir(data)\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\am_snap_level_spot.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\am_snap_level_spot.7z')\n",
    "            bad.append(data + '\\\\am_snap_level_spot.7z')\n",
    "            continue\n",
    "        path1 = path + '\\\\' + date\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\pm_snap_level_spot.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\pm_snap_level_spot.7z')\n",
    "            bad.append(data + '\\\\pm_snap_level_spot.7z')\n",
    "            continue\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        \n",
    "        if len(np.array(glob.glob(path1 +'\\\\***_hq_snap_spot.txt'))) != 2:\n",
    "            print(\"Less data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            bad.append(data + '\\\\***_hq_snap_spot.txt')\n",
    "            print(np.array(glob.glob(path1 +'\\\\***'))[0])\n",
    "            continue\n",
    "        am_snap1 = pd.read_table(path1 + \"\\\\am_hq_snap_spot.txt\", header=None)\n",
    "        try:\n",
    "            assert(am_snap1.shape[1] == len(columns1))\n",
    "            am_snap1.columns = columns1       \n",
    "        except:\n",
    "            assert(am_snap1.shape[1] == len(columns5))\n",
    "            am_snap1.columns = columns5     \n",
    "        pm_snap1 = pd.read_table(path1 + \"\\\\pm_hq_snap_spot.txt\", header=None)\n",
    "        try:\n",
    "            assert(pm_snap1.shape[1] == len(columns1))\n",
    "            pm_snap1.columns = columns1       \n",
    "        except:\n",
    "            assert(pm_snap1.shape[1] == len(columns5))\n",
    "            pm_snap1.columns = columns5     \n",
    "        snapshot1 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "        del am_snap1\n",
    "        del pm_snap1\n",
    "        \n",
    "        am_snap1 = pd.read_table(path1 + \"\\\\am_snap_level_spot.txt\", header=None)\n",
    "        assert(am_snap1.shape[1] == len(columns2))\n",
    "        am_snap1.columns = columns2       \n",
    "        pm_snap1 = pd.read_table(path1 + \"\\\\pm_snap_level_spot.txt\", header=None)\n",
    "        assert(pm_snap1.shape[1] == len(columns2))\n",
    "        pm_snap1.columns = columns2       \n",
    "        snapshot2 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "        del am_snap1\n",
    "        del pm_snap1\n",
    "        \n",
    "        snapshot1 = snapshot1[(snapshot1[\"SecurityID\"] < 4000) | (snapshot1[\"SecurityID\"] > 300000)]\n",
    "        snapshot2 = snapshot2[(snapshot2[\"SecurityID\"] < 4000) | (snapshot2[\"SecurityID\"] > 300000)]\n",
    "        snapshot1['time'] = (snapshot1['OrigTime'] - int(snapshot1['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "        snapshot2['time'] = (snapshot2['OrigTime'] - int(snapshot2['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "        snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"], how=\"outer\")\n",
    "        try:\n",
    "            assert((snapshot.shape[0] == snapshot1.shape[0]) & (snapshot.shape[0] == snapshot2.shape[0]))\n",
    "        except:\n",
    "            if snapshot.shape[0] == snapshot1.shape[0]:\n",
    "                print(\"snapshot1 have more ticks than snapshot2\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            if snapshot.shape[0] == snapshot2.shape[0]:\n",
    "                print(\"snapshot2 have more ticks than snapshot1\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            else:\n",
    "                print(\"snapshot2 don't join with snapshot1\")\n",
    "            snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"])\n",
    "        del snapshot1\n",
    "        del snapshot2\n",
    "           \n",
    "        snapshot = snapshot.rename(columns={\"SecurityID\":\"StockID\"})\n",
    "        \n",
    "        print(\"am & pm data 未分卷\")\n",
    "        \n",
    "    elif len(np.array(glob.glob(data +'\\\\am_snap_level_spot.7z.001'))) == 1:    \n",
    "        date = os.path.basename(data)\n",
    "        path = r'F:\\SZ\\2018' \n",
    "        os.chdir(data)\n",
    "        os.system(\"copy /b am_snap_level_spot.7z.* am_snap_level_spot.7z\")\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\am_snap_level_spot.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\am_snap_level_spot.7z')\n",
    "            bad.append(data + '\\\\am_snap_level_spot.7z')\n",
    "            continue\n",
    "        path1 = path + '\\\\' + date\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        \n",
    "        os.system(\"copy /b pm_snap_level_spot.7z.* pm_snap_level_spot.7z\")\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\pm_snap_level_spot.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\pm_snap_level_spot.7z')\n",
    "            bad.append(data + '\\\\pm_snap_level_spot.7z')\n",
    "            continue\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        \n",
    "        if len(np.array(glob.glob(path1 +'\\\\***_hq_snap_spot.txt'))) != 2:\n",
    "            print(\"Less data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            bad.append(data + '\\\\***_hq_snap_spot.txt')\n",
    "            print(np.array(glob.glob(path1 +'\\\\***'))[0])\n",
    "            continue\n",
    "        am_snap1 = pd.read_table(path1 + \"\\\\am_hq_snap_spot.txt\", header=None)\n",
    "        try:\n",
    "            assert(am_snap1.shape[1] == len(columns1))\n",
    "            am_snap1.columns = columns1       \n",
    "        except:\n",
    "            assert(am_snap1.shape[1] == len(columns5))\n",
    "            am_snap1.columns = columns5       \n",
    "        pm_snap1 = pd.read_table(path1 + \"\\\\pm_hq_snap_spot.txt\", header=None)\n",
    "        try:\n",
    "            assert(pm_snap1.shape[1] == len(columns1))\n",
    "            pm_snap1.columns = columns1       \n",
    "        except:\n",
    "            assert(pm_snap1.shape[1] == len(columns5))\n",
    "            pm_snap1.columns = columns5     \n",
    "        snapshot1 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "        del am_snap1\n",
    "        del pm_snap1\n",
    "        \n",
    "        am_snap1 = pd.read_table(path1 + \"\\\\am_snap_level_spot.txt\", header=None)\n",
    "        assert(am_snap1.shape[1] == len(columns2))\n",
    "        am_snap1.columns = columns2        \n",
    "        pm_snap1 = pd.read_table(path1 + \"\\\\pm_snap_level_spot.txt\", header=None)\n",
    "        assert(pm_snap1.shape[1] == len(columns2))\n",
    "        pm_snap1.columns = columns2       \n",
    "        snapshot2 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "        del am_snap1\n",
    "        del pm_snap1\n",
    "        \n",
    "        snapshot1 = snapshot1[(snapshot1[\"SecurityID\"] < 4000) | (snapshot1[\"SecurityID\"] > 300000)]\n",
    "        snapshot2 = snapshot2[(snapshot2[\"SecurityID\"] < 4000) | (snapshot2[\"SecurityID\"] > 300000)]\n",
    "        snapshot1['time'] = (snapshot1['OrigTime'] - int(snapshot1['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "        snapshot2['time'] = (snapshot2['OrigTime'] - int(snapshot2['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "        snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"], how=\"outer\")\n",
    "        try:\n",
    "            assert((snapshot.shape[0] == snapshot1.shape[0]) & (snapshot.shape[0] == snapshot2.shape[0]))\n",
    "        except:\n",
    "            if snapshot.shape[0] == snapshot1.shape[0]:\n",
    "                print(\"snapshot1 have more ticks than snapshot2\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            if snapshot.shape[0] == snapshot2.shape[0]:\n",
    "                print(\"snapshot2 have more ticks than snapshot1\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            else:\n",
    "                print(\"snapshot2 don't join with snapshot1\")\n",
    "            snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"])\n",
    "        del snapshot1\n",
    "        del snapshot2\n",
    "        \n",
    "        snapshot = snapshot.rename(columns={\"SecurityID\":\"StockID\"})\n",
    "        \n",
    "        print(\"am & pm data 分卷\") \n",
    "   \n",
    "\n",
    "    elif len(np.array(glob.glob(data +'\\\\snap_level.7z'))) == 1: \n",
    "        date = os.path.basename(data)\n",
    "        path = r'F:\\SZ\\2018' \n",
    "        os.chdir(data)\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\snap_level.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\snap_level.7z')\n",
    "            bad.append(data + '\\\\snap_level.7z')\n",
    "            continue\n",
    "        path1 = path + '\\\\' + date\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        \n",
    "        if len(np.array(glob.glob(path1 +'\\\\hq_snap.txt'))) != 1:\n",
    "            print(\"Less data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            bad.append(data + '\\\\hq_snap.txt')\n",
    "            print(np.array(glob.glob(path1 +'\\\\***'))[0])\n",
    "            continue\n",
    "        snapshot1 = pd.read_table(path1 + \"\\\\hq_snap.txt\", header=None, encoding=\"UTF-8-sig\")\n",
    "        try:\n",
    "            assert(snapshot1.shape[1] == len(columns1))\n",
    "            snapshot1.columns = columns1\n",
    "        except:\n",
    "            assert(snapshot1.shape[1] == len(columns5))\n",
    "            snapshot1.columns = columns5\n",
    "        \n",
    "        snapshot1[\"SecurityID\"] = snapshot1[\"SecurityID\"].astype(int)\n",
    "        snapshot2[\"SecurityID\"] = snapshot2[\"SecurityID\"].astype(int)\n",
    "        snapshot1 = snapshot1[(snapshot1[\"SecurityID\"] < 4000) | (snapshot1[\"SecurityID\"] > 300000)]\n",
    "        snapshot2 = snapshot2[(snapshot2[\"SecurityID\"] < 4000) | (snapshot2[\"SecurityID\"] > 300000)]\n",
    "        snapshot1['time'] = (snapshot1['OrigTime'] - int(snapshot1['OrigTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)\n",
    "        snapshot2['time'] = (snapshot2['OrigTime'] - int(snapshot2['OrigTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)\n",
    "        \n",
    "        snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"], how=\"outer\")\n",
    "        try:\n",
    "            assert((snapshot.shape[0] == snapshot1.shape[0]) & (snapshot.shape[0] == snapshot2.shape[0]))\n",
    "        except:\n",
    "            if snapshot.shape[0] == snapshot1.shape[0]:\n",
    "                print(\"snapshot1 have more ticks than snapshot2\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            if snapshot.shape[0] == snapshot2.shape[0]:\n",
    "                print(\"snapshot2 have more ticks than snapshot1\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            else:\n",
    "                print(\"snapshot2 don't join with snapshot1\")\n",
    "            snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"])\n",
    "        del snapshot1\n",
    "        del snapshot2\n",
    "        \n",
    "        snapshot = snapshot.rename(columns={\"SecurityID\":\"StockID\"})\n",
    "        \n",
    "        print(\"深交所数据\")\n",
    "        \n",
    "    \n",
    "    snapshot[\"clockAtArrival\"] = snapshot[\"OrigTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    snapshot = snapshot.rename(columns={\"Date\":\"date\", \"NumTrades\":\"cum_tradesCnt\", \"HighPx\":\"high\", \"LowPx\":\"low\", \"totalofferqty\":\n",
    "                                   \"totalAskQuantity\", \"totalbidqty\":\"totalBidQuantity\", \"PreClosePx\":\"prevClose\", \"openPrice\":\"open\"})\n",
    "    snapshot[\"ID\"] = snapshot[\"StockID\"] + 2000000\n",
    "    snapshot[\"ordering\"] = snapshot.groupby(\"ID\").cumcount()\n",
    "    snapshot[\"ordering\"] = snapshot[\"ordering\"] + 1\n",
    "    snapshot[\"time\"] = snapshot[\"time\"].astype('int64') * 1000\n",
    "    \n",
    "\n",
    "    snapshot.loc[(~snapshot[\"ORDERQTY_B1\"].isnull()), \"ORDERQTY_B1\"]=snapshot[(~snapshot[\"ORDERQTY_B1\"].isnull())][\"ORDERQTY_B1\"].apply(lambda x: [int(i) for i in x.split('|')])\n",
    "    snapshot.loc[(~snapshot[\"ORDERQTY_S1\"].isnull()), \"ORDERQTY_S1\"]=snapshot[(~snapshot[\"ORDERQTY_S1\"].isnull())][\"ORDERQTY_S1\"].apply(lambda x: [int(i) for i in x.split('|')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        snapshot[\"bid1Top\" + str(i) + 'q'] = 0\n",
    "        snapshot[\"ask1Top\" + str(i) + 'q'] = 0\n",
    "    for i in range(1, 51):\n",
    "        snapshot.loc[i <= snapshot[\"bid1n\"], \"bid1Top\" + str(i) + 'q'] = snapshot.loc[i <= snapshot[\"bid1n\"], \"ORDERQTY_B1\"].apply(lambda x: x[i-1])\n",
    "        snapshot.loc[i <= snapshot[\"ask1n\"], \"ask1Top\" + str(i) + 'q'] = snapshot.loc[i <= snapshot[\"ask1n\"], \"ORDERQTY_S1\"].apply(lambda x: x[i-1])    \n",
    "\n",
    "    \n",
    "    \n",
    "    for columns in [\"cum_tradesCnt\", \"cum_volume\", 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q','ask7q','ask8q','ask9q',\n",
    "                        'ask10q', 'bid1q','bid2q','bid3q','bid4q','bid5q','bid6q','bid7q','bid8q','bid9q','bid10q',\n",
    "                    \"totalBidQuantity\", \"totalAskQuantity\", \"bid1n\", \"ask1n\"]:\n",
    "        snapshot[columns] = snapshot[columns].astype('int64')\n",
    "\n",
    "    \n",
    "    \n",
    "    for cols in [\"prevClose\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "             'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'vwapBid', \"vwapAsk\"]:\n",
    "        snapshot[cols] = (snapshot[cols] * 10000).round(0).astype('int64') # 'int64'\n",
    "  \n",
    "\n",
    "    for cols in [\"cum_amount\"]:\n",
    "        snapshot[cols] = (snapshot[cols] * 10000).round(0).astype('int64')\n",
    "    \n",
    "    snapshot['datetime'] = snapshot[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "\n",
    "\n",
    "\n",
    "    for cols in ['bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', \n",
    "                 'ask8n', 'ask9n', 'ask10n', \"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]:\n",
    "        snapshot[cols] = 0\n",
    "    \n",
    "    snapshot[\"prevClose\"] = np.where(snapshot[\"time\"] >= 91500000000, snapshot.groupby(\"ID\")[\"prevClose\"].transform(\"max\"), snapshot[\"prevClose\"]) \n",
    "    snapshot[\"open\"] = np.where(snapshot[\"cum_volume\"] > 0, snapshot.groupby(\"ID\")[\"open\"].transform(\"max\"), snapshot[\"open\"])\n",
    "    assert(sum(snapshot[snapshot[\"open\"] != 0].groupby(\"ID\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(snapshot[snapshot[\"prevClose\"] != 0].groupby(\"ID\")[\"prevClose\"].nunique() != 1) == 0)\n",
    "    assert(snapshot[snapshot[\"time\"] >= 91500000000][\"prevClose\"].min() > 0)\n",
    "    assert(snapshot[snapshot[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "        \n",
    "    snapshot = snapshot[[\"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ID\", \"ordering\", \"cum_tradesCnt\", \"cum_volume\", \"cum_amount\", \"prevClose\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"totalBidQuantity\", \"totalAskQuantity\",\"vwapBid\", \"vwapAsk\",\n",
    "        \"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]]\n",
    "    \n",
    "    print(snapshot[\"date\"].iloc[0])\n",
    "    print(snapshot.groupby(\"ID\")[\"time\"].min().max())\n",
    "    \n",
    "    db = DB(\"mongodb://user_rw:faa96dfc@192.168.10.223\")\n",
    "    db.write('snapshot', snapshot)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "print(bad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. upload SZ 2019 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E:\\\\SZ\\\\2019\\\\0314(深交所数据)'], dtype='<U22')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from unrar import rarfile\n",
    "import py7zr\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "year = \"2019\"\n",
    "df = []\n",
    "bad = []\n",
    "readPath = 'E:\\\\SZ\\\\' + year + '\\\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = np.delete(dataPathLs, np.argwhere(dataPathLs == 'E:\\\\SZ\\\\2019\\\\0109(深交所数据)')[0])\n",
    "dataPathLs = np.delete(dataPathLs, np.argwhere(dataPathLs == 'E:\\\\SZ\\\\2019\\\\0314')[0])\n",
    "dataPathLs = np.delete(dataPathLs, np.argwhere(dataPathLs == 'E:\\\\SZ\\\\2019\\\\0315(深交所数据)')[0])\n",
    "dataPathLs[46:47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "20190314\n",
      "92503000000\n",
      "0:11:28.531866\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from unrar import rarfile\n",
    "import py7zr\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "year = \"2019\"\n",
    "df = []\n",
    "bad = []\n",
    "readPath = 'E:\\\\SZ\\\\' + year + '\\\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = np.delete(dataPathLs, np.argwhere(dataPathLs == 'E:\\\\SZ\\\\2019\\\\0109(深交所数据)')[0])\n",
    "dataPathLs = np.delete(dataPathLs, np.argwhere(dataPathLs == 'E:\\\\SZ\\\\2019\\\\0314')[0])\n",
    "dataPathLs = np.delete(dataPathLs, np.argwhere(dataPathLs == 'E:\\\\SZ\\\\2019\\\\0315(深交所数据)')[0])\n",
    "\n",
    "for data in dataPathLs[46:47]:\n",
    "    \n",
    "    if len(np.array(glob.glob(data +'\\\\***.txt'))) != 4:\n",
    "        print(\"Less data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(data + '\\\\Snapshot.pkl')\n",
    "        bad.append(data + '\\\\Snapshot.pkl')\n",
    "        continue\n",
    "    \n",
    "    print('---------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    if os.path.basename(data) == \"0610\":\n",
    "        print(\"skip 20190610\")\n",
    "        continue\n",
    "    \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    F1 = open(data + \"\\\\Snapshot.pkl\", 'rb')\n",
    "    snapshot = pickle.load(F1)\n",
    "    snapshot['time'] = (snapshot['OrigTime'] - int(snapshot[\"OrigTime\"].iloc[0]//1000000000 * 1000000000)).astype(int)\n",
    "    snapshot = snapshot[(snapshot[\"StockID\"] < 4000) | (snapshot[\"StockID\"] > 300000)]\n",
    " \n",
    "    \n",
    "    snapshot[\"clockAtArrival\"] = snapshot[\"OrigTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    snapshot = snapshot.rename(columns={\"Date\":\"date\", \"NumTrades\":\"cum_tradesCnt\", \"HighPx\":\"high\", \"LowPx\":\"low\", \"totalofferqty\":\n",
    "                                   \"totalAskQuantity\", \"totalbidqty\":\"totalBidQuantity\", \"NUMORDERS_B1\":\"bid1n\", \"NUMORDERS_S1\":\"ask1n\",\n",
    "                                    \"wa_offerPrice\": \"vwapAsk\", \"wa_bidPrice\":\"vwapBid\", \"PreClosePx\":\"prevClose\", \"openPrice\":\"open\"})\n",
    "    snapshot[\"ID\"] = snapshot[\"StockID\"] + 2000000\n",
    "    snapshot[\"time\"] = snapshot[\"time\"].astype(np.int64) * 1000\n",
    "    \n",
    "    snapshot.loc[(~snapshot[\"ORDERQTY_B1\"].isnull()), \"ORDERQTY_B1\"]=snapshot[(~snapshot[\"ORDERQTY_B1\"].isnull())][\"ORDERQTY_B1\"].apply(lambda x: [int(i) for i in x.split('|')])\n",
    "    snapshot.loc[(~snapshot[\"ORDERQTY_S1\"].isnull()), \"ORDERQTY_S1\"]=snapshot[(~snapshot[\"ORDERQTY_S1\"].isnull())][\"ORDERQTY_S1\"].apply(lambda x: [int(i) for i in x.split('|')])\n",
    "    ## lambda x: [int(i) for i in x.split('|')]\n",
    "    for i in range(1, 51):\n",
    "        snapshot[\"bid1Top\" + str(i) + 'q'] = 0\n",
    "        snapshot[\"ask1Top\" + str(i) + 'q'] = 0\n",
    "    for i in range(1, 51):\n",
    "        snapshot.loc[i <= snapshot[\"bid1n\"], \"bid1Top\" + str(i) + 'q'] = snapshot.loc[i <= snapshot[\"bid1n\"], \"ORDERQTY_B1\"].apply(lambda x: x[i-1])\n",
    "        snapshot.loc[i <= snapshot[\"ask1n\"], \"ask1Top\" + str(i) + 'q'] = snapshot.loc[i <= snapshot[\"ask1n\"], \"ORDERQTY_S1\"].apply(lambda x: x[i-1])    \n",
    "\n",
    "\n",
    "    snapshot = snapshot.fillna(0)\n",
    "    snapshot = snapshot[~((snapshot[\"bid1p\"] == 0) & (snapshot[\"ask1p\"] == 0))]\n",
    "    snapshot[\"ordering\"] = snapshot.groupby(\"ID\").cumcount()\n",
    "    snapshot[\"ordering\"] = snapshot[\"ordering\"] + 1\n",
    "    \n",
    "    \n",
    "    for columns in [\"cum_tradesCnt\", \"cum_volume\", 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q','ask7q','ask8q','ask9q',\n",
    "                        'ask10q', 'bid1q','bid2q','bid3q','bid4q','bid5q','bid6q','bid7q','bid8q','bid9q','bid10q',\n",
    "                    \"totalBidQuantity\", \"totalAskQuantity\", \"bid1n\", \"ask1n\"]:\n",
    "        snapshot[columns] = snapshot[columns].astype('int64')\n",
    "\n",
    "\n",
    "    for cols in [\"prevClose\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "             'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'vwapBid', \"vwapAsk\"]:\n",
    "        snapshot[cols] = (snapshot[cols] * 10000).round(0).astype('int64') \n",
    "    for cols in [\"cum_amount\"]:\n",
    "        snapshot[cols] = (snapshot[cols] * 10000).round(0).astype('int64')\n",
    "    snapshot['datetime'] = snapshot[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    for cols in ['bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', \n",
    "                 'ask8n', 'ask9n', 'ask10n', \"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]:\n",
    "        snapshot[cols] = 0\n",
    "    \n",
    "    assert(sum(snapshot[snapshot[\"open\"] != 0].groupby(\"ID\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(snapshot[snapshot[\"prevClose\"] != 0].groupby(\"ID\")[\"prevClose\"].nunique() != 1) == 0)\n",
    "    snapshot[\"prevClose\"] = np.where(snapshot[\"time\"] >= 91500000000, snapshot.groupby(\"ID\")[\"prevClose\"].transform(\"max\"), snapshot[\"prevClose\"]) \n",
    "    snapshot[\"open\"] = np.where(snapshot[\"cum_volume\"] > 0, snapshot.groupby(\"ID\")[\"open\"].transform(\"max\"), snapshot[\"open\"])\n",
    "    assert(sum(snapshot[snapshot[\"open\"] != 0].groupby(\"ID\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(snapshot[snapshot[\"prevClose\"] != 0].groupby(\"ID\")[\"prevClose\"].nunique() != 1) == 0)\n",
    "    assert(snapshot[snapshot[\"time\"] >= 91500000000][\"prevClose\"].min() > 0)\n",
    "    assert(snapshot[snapshot[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "    snapshot = snapshot[[\"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ID\", \"ordering\", \"cum_tradesCnt\", \"cum_volume\", \"cum_amount\", \"prevClose\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"totalBidQuantity\", \"totalAskQuantity\",\"vwapBid\", \"vwapAsk\",\n",
    "        \"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]]\n",
    "    \n",
    "    print(snapshot[\"date\"].iloc[0])\n",
    "    print(snapshot.groupby(\"ID\")[\"time\"].min().max())\n",
    "    \n",
    "    db = DB(\"mongodb://user_rw:faa96dfc@192.168.10.223\")\n",
    "    db.write('snapshot', snapshot)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "print(bad)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-02 09:30:18 000000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.fromtimestamp(1577928618000000/1e6).strftime(\"%Y-%m-%d %H:%M:%S %f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. upload SZ 20190610 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish load data\n",
      "0:07:00.356552\n",
      "20190610\n",
      "92503000000\n",
      "0:15:26.115033\n"
     ]
    }
   ],
   "source": [
    "y = '20190610'\n",
    "\n",
    "readPath = '\\\\\\\\192.168.10.30\\\\Kevin_zhenyu\\\\temp\\\\kuanrui\\\\szse_20190610_csv\\\\szse_' + y + '\\\\snap\\\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dateLs = np.array([int(os.path.basename(i).split('.')[0]) for i in dataPathLs])\n",
    "dataPathLs = dataPathLs[(dateLs < 4000) | ((dateLs > 300000) & (dateLs < 310000))]\n",
    "logSZ1 = []\n",
    "startTm = datetime.datetime.now()\n",
    "for i in dataPathLs:\n",
    "    df = pd.read_csv(i)\n",
    "    df[\"StockID\"] = int(os.path.basename(i).split('.')[0])\n",
    "    logSZ1 += [df]\n",
    "del df\n",
    "logSZ1 = pd.concat(logSZ1).reset_index(drop=True)\n",
    "print(\"finish load data\")\n",
    "print(datetime.datetime.now() - startTm)\n",
    "logSZ1 = logSZ1.rename(columns={\"成交总量\":\"cum_volume\", \"成交总金额\":\"cum_amount\", \"最新价\":\"close\", \"开始价\":\"open\",\n",
    "                               \"昨收价\":\"prevClose\", \"成交笔数\":\"cum_tradesCnt\", \"最高价\":\"high\", \"最低价\":\"low\", \n",
    "                               \"买入总量\":\"totalBidQuantity\", \"卖出总量\":\"totalAskQuantity\", \"委买加权平均价\":\"vwapBid\",\n",
    "                               \"委卖加权平均价\":\"vwapAsk\"})\n",
    "for i in range(1, 11):\n",
    "    logSZ1 = logSZ1.rename(columns={\"申买价\" + str(i): \"bid\" + str(i) + \"p\", \"申卖价\" + str(i): \"ask\" + str(i) + \"p\",\n",
    "                                   \"申买量\" + str(i): \"bid\" + str(i) + \"q\", \"申卖量\" + str(i): \"ask\" + str(i) + \"q\",\n",
    "                                   \"申买价位总委托笔数\" + str(i): \"bid\" + str(i) + \"n\", \"申卖价位总委托笔数\" + str(i): \"ask\" + str(i) + \"n\"})\n",
    "for i in range(1, 51):\n",
    "    logSZ1 = logSZ1.rename(columns={\"买一价前50笔订单\" + str(i): \"bid1Top\" + str(i) + \"q\", \n",
    "                                    \"卖一价前50笔订单\" + str(i): \"ask1Top\" + str(i) + \"q\"})\n",
    "logSZ1[\"time\"] = ((logSZ1[\"日期时间\"] - 20190610000000000)*1000).astype(\"int64\")\n",
    "logSZ1[\"date\"] = logSZ1[\"日期时间\"]//1000000000\n",
    "logSZ1[\"clockAtArrival\"] = logSZ1[\"日期时间\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "logSZ1[\"ID\"] = logSZ1[\"StockID\"] + 2000000\n",
    "    \n",
    "logSZ1 = logSZ1.fillna(0)\n",
    "logSZ1 = logSZ1[~((logSZ1[\"bid1p\"] == 0) & (logSZ1[\"ask1p\"] == 0))]\n",
    "logSZ1[\"ordering\"] = logSZ1.groupby(\"ID\").cumcount()\n",
    "logSZ1[\"ordering\"] = logSZ1[\"ordering\"] + 1\n",
    "\n",
    "for columns in [\"cum_tradesCnt\", \"cum_volume\", 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q','ask7q','ask8q','ask9q',\n",
    "                        'ask10q', 'bid1q','bid2q','bid3q','bid4q','bid5q','bid6q','bid7q','bid8q','bid9q','bid10q',\n",
    "                    \"totalBidQuantity\", \"totalAskQuantity\", 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n']:\n",
    "    logSZ1[columns] = logSZ1[columns].astype('int64')\n",
    "\n",
    "\n",
    "for cols in [\"prevClose\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "             'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'vwapBid', \"vwapAsk\"]:\n",
    "    logSZ1[cols] = (logSZ1[cols] * 10).astype('int64') \n",
    "for cols in [\"cum_amount\"]:\n",
    "    logSZ1[cols] = (logSZ1[cols] * 10).astype('int64')\n",
    "\n",
    "logSZ1['datetime'] = logSZ1[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "for cols in [\"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]:\n",
    "    logSZ1[cols] = 0\n",
    "\n",
    "    \n",
    "assert(sum(logSZ1[logSZ1[\"open\"] != 0].groupby(\"ID\")[\"open\"].nunique() != 1) == 0)\n",
    "assert(sum(logSZ1[logSZ1[\"prevClose\"] != 0].groupby(\"ID\")[\"prevClose\"].nunique() != 1) == 0)\n",
    "logSZ1[\"prevClose\"] = np.where(logSZ1[\"time\"] >= 91500000000, logSZ1.groupby(\"ID\")[\"prevClose\"].transform(\"max\"), logSZ1[\"prevClose\"]) \n",
    "logSZ1[\"open\"] = np.where(logSZ1[\"cum_volume\"] > 0, logSZ1.groupby(\"ID\")[\"open\"].transform(\"max\"), logSZ1[\"open\"])\n",
    "assert(sum(logSZ1[logSZ1[\"open\"] != 0].groupby(\"ID\")[\"open\"].nunique() != 1) == 0)\n",
    "assert(sum(logSZ1[logSZ1[\"prevClose\"] != 0].groupby(\"ID\")[\"prevClose\"].nunique() != 1) == 0)\n",
    "assert(logSZ1[logSZ1[\"time\"] >= 91500000000][\"prevClose\"].min() > 0)\n",
    "assert(logSZ1[logSZ1[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "logSZ1 = logSZ1[[\"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ID\", \"ordering\", \"cum_tradesCnt\", \"cum_volume\", \"cum_amount\", \"prevClose\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"totalBidQuantity\", \"totalAskQuantity\",\"vwapBid\", \"vwapAsk\",\n",
    "        \"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]]\n",
    "    \n",
    "print(logSZ1[\"date\"].iloc[0])\n",
    "print(logSZ1.groupby(\"ID\")[\"time\"].min().max())\n",
    "    \n",
    "db = DB(\"mongodb://user_rw:faa96dfc@192.168.10.223\")\n",
    "db.write('snapshot', logSZ1)\n",
    "print(datetime.datetime.now() - startTm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. upload SZ 2020 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am & pm data txt\n",
      "20200408\n",
      "92503000000\n",
      "0:08:50.904691\n",
      "am & pm data txt\n",
      "20200409\n",
      "92603000000\n",
      "0:08:42.617013\n",
      "am & pm data txt\n",
      "20200410\n",
      "92503000000\n",
      "0:08:55.426669\n",
      "am & pm data txt\n",
      "20200413\n",
      "92503000000\n",
      "0:07:53.871584\n",
      "am & pm data txt\n",
      "20200414\n",
      "92503000000\n",
      "0:08:23.667178\n",
      "am & pm data txt\n",
      "20200415\n",
      "92503000000\n",
      "0:08:39.927697\n",
      "am & pm data txt\n",
      "20200416\n",
      "92503000000\n",
      "0:08:34.254867\n",
      "am & pm data txt\n",
      "20200417\n",
      "92503000000\n",
      "0:09:21.884571\n",
      "am & pm data txt\n",
      "20200420\n",
      "92503000000\n",
      "0:08:34.978929\n",
      "am & pm data txt\n",
      "20200421\n",
      "92503000000\n",
      "0:08:48.597511\n",
      "am & pm data txt\n",
      "20200422\n",
      "92503000000\n",
      "0:08:38.780289\n",
      "am & pm data txt\n",
      "20200423\n",
      "92503000000\n",
      "0:08:41.442645\n",
      "am & pm data txt\n",
      "20200424\n",
      "92503000000\n",
      "0:08:49.585872\n",
      "am & pm data txt\n",
      "20200427\n",
      "92503000000\n",
      "0:08:18.297537\n",
      "am & pm data txt\n",
      "20200428\n",
      "92503000000\n",
      "0:08:34.640841\n",
      "am & pm data txt\n",
      "20200429\n",
      "92503000000\n",
      "0:13:23.752430\n",
      "am & pm data txt\n",
      "20200430\n",
      "92503000000\n",
      "0:15:34.215146\n",
      "am & pm data txt\n",
      "20200506\n",
      "92603000000\n",
      "0:16:16.290470\n",
      "am & pm data txt\n",
      "20200507\n",
      "92503000000\n",
      "0:08:37.673435\n",
      "am & pm data txt\n",
      "20200508\n",
      "92503000000\n",
      "0:09:36.293446\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from unrar import rarfile\n",
    "import py7zr\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "columns1 = [\"Date\",\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",\"PreClosePx\",\n",
    "                   \"PxChnage1\",\"PXChange2\",\"openPrice\",\"HighPx\",\"LowPx\",\"close\",\"NumTrades\",\"cum_volume\",\"cum_amount\",\"PE1\",\"PE2\",\"TradingPhase\",\n",
    "                   \"totalofferqty\", \"vwapAsk\", \"totalbidqty\", \"vwapBid\", \"PreNAV\", \"RealTimeNAV\", \"WarrantPremiumRate\", \"UpLimitPx\",\n",
    "                   \"DownLimitPx\", \"TotalLongPosition\", \"unknown1\", \"unknown2\", \"unknown3\"]\n",
    "columns2 = ['Date',\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",'ask1p','bid1p',\n",
    "                   \"ask1q\",\"bid1q\", 'ask2p','bid2p',\"ask2q\",\"bid2q\",'ask3p','bid3p',\"ask3q\",\"bid3q\",'ask4p','bid4p',\"ask4q\",\"bid4q\",'ask5p',\n",
    "                    'bid5p',\"ask5q\",\"bid5q\",'ask6p','bid6p',\"ask6q\",\"bid6q\",'ask7p','bid7p',\"ask7q\",\"bid7q\",'ask8p','bid8p',\"ask8q\",\"bid8q\",\n",
    "                   'ask9p','bid9p',\"ask9q\",\"bid9q\",'ask10p','bid10p',\"ask10q\",\"bid10q\",\"bid1n\",\"NOORDERS_B1\",\"ORDERQTY_B1\",\n",
    "                    \"ask1n\",\"NOORDERS_S1\",\"ORDERQTY_S1\"]\n",
    "columns3 = [\"Date\", \"SENDTIME\", \"SecurityID\", \"DATATIMESTAMP\", \"PreClosePx\", \"openPrice\", \"HighPx\", \"LowPx\", \"close\", \"NumTrades\", \"cum_volume\",\n",
    "           \"cum_amount\", \"TOTALLONGPOSITION\", \"PERATIO1\", \"PERATIO2\", \"ENDOFDAYMAKER\", \"totalofferqty\", \"vwapAsk\", \"totalbidqty\", \"vwapBid\"]\n",
    "columns4 = [\"Date\", \"SENDTIME\", \"SecurityID\", \"DATATIMESTAMP\", \"ask10p\", \"ask9p\", \"ask8p\", \"ask7p\", \"ask6p\", \"ask5p\", \"ask4p\", \"ask3p\", \"ask2p\",\n",
    "           \"ask1p\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid6p\", \"bid7p\", \"bid8p\", \"bid9p\", \"bid10p\", \"ask10q\", \"ask9q\", \"ask8q\", \"ask7q\",\n",
    "           \"ask6q\", \"ask5q\", \"ask4q\", \"ask3q\", \"ask2q\", \"ask1q\", \"bid1q\", \"bid2q\", \"bid3q\", \"bid4q\", \"bid5q\", \"bid6q\", \"bid7q\", \"bid8q\", \"bid9q\", \n",
    "           \"bid10q\", \"bid1n\", \"NOORDERS _B1\", \"ORDERQTY_B1\", \"ask1n\", \"NOORDERS _S1\", \"ORDERQTY_S1\"]\n",
    "columns5 =  [\"Date\",\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",\"PreClosePx\",\n",
    "                   \"PxChnage1\",\"PXChange2\",\"openPrice\",\"HighPx\",\"LowPx\",\"close\",\"NumTrades\",\"cum_volume\",\"cum_amount\",\"PE1\",\"PE2\",\"TradingPhase\",\n",
    "                   \"totalofferqty\", \"vwapAsk\", \"totalbidqty\", \"vwapBid\", \"PreNAV\", \"RealTimeNAV\", \"WarrantPremiumRate\", \"UpLimitPx\",\n",
    "                   \"DownLimitPx\", \"TotalLongPosition\"]\n",
    "\n",
    "df = []\n",
    "bad = []\n",
    "readPath = 'L:\\\\2020 data\\\\SZ\\\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "for data in dataPathLs[62:]:\n",
    "    if len(np.array(glob.glob(data +'\\\\***'))) == 0:\n",
    "        continue\n",
    "        \n",
    "    # am & pm  \n",
    "    startTm = datetime.datetime.now()\n",
    "    \n",
    "    if len(np.array(glob.glob(data +'\\\\***_hq_snap_spot.txt'))) != 2:\n",
    "            print(\"Less hq_snap_spot data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            bad.append(data + '\\\\***_hq_snap_spot.txt')\n",
    "            print(np.array(glob.glob(data +'\\\\***'))[0])\n",
    "            continue\n",
    "    \n",
    "    if len(np.array(glob.glob(data +'\\\\***_snap_level_spot.txt'))) != 2:\n",
    "            print(\"Less snap_level_spot data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            bad.append(data + '\\\\***_snap_level_spot.txt')\n",
    "            print(np.array(glob.glob(data +'\\\\***'))[0])\n",
    "            continue\n",
    "            \n",
    "\n",
    "    am_snap1 = pd.read_table(data + \"\\\\am_hq_snap_spot.txt\", header=None)\n",
    "    try:\n",
    "        assert(am_snap1.shape[1] == len(columns1))\n",
    "        am_snap1.columns = columns1       \n",
    "    except:\n",
    "        assert(am_snap1.shape[1] == len(columns5))\n",
    "        am_snap1.columns = columns5     \n",
    "    pm_snap1 = pd.read_table(data + \"\\\\pm_hq_snap_spot.txt\", header=None)\n",
    "    try:\n",
    "        assert(pm_snap1.shape[1] == len(columns1))\n",
    "        pm_snap1.columns = columns1       \n",
    "    except:\n",
    "        assert(pm_snap1.shape[1] == len(columns5))\n",
    "        pm_snap1.columns = columns5     \n",
    "    snapshot1 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "    del am_snap1\n",
    "    del pm_snap1\n",
    "        \n",
    "    am_snap1 = pd.read_table(data + \"\\\\am_snap_level_spot.txt\", header=None)\n",
    "    assert(am_snap1.shape[1] == len(columns2))\n",
    "    am_snap1.columns = columns2       \n",
    "    pm_snap1 = pd.read_table(data + \"\\\\pm_snap_level_spot.txt\", header=None)\n",
    "    assert(pm_snap1.shape[1] == len(columns2))\n",
    "    pm_snap1.columns = columns2       \n",
    "    snapshot2 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "    del am_snap1\n",
    "    del pm_snap1\n",
    "        \n",
    "    snapshot1 = snapshot1[(snapshot1[\"SecurityID\"] < 4000) | (snapshot1[\"SecurityID\"] > 300000)]\n",
    "    snapshot2 = snapshot2[(snapshot2[\"SecurityID\"] < 4000) | (snapshot2[\"SecurityID\"] > 300000)]\n",
    "    snapshot1['time'] = (snapshot1['OrigTime'] - int(snapshot1['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "    snapshot2['time'] = (snapshot2['OrigTime'] - int(snapshot2['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "    snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"], how=\"outer\")\n",
    "    try:\n",
    "        assert((snapshot.shape[0] == snapshot1.shape[0]) & (snapshot.shape[0] == snapshot2.shape[0]))\n",
    "    except:\n",
    "        if snapshot.shape[0] == snapshot1.shape[0]:\n",
    "            print(\"snapshot1 have more ticks than snapshot2\")\n",
    "            if all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                print(\"More ticks happens after 15:00\")\n",
    "            elif all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                print(\"More ticks happens before 9:30\")\n",
    "            else:\n",
    "                print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "        elif snapshot.shape[0] == snapshot2.shape[0]:\n",
    "            print(\"snapshot2 have more ticks than snapshot1\")\n",
    "            if all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                print(\"More ticks happens after 15:00\")\n",
    "            elif all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                print(\"More ticks happens before 9:30\")\n",
    "            else:\n",
    "                print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "        else:\n",
    "            print(\"snapshot2 don't join with snapshot1\")\n",
    "        snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"])\n",
    "    del snapshot1\n",
    "    del snapshot2\n",
    "           \n",
    "    snapshot = snapshot.rename(columns={\"SecurityID\":\"StockID\"})\n",
    "        \n",
    "    print(\"am & pm data txt\")\n",
    "        \n",
    "    \n",
    "    snapshot[\"clockAtArrival\"] = snapshot[\"OrigTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    snapshot = snapshot.rename(columns={\"Date\":\"date\", \"NumTrades\":\"cum_tradesCnt\", \"HighPx\":\"high\", \"LowPx\":\"low\", \"totalofferqty\":\n",
    "                                   \"totalAskQuantity\", \"totalbidqty\":\"totalBidQuantity\", \"PreClosePx\":\"prevClose\", \"openPrice\":\"open\"})\n",
    "    snapshot[\"ID\"] = snapshot[\"StockID\"] + 2000000\n",
    "    snapshot[\"time\"] = snapshot[\"time\"].astype('int64') * 1000\n",
    "    \n",
    "\n",
    "    snapshot.loc[(~snapshot[\"ORDERQTY_B1\"].isnull()), \"ORDERQTY_B1\"]=snapshot[(~snapshot[\"ORDERQTY_B1\"].isnull())][\"ORDERQTY_B1\"].apply(lambda x: [int(i) for i in x.split('|')])\n",
    "    snapshot.loc[(~snapshot[\"ORDERQTY_S1\"].isnull()), \"ORDERQTY_S1\"]=snapshot[(~snapshot[\"ORDERQTY_S1\"].isnull())][\"ORDERQTY_S1\"].apply(lambda x: [int(i) for i in x.split('|')])\n",
    "\n",
    "    for i in range(1, 51):\n",
    "        snapshot[\"bid1Top\" + str(i) + 'q'] = 0\n",
    "        snapshot[\"ask1Top\" + str(i) + 'q'] = 0\n",
    "    for i in range(1, 51):\n",
    "        snapshot.loc[i <= snapshot[\"bid1n\"], \"bid1Top\" + str(i) + 'q'] = snapshot.loc[i <= snapshot[\"bid1n\"], \"ORDERQTY_B1\"].apply(lambda x: x[i-1])\n",
    "        snapshot.loc[i <= snapshot[\"ask1n\"], \"ask1Top\" + str(i) + 'q'] = snapshot.loc[i <= snapshot[\"ask1n\"], \"ORDERQTY_S1\"].apply(lambda x: x[i-1])    \n",
    "\n",
    "    snapshot = snapshot.fillna(0)\n",
    "    snapshot = snapshot[~((snapshot[\"bid1p\"] == 0) & (snapshot[\"ask1p\"] == 0))]\n",
    "    snapshot[\"ordering\"] = snapshot.groupby(\"ID\").cumcount()\n",
    "    snapshot[\"ordering\"] = snapshot[\"ordering\"] + 1\n",
    "    \n",
    "    for columns in [\"cum_tradesCnt\", \"cum_volume\", 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q','ask7q','ask8q','ask9q',\n",
    "                        'ask10q', 'bid1q','bid2q','bid3q','bid4q','bid5q','bid6q','bid7q','bid8q','bid9q','bid10q',\n",
    "                    \"totalBidQuantity\", \"totalAskQuantity\", \"bid1n\", \"ask1n\"]:\n",
    "        snapshot[columns] = snapshot[columns].astype('int64')\n",
    "\n",
    "    \n",
    "    \n",
    "    for cols in [\"prevClose\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "             'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'vwapBid', \"vwapAsk\"]:\n",
    "        snapshot[cols] = (snapshot[cols] * 10000).round(0).astype('int64') # 'int64'\n",
    "  \n",
    "\n",
    "    for cols in [\"cum_amount\"]:\n",
    "        snapshot[cols] = (snapshot[cols] * 10000).round(0).astype('int64')\n",
    "    \n",
    "    snapshot['datetime'] = snapshot[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "\n",
    "\n",
    "\n",
    "    for cols in ['bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', \n",
    "                 'ask8n', 'ask9n', 'ask10n', \"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]:\n",
    "        snapshot[cols] = 0\n",
    "    \n",
    "    assert(sum(snapshot[snapshot[\"open\"] != 0].groupby(\"ID\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(snapshot[snapshot[\"prevClose\"] != 0].groupby(\"ID\")[\"prevClose\"].nunique() != 1) == 0)\n",
    "    snapshot[\"prevClose\"] = np.where(snapshot[\"time\"] >= 91500000000, snapshot.groupby(\"ID\")[\"prevClose\"].transform(\"max\"), snapshot[\"prevClose\"]) \n",
    "    snapshot[\"open\"] = np.where(snapshot[\"cum_volume\"] > 0, snapshot.groupby(\"ID\")[\"open\"].transform(\"max\"), snapshot[\"open\"])\n",
    "    assert(sum(snapshot[snapshot[\"open\"] != 0].groupby(\"ID\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(snapshot[snapshot[\"prevClose\"] != 0].groupby(\"ID\")[\"prevClose\"].nunique() != 1) == 0)\n",
    "    assert(snapshot[snapshot[\"time\"] >= 91500000000][\"prevClose\"].min() > 0)\n",
    "    assert(snapshot[snapshot[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "        \n",
    "    snapshot = snapshot[[\"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ID\", \"ordering\", \"cum_tradesCnt\", \"cum_volume\", \"cum_amount\", \"prevClose\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"totalBidQuantity\", \"totalAskQuantity\",\"vwapBid\", \"vwapAsk\",\n",
    "        \"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]]\n",
    "    \n",
    "    print(snapshot[\"date\"].iloc[0])\n",
    "    print(snapshot.groupby(\"ID\")[\"time\"].min().max())\n",
    "    \n",
    "    db = DB(\"mongodb://user_rw:faa96dfc@192.168.10.223\")\n",
    "    db.write('snapshot', snapshot)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "print(bad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. upload SZ 2017 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (51,54) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am & pm data 未分卷\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from unrar import rarfile\n",
    "import py7zr\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "\n",
    "columns1 = [\"Date\",\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",\"PreClosePx\",\n",
    "                   \"PxChnage1\",\"PXChange2\",\"openPrice\",\"HighPx\",\"LowPx\",\"close\",\"NumTrades\",\"cum_volume\",\"cum_amount\",\"PE1\",\"PE2\",\"TradingPhase\",\n",
    "                   \"totalofferqty\", \"vwapAsk\", \"totalbidqty\", \"vwapBid\", \"PreNAV\", \"RealTimeNAV\", \"WarrantPremiumRate\", \"UpLimitPx\",\n",
    "                   \"DownLimitPx\", \"TotalLongPosition\", \"unknown1\", \"unknown2\", \"unknown3\"]\n",
    "columns2 = ['Date',\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",'ask1p','bid1p',\n",
    "                   \"ask1q\",\"bid1q\", 'ask2p','bid2p',\"ask2q\",\"bid2q\",'ask3p','bid3p',\"ask3q\",\"bid3q\",'ask4p','bid4p',\"ask4q\",\"bid4q\",'ask5p',\n",
    "                    'bid5p',\"ask5q\",\"bid5q\",'ask6p','bid6p',\"ask6q\",\"bid6q\",'ask7p','bid7p',\"ask7q\",\"bid7q\",'ask8p','bid8p',\"ask8q\",\"bid8q\",\n",
    "                   'ask9p','bid9p',\"ask9q\",\"bid9q\",'ask10p','bid10p',\"ask10q\",\"bid10q\",\"bid1n\",\"NOORDERS_B1\",\"ORDERQTY_B1\",\n",
    "                    \"ask1n\",\"NOORDERS_S1\",\"ORDERQTY_S1\"]\n",
    "columns3 = [\"Date\", \"SENDTIME\", \"SecurityID\", \"DATATIMESTAMP\", \"PreClosePx\", \"openPrice\", \"HighPx\", \"LowPx\", \"close\", \"NumTrades\", \"cum_volume\",\n",
    "           \"cum_amount\", \"TOTALLONGPOSITION\", \"PERATIO1\", \"PERATIO2\", \"ENDOFDAYMAKER\", \"totalofferqty\", \"vwapAsk\", \"totalbidqty\", \"vwapBid\"]\n",
    "columns4 = [\"Date\", \"SENDTIME\", \"SecurityID\", \"DATATIMESTAMP\", \"ask10p\", \"ask9p\", \"ask8p\", \"ask7p\", \"ask6p\", \"ask5p\", \"ask4p\", \"ask3p\", \"ask2p\",\n",
    "           \"ask1p\", \"bid1p\", \"bid2p\", \"bid3p\", \"bid4p\", \"bid5p\", \"bid6p\", \"bid7p\", \"bid8p\", \"bid9p\", \"bid10p\", \"ask10q\", \"ask9q\", \"ask8q\", \"ask7q\",\n",
    "           \"ask6q\", \"ask5q\", \"ask4q\", \"ask3q\", \"ask2q\", \"ask1q\", \"bid1q\", \"bid2q\", \"bid3q\", \"bid4q\", \"bid5q\", \"bid6q\", \"bid7q\", \"bid8q\", \"bid9q\", \n",
    "           \"bid10q\", \"bid1n\", \"NOORDERS _B1\", \"ORDERQTY_B1\", \"ask1n\", \"NOORDERS _S1\", \"ORDERQTY_S1\"]\n",
    "columns5 =  [\"Date\",\"OrigTime\",\"SendTime\",\"ercvtime\",\"dbtime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\",\"PreClosePx\",\n",
    "                   \"PxChnage1\",\"PXChange2\",\"openPrice\",\"HighPx\",\"LowPx\",\"close\",\"NumTrades\",\"cum_volume\",\"cum_amount\",\"PE1\",\"PE2\",\"TradingPhase\",\n",
    "                   \"totalofferqty\", \"vwapAsk\", \"totalbidqty\", \"vwapBid\", \"PreNAV\", \"RealTimeNAV\", \"WarrantPremiumRate\", \"UpLimitPx\",\n",
    "                   \"DownLimitPx\", \"TotalLongPosition\"]\n",
    "\n",
    "year = \"2017\"\n",
    "df = []\n",
    "bad = []\n",
    "readPath = 'J:\\\\LEVEL2_shenzhen\\\\' + year + '\\\\***'\n",
    "dataPathLs = np.array(glob.glob(readPath))\n",
    "dataPathLs = np.delete(dataPathLs, np.argwhere(dataPathLs == 'J:\\\\LEVEL2_shenzhen\\\\2017\\\\0620(深交所数据)')[0])\n",
    "dataPathLs = np.delete(dataPathLs, np.argwhere(dataPathLs == 'J:\\\\LEVEL2_shenzhen\\\\2017\\\\0622(深交所数据)')[0])\n",
    "\n",
    "for data in dataPathLs[196:]:\n",
    "    \n",
    "    if len(np.array(glob.glob(data +'\\\\***'))) == 0:\n",
    "        continue\n",
    "        \n",
    "    # am & pm  \n",
    "    startTm = datetime.datetime.now()\n",
    "    if len(np.array(glob.glob(data +'\\\\pm_snap_level_spot.7z'))) == 1:\n",
    "        date = os.path.basename(data)\n",
    "        path = r'F:\\SZ\\2017' \n",
    "        os.chdir(data)\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\am_snap_level_spot.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\am_snap_level_spot.7z')\n",
    "            bad.append(data + '\\\\am_snap_level_spot.7z')\n",
    "            continue\n",
    "        path1 = path + '\\\\' + date\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\pm_snap_level_spot.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\pm_snap_level_spot.7z')\n",
    "            bad.append(data + '\\\\pm_snap_level_spot.7z')\n",
    "            continue\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        \n",
    "        if len(np.array(glob.glob(path1 +'\\\\***_hq_snap_spot.txt'))) != 2:\n",
    "            print(\"Less data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            bad.append(data + '\\\\***_hq_snap_spot.txt')\n",
    "            print(np.array(glob.glob(path1 +'\\\\***'))[0])\n",
    "            continue\n",
    "        am_snap1 = pd.read_table(path1 + \"\\\\am_hq_snap_spot.txt\", header=None)\n",
    "        try:\n",
    "            assert(am_snap1.shape[1] == len(columns1))\n",
    "            am_snap1.columns = columns1       \n",
    "        except:\n",
    "            assert(am_snap1.shape[1] == len(columns5))\n",
    "            am_snap1.columns = columns5     \n",
    "        pm_snap1 = pd.read_table(path1 + \"\\\\pm_hq_snap_spot.txt\", header=None)\n",
    "        try:\n",
    "            assert(pm_snap1.shape[1] == len(columns1))\n",
    "            pm_snap1.columns = columns1       \n",
    "        except:\n",
    "            assert(pm_snap1.shape[1] == len(columns5))\n",
    "            pm_snap1.columns = columns5     \n",
    "        snapshot1 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "        del am_snap1\n",
    "        del pm_snap1\n",
    "        \n",
    "        am_snap1 = pd.read_table(path1 + \"\\\\am_snap_level_spot.txt\", header=None)\n",
    "        assert(am_snap1.shape[1] == len(columns2))\n",
    "        am_snap1.columns = columns2       \n",
    "        pm_snap1 = pd.read_table(path1 + \"\\\\pm_snap_level_spot.txt\", header=None)\n",
    "        assert(pm_snap1.shape[1] == len(columns2))\n",
    "        pm_snap1.columns = columns2       \n",
    "        snapshot2 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "        del am_snap1\n",
    "        del pm_snap1\n",
    "        \n",
    "        snapshot1 = snapshot1[(snapshot1[\"SecurityID\"] < 4000) | (snapshot1[\"SecurityID\"] > 300000)]\n",
    "        snapshot2 = snapshot2[(snapshot2[\"SecurityID\"] < 4000) | (snapshot2[\"SecurityID\"] > 300000)]\n",
    "        snapshot1['time'] = (snapshot1['OrigTime'] - int(snapshot1['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "        snapshot2['time'] = (snapshot2['OrigTime'] - int(snapshot2['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "        snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"], how=\"outer\")\n",
    "        try:\n",
    "            assert((snapshot.shape[0] == snapshot1.shape[0]) & (snapshot.shape[0] == snapshot2.shape[0]))\n",
    "        except:\n",
    "            if snapshot.shape[0] == snapshot1.shape[0]:\n",
    "                print(\"snapshot1 have more ticks than snapshot2\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            if snapshot.shape[0] == snapshot2.shape[0]:\n",
    "                print(\"snapshot2 have more ticks than snapshot1\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            else:\n",
    "                print(\"snapshot2 don't join with snapshot1\")\n",
    "            snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"])\n",
    "        del snapshot1\n",
    "        del snapshot2\n",
    "           \n",
    "        snapshot = snapshot.rename(columns={\"SecurityID\":\"StockID\"})\n",
    "        \n",
    "        print(\"am & pm data 未分卷\")\n",
    "        \n",
    "    elif len(np.array(glob.glob(data +'\\\\am_snap_level_spot.7z.001'))) == 1:    \n",
    "        date = os.path.basename(data)\n",
    "        path = r'F:\\SZ\\2017' \n",
    "        os.chdir(data)\n",
    "        os.system(\"copy /b am_snap_level_spot.7z.* am_snap_level_spot.7z\")\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\am_snap_level_spot.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\am_snap_level_spot.7z')\n",
    "            bad.append(data + '\\\\am_snap_level_spot.7z')\n",
    "            continue\n",
    "        path1 = path + '\\\\' + date\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        \n",
    "        os.system(\"copy /b pm_snap_level_spot.7z.* pm_snap_level_spot.7z\")\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\pm_snap_level_spot.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\pm_snap_level_spot.7z')\n",
    "            bad.append(data + '\\\\pm_snap_level_spot.7z')\n",
    "            continue\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        \n",
    "        if len(np.array(glob.glob(path1 +'\\\\***_hq_snap_spot.txt'))) != 2:\n",
    "            print(\"Less data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            bad.append(data + '\\\\***_hq_snap_spot.txt')\n",
    "            print(np.array(glob.glob(path1 +'\\\\***'))[0])\n",
    "            continue\n",
    "        am_snap1 = pd.read_table(path1 + \"\\\\am_hq_snap_spot.txt\", header=None)\n",
    "        try:\n",
    "            assert(am_snap1.shape[1] == len(columns1))\n",
    "            am_snap1.columns = columns1       \n",
    "        except:\n",
    "            assert(am_snap1.shape[1] == len(columns5))\n",
    "            am_snap1.columns = columns5       \n",
    "        pm_snap1 = pd.read_table(path1 + \"\\\\pm_hq_snap_spot.txt\", header=None)\n",
    "        try:\n",
    "            assert(pm_snap1.shape[1] == len(columns1))\n",
    "            pm_snap1.columns = columns1       \n",
    "        except:\n",
    "            assert(pm_snap1.shape[1] == len(columns5))\n",
    "            pm_snap1.columns = columns5     \n",
    "        snapshot1 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "        del am_snap1\n",
    "        del pm_snap1\n",
    "        \n",
    "        am_snap1 = pd.read_table(path1 + \"\\\\am_snap_level_spot.txt\", header=None)\n",
    "        assert(am_snap1.shape[1] == len(columns2))\n",
    "        am_snap1.columns = columns2        \n",
    "        pm_snap1 = pd.read_table(path1 + \"\\\\pm_snap_level_spot.txt\", header=None)\n",
    "        assert(pm_snap1.shape[1] == len(columns2))\n",
    "        pm_snap1.columns = columns2       \n",
    "        snapshot2 = pd.concat([am_snap1, pm_snap1]).sort_values(by=[\"SecurityID\", \"OrigTime\"])\n",
    "        del am_snap1\n",
    "        del pm_snap1\n",
    "        \n",
    "        snapshot1 = snapshot1[(snapshot1[\"SecurityID\"] < 4000) | (snapshot1[\"SecurityID\"] > 300000)]\n",
    "        snapshot2 = snapshot2[(snapshot2[\"SecurityID\"] < 4000) | (snapshot2[\"SecurityID\"] > 300000)]\n",
    "        snapshot1['time'] = (snapshot1['OrigTime'] - int(snapshot1['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "        snapshot2['time'] = (snapshot2['OrigTime'] - int(snapshot2['OrigTime'].iloc[0]//1000000000*1000000000)).astype(int)\n",
    "        snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"], how=\"outer\")\n",
    "        try:\n",
    "            assert((snapshot.shape[0] == snapshot1.shape[0]) & (snapshot.shape[0] == snapshot2.shape[0]))\n",
    "        except:\n",
    "            if snapshot.shape[0] == snapshot1.shape[0]:\n",
    "                print(\"snapshot1 have more ticks than snapshot2\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            if snapshot.shape[0] == snapshot2.shape[0]:\n",
    "                print(\"snapshot2 have more ticks than snapshot1\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            else:\n",
    "                print(\"snapshot2 don't join with snapshot1\")\n",
    "            snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"])\n",
    "        del snapshot1\n",
    "        del snapshot2\n",
    "        \n",
    "        snapshot = snapshot.rename(columns={\"SecurityID\":\"StockID\"})\n",
    "        \n",
    "        print(\"am & pm data 分卷\") \n",
    "   \n",
    "\n",
    "    elif len(np.array(glob.glob(data +'\\\\snap_level.7z'))) == 1: \n",
    "        date = os.path.basename(data)\n",
    "        path = r'F:\\SZ\\2017' \n",
    "        os.chdir(data)\n",
    "        try:\n",
    "            a = py7zr.SevenZipFile(data + '\\\\snap_level.7z','r',filters=None)\n",
    "        except:\n",
    "            print(\"Bad unzip here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            print(data + '\\\\snap_level.7z')\n",
    "            bad.append(data + '\\\\snap_level.7z')\n",
    "            continue\n",
    "        path1 = path + '\\\\' + date\n",
    "        a.extractall(path = path1)\n",
    "        a.close()\n",
    "        \n",
    "        if len(np.array(glob.glob(path1 +'\\\\hq_snap.txt'))) != 1:\n",
    "            print(\"Less data!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            bad.append(data + '\\\\hq_snap.txt')\n",
    "            print(np.array(glob.glob(path1 +'\\\\***'))[0])\n",
    "            continue\n",
    "        snapshot1 = pd.read_table(path1 + \"\\\\hq_snap.txt\", header=None, encoding=\"UTF-8-sig\")\n",
    "        try:\n",
    "            assert(snapshot1.shape[1] == len(columns1))\n",
    "            snapshot1.columns = columns1\n",
    "        except:\n",
    "            assert(snapshot1.shape[1] == len(columns5))\n",
    "            snapshot1.columns = columns5\n",
    "        \n",
    "        snapshot1[\"SecurityID\"] = snapshot1[\"SecurityID\"].astype(int)\n",
    "        snapshot2[\"SecurityID\"] = snapshot2[\"SecurityID\"].astype(int)\n",
    "        snapshot1 = snapshot1[(snapshot1[\"SecurityID\"] < 4000) | (snapshot1[\"SecurityID\"] > 300000)]\n",
    "        snapshot2 = snapshot2[(snapshot2[\"SecurityID\"] < 4000) | (snapshot2[\"SecurityID\"] > 300000)]\n",
    "        snapshot1['time'] = (snapshot1['OrigTime'] - int(snapshot1['OrigTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)\n",
    "        snapshot2['time'] = (snapshot2['OrigTime'] - int(snapshot2['OrigTime'].iloc[0]//1000000000*1000000000)).astype(np.int64)\n",
    "        \n",
    "        snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"], how=\"outer\")\n",
    "        try:\n",
    "            assert((snapshot.shape[0] == snapshot1.shape[0]) & (snapshot.shape[0] == snapshot2.shape[0]))\n",
    "        except:\n",
    "            if snapshot.shape[0] == snapshot1.shape[0]:\n",
    "                print(\"snapshot1 have more ticks than snapshot2\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_y\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            if snapshot.shape[0] == snapshot2.shape[0]:\n",
    "                print(\"snapshot2 have more ticks than snapshot1\")\n",
    "                if all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() > 150000000):\n",
    "                    print(\"More ticks happens after 15:00\")\n",
    "                elif all(snapshot[(snapshot[\"dbtime_x\"].isnull()) & ((snapshot[\"SecurityID\"] < 4000) | (snapshot[\"SecurityID\"] > 200000))][\"time\"].unique() < 93000000):\n",
    "                    print(\"More ticks happens before 9:30\")\n",
    "                else:\n",
    "                    print(\"There are ticks happens before 15:00, after 9:30\")\n",
    "            else:\n",
    "                print(\"snapshot2 don't join with snapshot1\")\n",
    "            snapshot = pd.merge(snapshot1, snapshot2, on=['Date',\"OrigTime\",\"ChannelNo\",\"SecurityID\",\"SecurityIDsource\", \"MDStreamID\", \"time\"])\n",
    "        del snapshot1\n",
    "        del snapshot2\n",
    "        \n",
    "        snapshot = snapshot.rename(columns={\"SecurityID\":\"StockID\"})\n",
    "        \n",
    "        print(\"深交所数据\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Not inside!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    " \n",
    "    \n",
    "    snapshot[\"clockAtArrival\"] = snapshot[\"OrigTime\"].astype(str).apply(lambda x: np.int64(datetime.datetime.strptime(x, '%Y%m%d%H%M%S%f').timestamp()*1e6))\n",
    "    snapshot = snapshot.rename(columns={\"Date\":\"date\", \"NumTrades\":\"cum_tradesCnt\", \"HighPx\":\"high\", \"LowPx\":\"low\", \"totalofferqty\":\n",
    "                                   \"totalAskQuantity\", \"totalbidqty\":\"totalBidQuantity\", \"NUMORDERS_B1\":\"bid1n\", \"NUMORDERS_S1\":\"ask1n\",\n",
    "                                    \"wa_offerPrice\": \"vwapAsk\", \"wa_bidPrice\":\"vwapBid\", \"PreClosePx\":\"prevClose\", \"openPrice\":\"open\"})\n",
    "    snapshot[\"ID\"] = snapshot[\"StockID\"] + 2000000\n",
    "    snapshot[\"time\"] = snapshot[\"time\"].astype(np.int64) * 1000\n",
    "    \n",
    "    snapshot.loc[(~snapshot[\"ORDERQTY_B1\"].isnull()), \"ORDERQTY_B1\"]=snapshot[(~snapshot[\"ORDERQTY_B1\"].isnull())][\"ORDERQTY_B1\"].apply(lambda x: [int(i) for i in x.split('|')])\n",
    "    snapshot.loc[(~snapshot[\"ORDERQTY_S1\"].isnull()), \"ORDERQTY_S1\"]=snapshot[(~snapshot[\"ORDERQTY_S1\"].isnull())][\"ORDERQTY_S1\"].apply(lambda x: [int(i) for i in x.split('|')])\n",
    "    ## lambda x: [int(i) for i in x.split('|')]\n",
    "    for i in range(1, 51):\n",
    "        snapshot[\"bid1Top\" + str(i) + 'q'] = 0\n",
    "        snapshot[\"ask1Top\" + str(i) + 'q'] = 0\n",
    "    for i in range(1, 51):\n",
    "        snapshot.loc[i <= snapshot[\"bid1n\"], \"bid1Top\" + str(i) + 'q'] = snapshot.loc[i <= snapshot[\"bid1n\"], \"ORDERQTY_B1\"].apply(lambda x: x[i-1])\n",
    "        snapshot.loc[i <= snapshot[\"ask1n\"], \"ask1Top\" + str(i) + 'q'] = snapshot.loc[i <= snapshot[\"ask1n\"], \"ORDERQTY_S1\"].apply(lambda x: x[i-1])    \n",
    "\n",
    "\n",
    "    snapshot = snapshot.fillna(0)\n",
    "    snapshot = snapshot[~((snapshot[\"bid1p\"] == 0) & (snapshot[\"ask1p\"] == 0))]\n",
    "    snapshot[\"ordering\"] = snapshot.groupby(\"ID\").cumcount()\n",
    "    snapshot[\"ordering\"] = snapshot[\"ordering\"] + 1\n",
    "    \n",
    "    \n",
    "    for columns in [\"cum_tradesCnt\", \"cum_volume\", 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q','ask7q','ask8q','ask9q',\n",
    "                        'ask10q', 'bid1q','bid2q','bid3q','bid4q','bid5q','bid6q','bid7q','bid8q','bid9q','bid10q',\n",
    "                    \"totalBidQuantity\", \"totalAskQuantity\", \"bid1n\", \"ask1n\"]:\n",
    "        snapshot[columns] = snapshot[columns].astype('int64')\n",
    "\n",
    "\n",
    "    for cols in [\"prevClose\", 'open', \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p',\n",
    "             'bid2p','bid1p','ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'vwapBid', \"vwapAsk\"]:\n",
    "        snapshot[cols] = (snapshot[cols] * 10000).round(0).astype('int64') \n",
    "    for cols in [\"cum_amount\"]:\n",
    "        snapshot[cols] = (snapshot[cols] * 10000).round(0).astype('int64')\n",
    "    snapshot['datetime'] = snapshot[\"clockAtArrival\"].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    for cols in ['bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', \n",
    "                 'ask8n', 'ask9n', 'ask10n', \"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]:\n",
    "        snapshot[cols] = 0\n",
    "    \n",
    "    assert(sum(snapshot[snapshot[\"open\"] != 0].groupby(\"ID\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(snapshot[snapshot[\"prevClose\"] != 0].groupby(\"ID\")[\"prevClose\"].nunique() != 1) == 0)\n",
    "    snapshot[\"prevClose\"] = np.where(snapshot[\"time\"] >= 91500000000, snapshot.groupby(\"ID\")[\"prevClose\"].transform(\"max\"), snapshot[\"prevClose\"]) \n",
    "    snapshot[\"open\"] = np.where(snapshot[\"cum_volume\"] > 0, snapshot.groupby(\"ID\")[\"open\"].transform(\"max\"), snapshot[\"open\"])\n",
    "    assert(sum(snapshot[snapshot[\"open\"] != 0].groupby(\"ID\")[\"open\"].nunique() != 1) == 0)\n",
    "    assert(sum(snapshot[snapshot[\"prevClose\"] != 0].groupby(\"ID\")[\"prevClose\"].nunique() != 1) == 0)\n",
    "    assert(snapshot[snapshot[\"time\"] >= 91500000000][\"prevClose\"].min() > 0)\n",
    "    assert(snapshot[snapshot[\"cum_volume\"] > 0][\"open\"].min() > 0)\n",
    "    \n",
    "    snapshot = snapshot[[\"date\", \"time\", \"clockAtArrival\", \"datetime\", \"ID\", \"ordering\", \"cum_tradesCnt\", \"cum_volume\", \"cum_amount\", \"prevClose\",\n",
    "                            \"open\", \"high\", \"low\", \"close\", 'bid10p','bid9p','bid8p','bid7p','bid6p','bid5p','bid4p','bid3p','bid2p','bid1p',\n",
    "                            'ask1p','ask2p','ask3p','ask4p','ask5p','ask6p','ask7p','ask8p','ask9p','ask10p', 'bid10q','bid9q','bid8q',\n",
    "                             'bid7q','bid6q','bid5q','bid4q','bid3q','bid2q','bid1q', 'ask1q','ask2q','ask3q','ask4q','ask5q','ask6q',\n",
    "                             'ask7q','ask8q','ask9q','ask10q', 'bid10n', 'bid9n', 'bid8n', 'bid7n', 'bid6n', 'bid5n', 'bid4n', 'bid3n', 'bid2n', 'bid1n', \n",
    "                             'ask1n', 'ask2n', 'ask3n', 'ask4n', 'ask5n', 'ask6n','ask7n', 'ask8n', 'ask9n', 'ask10n','bid1Top1q','bid1Top2q','bid1Top3q','bid1Top4q','bid1Top5q','bid1Top6q',\n",
    "        'bid1Top7q','bid1Top8q','bid1Top9q','bid1Top10q','bid1Top11q','bid1Top12q','bid1Top13q','bid1Top14q','bid1Top15q','bid1Top16q','bid1Top17q','bid1Top18q',\n",
    "        'bid1Top19q','bid1Top20q','bid1Top21q','bid1Top22q','bid1Top23q','bid1Top24q','bid1Top25q','bid1Top26q','bid1Top27q','bid1Top28q','bid1Top29q',\n",
    "        'bid1Top30q','bid1Top31q','bid1Top32q','bid1Top33q','bid1Top34q','bid1Top35q','bid1Top36q','bid1Top37q','bid1Top38q','bid1Top39q','bid1Top40q',\n",
    "        'bid1Top41q','bid1Top42q','bid1Top43q','bid1Top44q','bid1Top45q','bid1Top46q','bid1Top47q','bid1Top48q','bid1Top49q','bid1Top50q', 'ask1Top1q',\n",
    "        'ask1Top2q','ask1Top3q','ask1Top4q','ask1Top5q','ask1Top6q','ask1Top7q','ask1Top8q','ask1Top9q','ask1Top10q','ask1Top11q','ask1Top12q','ask1Top13q',\n",
    "        'ask1Top14q','ask1Top15q','ask1Top16q','ask1Top17q','ask1Top18q','ask1Top19q','ask1Top20q','ask1Top21q','ask1Top22q','ask1Top23q',\n",
    "        'ask1Top24q','ask1Top25q','ask1Top26q','ask1Top27q','ask1Top28q','ask1Top29q','ask1Top30q','ask1Top31q','ask1Top32q','ask1Top33q',\n",
    "        'ask1Top34q','ask1Top35q','ask1Top36q','ask1Top37q','ask1Top38q','ask1Top39q','ask1Top40q','ask1Top41q','ask1Top42q','ask1Top43q',\n",
    "        'ask1Top44q','ask1Top45q','ask1Top46q','ask1Top47q','ask1Top48q','ask1Top49q','ask1Top50q',\"totalBidQuantity\", \"totalAskQuantity\",\"vwapBid\", \"vwapAsk\",\n",
    "        \"totalBidOrders\",'totalAskOrders','totalBidLevels', 'totalAskLevels', 'bidTradeMaxDuration', 'askTradeMaxDuration', 'cum_canceledBuyOrders', 'cum_canceledBuyVolume',\n",
    "        \"cum_canceledBuyAmount\", \"cum_canceledSellOrders\", 'cum_canceledSellVolume',\"cum_canceledSellAmount\"]]\n",
    "    \n",
    "    print(snapshot[\"date\"].iloc[0])\n",
    "    print(snapshot.groupby(\"ID\")[\"time\"].min().max())\n",
    "    \n",
    "    db = DB(\"mongodb://user_rw:faa96dfc@192.168.10.223\")\n",
    "    db.write('snapshot', snapshot)\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "\n",
    "print(bad)    \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
