{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-27 21:00:00.604791\n",
      "still wait for data coming\n",
      "2020-11-27 21:05:01.037309\n",
      "still wait for data coming\n",
      "2020-11-27 21:10:01.156316\n",
      "still wait for data coming\n",
      "2020-11-27 21:15:01.294805\n",
      "still wait for data coming\n",
      "2020-11-27 21:20:01.435141\n",
      "still wait for data coming\n",
      "2020-11-27 21:25:01.555215\n",
      "still wait for data coming\n",
      "2020-11-27 21:30:01.675671\n",
      "still wait for data coming\n",
      "2020-11-27 21:35:01.839680\n",
      "still wait for data coming\n",
      "2020-11-27 21:40:01.959731\n",
      "still wait for data coming\n",
      "2020-11-27 21:45:02.078798\n",
      "still wait for data coming\n",
      "2020-11-27 21:50:02.220207\n",
      "still wait for data coming\n",
      "2020-11-27 21:55:02.337800\n",
      "still wait for data coming\n",
      "2020-11-27 22:00:02.457190\n",
      "still wait for data coming\n",
      "2020-11-27 22:05:02.595754\n",
      "still wait for data coming\n",
      "2020-11-27 22:10:02.716726\n",
      "still wait for data coming\n",
      "2020-11-27 22:15:02.836429\n",
      "still wait for data coming\n",
      "2020-11-27 22:20:02.988926\n",
      "still wait for data coming\n",
      "2020-11-27 22:25:03.109625\n",
      "still wait for data coming\n",
      "2020-11-27 22:30:03.228978\n",
      "still wait for data coming\n",
      "2020-11-27 22:35:03.388460\n",
      "still wait for data coming\n",
      "2020-11-27 22:40:03.508265\n",
      "still wait for data coming\n",
      "2020-11-27 22:45:03.626859\n",
      "still wait for data coming\n",
      "2020-11-27 22:50:03.768539\n",
      "still wait for data coming\n",
      "2020-11-27 22:55:03.888466\n",
      "still wait for data coming\n",
      "2020-11-27 23:00:04.007166\n",
      "still wait for data coming\n",
      "2020-11-27 23:05:04.046080\n",
      "still wait for data coming\n",
      "2020-11-27 23:10:04.165782\n",
      "still wait for data coming\n",
      "2020-11-27 23:15:04.285501\n",
      "still wait for data coming\n",
      "2020-11-27 23:20:04.404290\n",
      "still wait for data coming\n",
      "2020-11-27 23:25:04.545901\n",
      "still wait for data coming\n",
      "2020-11-27 23:30:04.664592\n",
      "still wait for data coming\n",
      "2020-11-27 23:35:04.782294\n",
      "still wait for data coming\n",
      "2020-11-27 23:40:04.923941\n",
      "still wait for data coming\n",
      "2020-11-27 23:45:05.050620\n",
      "still wait for data coming\n",
      "2020-11-27 23:50:05.171315\n",
      "still wait for data coming\n",
      "2020-11-27 23:55:05.312946\n",
      "still wait for data coming\n",
      "2020-11-28 00:00:05.463567\n",
      "still wait for data coming\n",
      "2020-11-28 00:05:05.471557\n",
      "still wait for data coming\n",
      "2020-11-28 00:10:05.480550\n",
      "still wait for data coming\n",
      "2020-11-28 00:15:05.498520\n",
      "still wait for data coming\n",
      "2020-11-28 00:20:05.509508\n",
      "still wait for data coming\n",
      "2020-11-28 00:25:05.519505\n",
      "still wait for data coming\n",
      "2020-11-28 00:30:05.528498\n",
      "still wait for data coming\n",
      "2020-11-28 00:35:05.542471\n",
      "still wait for data coming\n",
      "2020-11-28 00:40:05.552463\n",
      "still wait for data coming\n",
      "2020-11-28 00:45:05.561455\n",
      "still wait for data coming\n",
      "2020-11-28 00:50:05.569452\n",
      "still wait for data coming\n",
      "2020-11-28 00:55:05.583434\n",
      "still wait for data coming\n",
      "2020-11-28 01:00:05.593429\n",
      "still wait for data coming\n",
      "2020-11-28 01:05:05.603412\n",
      "still wait for data coming\n",
      "2020-11-28 01:10:05.612407\n",
      "still wait for data coming\n",
      "2020-11-28 01:15:05.626389\n",
      "still wait for data coming\n",
      "2020-11-28 01:20:05.636384\n",
      "still wait for data coming\n",
      "2020-11-28 01:25:05.647372\n",
      "still wait for data coming\n",
      "2020-11-28 01:30:05.656366\n",
      "still wait for data coming\n",
      "2020-11-28 01:35:05.670347\n",
      "still wait for data coming\n",
      "2020-11-28 01:40:05.679334\n",
      "still wait for data coming\n",
      "2020-11-28 01:45:05.687334\n",
      "still wait for data coming\n",
      "2020-11-28 01:50:05.700320\n",
      "still wait for data coming\n",
      "2020-11-28 01:55:05.716296\n",
      "still wait for data coming\n",
      "2020-11-28 02:00:05.726285\n",
      "still wait for data coming\n",
      "2020-11-28 02:05:05.749243\n",
      "still wait for data coming\n",
      "2020-11-28 02:10:05.757228\n",
      "still wait for data coming\n",
      "2020-11-28 02:15:05.771207\n",
      "still wait for data coming\n",
      "2020-11-28 02:20:05.780208\n",
      "still wait for data coming\n",
      "2020-11-28 02:25:05.789194\n",
      "still wait for data coming\n",
      "2020-11-28 02:30:05.798199\n",
      "still wait for data coming\n",
      "2020-11-28 02:35:05.812175\n",
      "still wait for data coming\n",
      "2020-11-28 02:40:05.821168\n",
      "still wait for data coming\n",
      "2020-11-28 02:45:05.829156\n",
      "still wait for data coming\n",
      "2020-11-28 02:50:05.838150\n",
      "still wait for data coming\n",
      "2020-11-28 02:55:05.852141\n",
      "still wait for data coming\n",
      "2020-11-28 03:00:05.861121\n",
      "still wait for data coming\n",
      "2020-11-28 03:05:05.869128\n",
      "still wait for data coming\n",
      "2020-11-28 03:10:05.877115\n",
      "still wait for data coming\n",
      "2020-11-28 03:15:05.891103\n",
      "still wait for data coming\n",
      "2020-11-28 03:20:05.899092\n",
      "still wait for data coming\n",
      "2020-11-28 03:25:05.908085\n",
      "still wait for data coming\n",
      "2020-11-28 03:30:05.916091\n",
      "still wait for data coming\n",
      "2020-11-28 03:35:05.931067\n",
      "still wait for data coming\n",
      "2020-11-28 03:40:05.939061\n",
      "still wait for data coming\n",
      "2020-11-28 03:45:05.948047\n",
      "still wait for data coming\n",
      "2020-11-28 03:50:05.958050\n",
      "still wait for data coming\n",
      "2020-11-28 03:55:05.972026\n",
      "still wait for data coming\n",
      "2020-11-28 04:00:05.981011\n",
      "still wait for data coming\n",
      "2020-11-28 04:05:05.989016\n",
      "still wait for data coming\n",
      "2020-11-28 04:10:05.997011\n",
      "still wait for data coming\n",
      "2020-11-28 04:15:06.010984\n",
      "still wait for data coming\n",
      "2020-11-28 04:20:06.018986\n",
      "still wait for data coming\n",
      "2020-11-28 04:25:06.027973\n",
      "still wait for data coming\n",
      "2020-11-28 04:30:06.036974\n",
      "still wait for data coming\n",
      "2020-11-28 04:35:06.050947\n",
      "still wait for data coming\n",
      "2020-11-28 04:40:06.059940\n",
      "still wait for data coming\n",
      "2020-11-28 04:45:06.067936\n",
      "still wait for data coming\n",
      "2020-11-28 04:50:06.075942\n",
      "still wait for data coming\n",
      "2020-11-28 04:55:06.089919\n",
      "still wait for data coming\n",
      "2020-11-28 05:00:06.097907\n",
      "still wait for data coming\n",
      "2020-11-28 05:05:06.106907\n",
      "still wait for data coming\n",
      "2020-11-28 05:10:06.114897\n",
      "still wait for data coming\n",
      "2020-11-28 05:15:06.129877\n",
      "still wait for data coming\n",
      "2020-11-28 05:20:06.138874\n",
      "still wait for data coming\n",
      "2020-11-28 05:25:06.147870\n",
      "still wait for data coming\n",
      "2020-11-28 05:30:06.155860\n",
      "still wait for data coming\n",
      "2020-11-28 05:35:06.169846\n",
      "still wait for data coming\n",
      "2020-11-28 05:40:06.178837\n",
      "still wait for data coming\n",
      "2020-11-28 05:45:06.187824\n",
      "still wait for data coming\n",
      "2020-11-28 05:50:06.195828\n",
      "still wait for data coming\n",
      "2020-11-28 05:55:06.209802\n",
      "still wait for data coming\n",
      "2020-11-28 06:00:06.217807\n",
      "still wait for data coming\n",
      "2020-11-28 06:05:06.226790\n",
      "still wait for data coming\n",
      "2020-11-28 06:10:06.235792\n",
      "still wait for data coming\n",
      "2020-11-28 06:15:06.249766\n",
      "still wait for data coming\n",
      "2020-11-28 06:20:06.258647\n",
      "still wait for data coming\n",
      "2020-11-28 06:25:06.266757\n",
      "still wait for data coming\n",
      "2020-11-28 06:30:06.275742\n",
      "still wait for data coming\n",
      "2020-11-28 06:35:06.289728\n",
      "still wait for data coming\n",
      "2020-11-28 06:40:06.298718\n",
      "still wait for data coming\n",
      "2020-11-28 06:45:06.307608\n",
      "still wait for data coming\n",
      "2020-11-28 06:50:06.315717\n",
      "still wait for data coming\n",
      "2020-11-28 06:55:06.329688\n",
      "still wait for data coming\n",
      "2020-11-28 07:00:06.337692\n",
      "still wait for data coming\n",
      "2020-11-28 07:05:06.346683\n",
      "still wait for data coming\n",
      "2020-11-28 07:10:06.354681\n",
      "still wait for data coming\n",
      "2020-11-28 07:15:06.368660\n",
      "still wait for data coming\n",
      "2020-11-28 07:20:06.376649\n",
      "still wait for data coming\n",
      "2020-11-28 07:25:06.385642\n",
      "still wait for data coming\n",
      "2020-11-28 07:30:06.393646\n",
      "still wait for data coming\n",
      "2020-11-28 07:35:06.407619\n",
      "still wait for data coming\n",
      "2020-11-28 07:40:06.416619\n",
      "still wait for data coming\n",
      "2020-11-28 07:45:06.425612\n",
      "still wait for data coming\n",
      "2020-11-28 07:50:06.434605\n",
      "still wait for data coming\n",
      "2020-11-28 07:55:06.448575\n",
      "still wait for data coming\n",
      "2020-11-28 08:00:06.456574\n",
      "still wait for data coming\n",
      "2020-11-28 08:05:06.464578\n",
      "still wait for data coming\n",
      "2020-11-28 08:10:06.472573\n",
      "still wait for data coming\n",
      "2020-11-28 08:15:06.486546\n",
      "still wait for data coming\n",
      "2020-11-28 08:20:06.495540\n",
      "still wait for data coming\n",
      "2020-11-28 08:25:06.504533\n",
      "still wait for data coming\n",
      "2020-11-28 08:30:06.512529\n",
      "still wait for data coming\n",
      "2020-11-28 08:35:06.526517\n",
      "still wait for data coming\n",
      "2020-11-28 08:40:06.535505\n",
      "still wait for data coming\n",
      "2020-11-28 08:45:06.544496\n",
      "still wait for data coming\n",
      "2020-11-28 08:50:06.552499\n",
      "still wait for data coming\n",
      "2020-11-28 08:55:06.566472\n",
      "still wait for data coming\n",
      "2020-11-28 09:00:06.574468\n",
      "still wait for data coming\n",
      "2020-11-28 09:05:06.583461\n",
      "still wait for data coming\n",
      "2020-11-28 09:10:06.591465\n",
      "still wait for data coming\n",
      "2020-11-28 09:15:06.605646\n",
      "still wait for data coming\n",
      "2020-11-28 09:20:06.613925\n",
      "still wait for data coming\n",
      "2020-11-28 09:25:06.622925\n",
      "still wait for data coming\n",
      "2020-11-28 09:30:06.630913\n",
      "still wait for data coming\n",
      "2020-11-28 09:35:06.644894\n",
      "still wait for data coming\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-28 09:40:06.653885\n",
      "still wait for data coming\n",
      "2020-11-28 09:45:06.662880\n",
      "still wait for data coming\n",
      "2020-11-28 09:50:06.670876\n",
      "still wait for data coming\n",
      "2020-11-28 09:55:06.684864\n",
      "still wait for data coming\n",
      "2020-11-28 10:00:06.693851\n",
      "still wait for data coming\n",
      "2020-11-28 10:05:06.701853\n",
      "still wait for data coming\n",
      "2020-11-28 10:10:06.710849\n",
      "still wait for data coming\n",
      "2020-11-28 10:15:06.724820\n",
      "still wait for data coming\n",
      "2020-11-28 10:20:06.732822\n",
      "still wait for data coming\n",
      "2020-11-28 10:25:06.740810\n",
      "still wait for data coming\n",
      "2020-11-28 10:30:06.749270\n",
      "still wait for data coming\n",
      "2020-11-28 10:35:06.763137\n",
      "still wait for data coming\n",
      "2020-11-28 10:40:06.771239\n",
      "still wait for data coming\n",
      "2020-11-28 10:45:06.780233\n",
      "still wait for data coming\n",
      "2020-11-28 10:50:06.788228\n",
      "still wait for data coming\n",
      "2020-11-28 10:55:06.802209\n",
      "still wait for data coming\n",
      "2020-11-28 11:00:06.810634\n",
      "still wait for data coming\n",
      "2020-11-28 11:05:06.819623\n",
      "still wait for data coming\n",
      "2020-11-28 11:10:06.827616\n",
      "still wait for data coming\n",
      "2020-11-28 11:15:06.841600\n",
      "still wait for data coming\n",
      "2020-11-28 11:20:06.849603\n",
      "still wait for data coming\n",
      "2020-11-28 11:25:06.858669\n",
      "still wait for data coming\n",
      "2020-11-28 11:30:06.866667\n",
      "still wait for data coming\n",
      "2020-11-28 11:35:06.880646\n",
      "still wait for data coming\n",
      "2020-11-28 11:40:06.888646\n",
      "still wait for data coming\n",
      "2020-11-28 11:45:06.896676\n",
      "still wait for data coming\n",
      "2020-11-28 11:50:06.905116\n",
      "still wait for data coming\n",
      "2020-11-28 11:55:06.919094\n",
      "still wait for data coming\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('max_rows', 100)\n",
    "pd.set_option('max_columns', 100)\n",
    "\n",
    "perc = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "def download_data():\n",
    "    print('start')\n",
    "    startTm = datetime.datetime.now()\n",
    "\n",
    "    startDate = (datetime.datetime.today() - datetime.timedelta(1)).strftime('%Y%m%d')\n",
    "    endDate = (datetime.datetime.today() - datetime.timedelta(1)).strftime('%Y%m%d')\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    dataPathLs = np.array(glob.glob(os.path.join(readPath, 'speedCompare***.csv')))\n",
    "    dateLs = np.array([os.path.basename(i).split('_')[1].split('.')[0] for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "    dateLs = dateLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "    date = dateLs[0]\n",
    "    assert(date == (datetime.datetime.today() - datetime.timedelta(1)).strftime('%Y%m%d'))\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    orderLog = pd.read_csv(os.path.join(readPath, 'speedCompare_%s.csv'%date))\n",
    "\n",
    "    for col in ['clockAtArrival', 'secid', 'updateType', 'vai', 'absFilledThisUpdate', 'orderDirection', 'absOrderSize',\n",
    "            'absOrderSizeCumFilled', 'date', 'accCode', 'mse']:\n",
    "        orderLog[col] = orderLog[col].fillna(0)\n",
    "        orderLog[col] = orderLog[col].astype('int64')\n",
    "\n",
    "    orderLog = orderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'clockAtArrival']).reset_index(drop=True)\n",
    "    orderLog = orderLog[orderLog[\"secid\"] >= 1000000]\n",
    "\n",
    "    targetStock = orderLog['secid'].unique()\n",
    "    targetStock = np.array([int(str(i)[1:]) for i in targetStock])\n",
    "    targetStockSZ = sorted(targetStock[targetStock < 600000])\n",
    "    targetStockSH = sorted(targetStock[targetStock >= 600000])\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.34\\\\random_backup\\\\Kevin_zhenyu\\\\rawData'\n",
    "    mdOrderLogPath = glob.glob(os.path.join(readPath, 'logs_%s_zs_92_01***'%date, 'mdOrderLog***.csv'))[-1]\n",
    "    mdTradeLogPath = glob.glob(os.path.join(readPath, 'logs_%s_zs_92_01***'%date, 'mdTradeLog***.csv'))[-1]\n",
    "\n",
    "    mdOrderLog = pd.read_csv(mdOrderLogPath)\n",
    "    mdOrderLog = mdOrderLog[mdOrderLog['SecurityID'].isin(targetStockSZ)]\n",
    "    mdOrderLog['OrderType'] = mdOrderLog['OrderType'].astype(str)\n",
    "    mdOrderLog = mdOrderLog[['clockAtArrival', 'sequenceNo', 'TransactTime', 'SecurityID', 'ApplSeqNum', 'Side',\n",
    "                         'OrderType', 'Price', 'OrderQty']]\n",
    "\n",
    "    mdTradeLog = pd.read_csv(mdTradeLogPath, encoding='utf-8')\n",
    "    mdTradeLog['ExecType'] = mdTradeLog['ExecType'].astype(str)\n",
    "    mdTradeLog = mdTradeLog[mdTradeLog['SecurityID'].isin(targetStockSZ)]\n",
    "    mdTradeLog['volumeThisUpdate'] = np.where(mdTradeLog['ExecType'] == 'F', mdTradeLog['TradeQty'], 0)\n",
    "    mdTradeLog['cum_volume'] = mdTradeLog.groupby(['SecurityID'])['volumeThisUpdate'].cumsum()\n",
    "    mdTradeLog = mdTradeLog[['clockAtArrival', 'sequenceNo', 'TransactTime', 'SecurityID', 'ApplSeqNum', 'cum_volume',\n",
    "                         'ExecType', 'TradePrice', 'TradeQty', 'TradeMoney', 'BidApplSeqNum', 'OfferApplSeqNum']]\n",
    "\n",
    "    mdMsgData = pd.concat([mdOrderLog, mdTradeLog], sort=False)\n",
    "    del mdOrderLog\n",
    "    del mdTradeLog\n",
    "\n",
    "    mdMsgData = mdMsgData.sort_values(by=['sequenceNo']).reset_index(drop=True)\n",
    "\n",
    "    mdMsgData[\"agg_trade\"] = np.where((mdMsgData[\"ApplSeqNum\"] == mdMsgData[\"BidApplSeqNum\"] + 1) & (mdMsgData[\"ExecType\"] == \"F\"), 1, np.where(\n",
    "    (mdMsgData[\"ApplSeqNum\"] == mdMsgData[\"OfferApplSeqNum\"] + 1) & (mdMsgData[\"ExecType\"] == \"F\"), 1, 0))\n",
    "    mdMsgData[\"agg\"] = mdMsgData.groupby([\"SecurityID\"])[\"agg_trade\"].shift(-1)\n",
    "    mdMsgData[\"orderNum\"] = np.where(mdMsgData[\"ExecType\"].isnull(), 1, 0)\n",
    "    mdMsgData[\"cumorderNum\"] = mdMsgData.groupby(\"SecurityID\")[\"orderNum\"].cumsum()\n",
    "    mdMsgData[\"cumorderNum2\"] = np.nan\n",
    "    mdMsgData.loc[mdMsgData[\"agg\"]==1, \"cumorderNum2\"] = mdMsgData.loc[mdMsgData[\"agg\"]==1, \"cumorderNum\"]\n",
    "    mdMsgData[\"cumorderNum2\"] = mdMsgData.groupby(\"SecurityID\")[\"cumorderNum2\"].ffill()\n",
    "    mdMsgData.loc[mdMsgData[\"cumorderNum2\"] == mdMsgData[\"cumorderNum\"], \"cum_volume\"] = mdMsgData[mdMsgData[\"cumorderNum2\"] == mdMsgData[\"cumorderNum\"]]\\\n",
    "    .groupby([\"SecurityID\", \"cumorderNum\"])[\"cum_volume\"].transform(\"max\")\n",
    "\n",
    "    mdMsgData = mdMsgData.sort_values(by=['sequenceNo']).reset_index(drop=True)\n",
    "\n",
    "    mdMsgData['cum_volume'] = mdMsgData.groupby(['SecurityID'])['cum_volume'].ffill()\n",
    "    mdMsgData['cum_volume'] = mdMsgData.groupby(['SecurityID'])['cum_volume'].backfill()\n",
    "    mdMsgData['ExecType'] = mdMsgData['ExecType'].fillna('2')\n",
    "    mdMsgData['TradeQty'] = mdMsgData['TradeQty'].fillna(0)\n",
    "\n",
    "    saveCols = ['clockAtArrival', 'sequenceNo', 'TransactTime', 'SecurityID', 'cum_volume', 'ApplSeqNum', \n",
    "            'Side', 'OrderType', 'Price', 'OrderQty', 'ExecType', 'TradePrice', 'TradeQty', 'TradeMoney',\n",
    "            'BidApplSeqNum', 'OfferApplSeqNum', \"agg\"]\n",
    "    mdMsgData = mdMsgData[saveCols]\n",
    "    savePath = 'L:\\\\orderLog\\\\mdData'\n",
    "    mdMsgData.to_pickle(os.path.join(savePath, 'mdLog_msg_%s.pkl'%date))\n",
    "    del mdMsgData\n",
    "\n",
    "    print(datetime.datetime.now() - startTm)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    pd.set_option('max_rows', 200)\n",
    "    pd.set_option('max_columns', 200)\n",
    "\n",
    "    perc = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "\n",
    "    startTm = datetime.datetime.now()\n",
    "\n",
    "    startDate = (datetime.datetime.today() - datetime.timedelta(1)).strftime('%Y%m%d')\n",
    "    endDate = (datetime.datetime.today() - datetime.timedelta(1)).strftime('%Y%m%d')\n",
    "\n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    dataPathLs = np.array(glob.glob(os.path.join(readPath, 'speedCompare***.csv')))\n",
    "    dateLs = np.array([os.path.basename(i).split('_')[1].split('.')[0] for i in dataPathLs])\n",
    "    dataPathLs = dataPathLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "    dateLs = dateLs[(dateLs >= startDate) & (dateLs <= endDate)]\n",
    "\n",
    "    thisDate = dateLs[0]\n",
    "    assert(thisDate == (datetime.datetime.today() - datetime.timedelta(1)).strftime('%Y%m%d'))\n",
    "    \n",
    "    readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "    rawOrderLog = pd.read_csv(os.path.join(readPath, 'speedCompare_%s.csv'%thisDate))\n",
    "    for col in ['clockAtArrival', 'caamd', 'secid', 'updateType', 'vai', 'absFilledThisUpdate', 'orderDirection', 'absOrderSize',\n",
    "                'absOrderSizeCumFilled', 'date', 'accCode', 'mse']:\n",
    "        rawOrderLog[col] = rawOrderLog[col].fillna(0)\n",
    "        rawOrderLog[col] = rawOrderLog[col].astype('int64')   \n",
    "    rawOrderLog = rawOrderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "    rawOrderLog = rawOrderLog[rawOrderLog[\"secid\"] >= 1000000]\n",
    "\n",
    "    display('There are accounts with duplicated ticks:')\n",
    "    display(rawOrderLog[rawOrderLog.duplicated(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep=False)]\\\n",
    "    .groupby(['date', 'colo', 'accCode'])['ars'].size())\n",
    "    rawOrderLog = rawOrderLog.drop_duplicates(['date', 'secid', 'vai', 'accCode', 'clockAtArrival', 'updateType', \\\n",
    "                                        'orderDirection', 'absOrderSize'], keep='first')\n",
    "\n",
    "    display('There are ticks with orderDirection 0')\n",
    "    display(rawOrderLog[rawOrderLog['orderDirection'] == 0][['date', 'colo', 'accCode', \\\n",
    "                'secid', 'vai', 'updateType', 'sdd', 'orderDirection', 'absOrderSize', 'internalId', 'orderId']])\n",
    "\n",
    "    assert(rawOrderLog[rawOrderLog['updateType'] == 0][rawOrderLog[rawOrderLog['updateType'] == 0]\\\n",
    "                                                       .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                    'vai', 'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "    try:\n",
    "        assert(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId'], keep=False)].shape[0] == 0)\n",
    "    except:\n",
    "        print('There are orders with all things same except sdd')\n",
    "        print(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId'], keep=False)])\n",
    "        assert(rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)][rawOrderLog[(rawOrderLog['updateType'] == 0) & (rawOrderLog['accCode'] != 8856)]\\\n",
    "                                                           .duplicated(['date', 'colo', 'accCode', 'secid', 'orderDirection',\n",
    "                                                                        'absOrderSize', 'internalId', 'sdd'], keep=False)].shape[0] == 0)\n",
    "    try:\n",
    "        assert(sum(rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() != 1) == 0) \n",
    "    except:\n",
    "        print('There are orders with same internalId but different orderId other than accCode 8856 case')\n",
    "        print(rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique()[rawOrderLog[(rawOrderLog['updateType'] != 0) & (rawOrderLog['accCode'] != 8856)].groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                    'orderDirection', 'absOrderSize', 'internalId'])['orderId'].nunique() > 1])\n",
    "\n",
    "    r2 = rawOrderLog[(rawOrderLog['accCode'] != 8856) & (rawOrderLog['orderDirection'] != 0)]\n",
    "    r1 = rawOrderLog[(rawOrderLog['accCode'] == 8856) & (rawOrderLog['orderDirection'] != 0)]\n",
    "    r1['test'] = r1.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize']).grouper.group_info[0]\n",
    "    r1 = r1.sort_values(by=['test', 'clockAtArrival'])\n",
    "    r1.loc[r1['updateType'] != 0, 'vai'] = np.nan\n",
    "    r1['vai'] = r1.groupby('test')['vai'].ffill()\n",
    "    r2['test'] = r2.groupby(['date', 'colo', 'accCode', 'secid', \n",
    "                'orderDirection', 'absOrderSize', 'internalId']).grouper.group_info[0]\n",
    "    r2 = r2.sort_values(by=['test', 'clockAtArrival'])\n",
    "    r2.loc[r2['updateType'] != 0, 'vai'] = np.nan\n",
    "    r2['vai'] = r2.groupby('test')['vai'].ffill()\n",
    "    assert(sum(r1[r1['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "    try:\n",
    "        assert(sum(r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1) == 0)\n",
    "    except:\n",
    "        a = r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique()[r2[r2['updateType'] != 0].groupby(['test', 'vai'])['orderId'].nunique() != 1].reset_index()\n",
    "        print(pd.merge(r2, a[['test', 'vai']], on=['test', 'vai'], how='inner')[['secid', 'accCode', 'colo', 'vai', 'updateType', 'sdd', 'internalId', 'orderId', 'absOrderSize', 'absFilledThisUpdate', 'absOrderSizeCumFilled', 'orderPrice', 'tradePrice']])\n",
    "    rawOrderLog = pd.concat([r1, r2])\n",
    "    del r1\n",
    "    del r2  \n",
    "\n",
    "    rawOrderLog['clock'] = rawOrderLog['clockAtArrival'].apply(lambda x: datetime.datetime.fromtimestamp(x/1e6))\n",
    "    rawOrderLog[\"broker\"] = np.where(rawOrderLog[\"accCode\"].astype(str).apply(lambda x: len(x) == 6), rawOrderLog['accCode'] // 10000, rawOrderLog['accCode'] // 100)\n",
    "    rawOrderLog['colo_broker'] = rawOrderLog['colo'].str[:2] + '_' + rawOrderLog['broker'].astype('str')\n",
    "    rawOrderLog['colo_account'] = rawOrderLog['colo'].str[:2] + '_' + rawOrderLog['accCode'].astype('str')\n",
    "    rawOrderLog['order'] = rawOrderLog.groupby(['date', 'colo', 'accCode', 'secid', 'vai']).grouper.group_info[0]\n",
    "    rawOrderLog['group'] = rawOrderLog.groupby(['date', 'secid', 'vai']).grouper.group_info[0]\n",
    "    rawOrderLog['startClock'] = rawOrderLog.groupby(['order'])['clockAtArrival'].transform('first')\n",
    "    rawOrderLog['duration'] = rawOrderLog['clockAtArrival'] - rawOrderLog['startClock']\n",
    "    rawOrderLog['orderPrice'] = rawOrderLog['orderPrice'].apply(lambda x: round(x, 2))\n",
    "    rawOrderLog['tradePrice'] = rawOrderLog['tradePrice'].apply(lambda x: round(x, 2))\n",
    "    rawOrderLog['orderDirection1'] = np.where(rawOrderLog[\"orderDirection\"] == -2, -1, np.where(\n",
    "        rawOrderLog[\"orderDirection\"] == 2, 1, rawOrderLog[\"orderDirection\"]))\n",
    "    orderLog = rawOrderLog.copy()\n",
    "\n",
    "\n",
    "    ### Assertion 1:  make sure same direction in same date, secid, vai\n",
    "    print('=======================================================================================')\n",
    "    print('1. same date, secid, vai: same direction')\n",
    "    orderLog['directNum'] = orderLog.groupby(['date', 'secid', 'vai'])['orderDirection1'].transform('nunique')\n",
    "    if len(orderLog[orderLog['directNum'] != 1]) > 0:\n",
    "        print('opposite direction for same date, same secid, same vai')\n",
    "        display(orderLog[(orderLog['directNum'] != 1) & (orderLog['updateType'] == 0)][['date', 'accCode', 'secid', 'vai', 'orderDirection', 'order']])\n",
    "        orderLog = orderLog[orderLog['directNum'] == 1]\n",
    "\n",
    "    assert((orderLog.groupby(['date', 'secid', 'vai'])['orderDirection1'].nunique() == 1).all() == True)\n",
    "\n",
    "    ## Assertion 2:  make sure each account, secid, vai only has one insertion\n",
    "    print('=======================================================================================')\n",
    "    print('2. same date, secid, vai, accCode: one insertion')\n",
    "    a = orderLog[orderLog['updateType'] == 0].groupby(['date', 'accCode', 'secid', 'vai', 'order'])['clockAtArrival'].count()\n",
    "    if len(a[a > 1]) > 0:\n",
    "        print('more than one insertion at same time')\n",
    "        a = a[a>1].reset_index()\n",
    "        display(a)\n",
    "        orderLog = orderLog[~(orderLog['order'].isin(a['order'].unique()))]\n",
    "\n",
    "    orderLog['isMsg'] = np.where(orderLog['updateType'] == 0, \n",
    "                                 np.where(orderLog['mse'] == 100, 1, 0), np.nan)\n",
    "    orderLog['isMsg'] = orderLog.groupby(['order'])['isMsg'].ffill()\n",
    "\n",
    "    placeSZE = orderLog[(orderLog['secid'] >= 2000000) & (orderLog['updateType'] == 0)]\n",
    "    print('%.2f%% SZE orders triggered by msg data'%(placeSZE[placeSZE['isMsg'] == 1].shape[0]/placeSZE.shape[0]*100))\n",
    "\n",
    "\n",
    "    ### Assertion 3:  check IPO stocks selling status\n",
    "    print('=======================================================================================')\n",
    "    print('3. IPO stocks selling (ars = 301, 302)')\n",
    "    if orderLog[orderLog['ars'].isin([301, 302])].shape[0] != 0:\n",
    "        kk = orderLog[orderLog['ars'].isin([301, 302])]\n",
    "        print(kk)\n",
    "        try:\n",
    "            assert(kk[kk['orderDirection1'] == 1].shape[0] == 0)\n",
    "            print('we only sell, never buy')\n",
    "        except:\n",
    "            print('There are IPO buy side orders!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            print(kk[kk['orderDirection1'] == 1])\n",
    "        kk1 = kk[kk['updateType'] == 0]\n",
    "        kk1 = kk1.sort_values(by=['accCode', 'secid','clockAtArrival'])\n",
    "        kk1['diff'] = kk1.groupby(['accCode', 'secid'])['clockAtArrival'].apply(lambda x: x-x.shift(1))\n",
    "        kk1['diff'] = kk1['diff'].fillna(0)\n",
    "        try:\n",
    "            assert(kk1[kk1['diff'] < 10e6].shape[0] == 0)\n",
    "            print('for each stock in the same account, there is no insertion within 10 seconds of the previous insertion')\n",
    "        except:\n",
    "            print('There are insertion within 10 seconds for orders under same account same stock!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            print(kk1[kk1['diff'] < 10e6])\n",
    "        kk2 = kk[(kk['updateType'] == 1)]\n",
    "        try:\n",
    "            assert(kk2[kk2['duration'] < 3e6].shape[0] == 0)\n",
    "            print('for each stock in the same account, the cancellation of an order happens more than 3 seconds after the insertion')\n",
    "        except:\n",
    "            print('There are cancellation within 3 seconds for orders under same account same stock!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            print(kk2[kk2['duration'] < 3e6])\n",
    "\n",
    "\n",
    "    ### Assertion 4: check updateType == 7 orders, make sure updateType == 7 orders < 20 per account, < 100 in total\n",
    "    print('=======================================================================================')\n",
    "    print('4. updateType 7 orders')\n",
    "    if orderLog[orderLog['updateType'] == 7].shape[0] != 0:\n",
    "        assert(orderLog[orderLog['updateType'] == 7].groupby('accCode')['order'].nunique().max() < 20)\n",
    "        assert(orderLog[orderLog['updateType'] == 7].groupby('accCode')['order'].nunique().sum() < 100)\n",
    "\n",
    "    ### Assertion 5: check updateType == 6 orders, make sure updateType == 6 orders < 5% per account\n",
    "    print('=======================================================================================')\n",
    "    print('5. updateType 6 orders')\n",
    "    k1 = orderLog[orderLog['updateType'] == 6].groupby('accCode')['order'].nunique().reset_index()\n",
    "    k2 = orderLog.groupby('accCode')['order'].nunique().reset_index()\n",
    "    k = pd.merge(k1, k2, on='accCode', how='left')\n",
    "    k['prob'] = k['order_x']/k['order_y']\n",
    "    try:\n",
    "        assert(sum(k['prob'] >= 0.05) == 0)\n",
    "    except:\n",
    "        print('There are accounts with more than 5% updateType 6 orders!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print(k[k['prob'] >= 0.05])\n",
    "\n",
    "    ### Assertion 6: check CYB orders, make sure CYB stocks total absOrderSize < 30w\n",
    "    print('=======================================================================================')\n",
    "    print('6. CYB stocks total order size < 30w')\n",
    "    try:\n",
    "        assert(orderLog[(orderLog['secid'] >= 2300000) & (orderLog['updateType'] == 0)]['absOrderSize'].max() <= 300000)\n",
    "    except:\n",
    "        print('CYB stocks total absOrderSize >= 30w!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "\n",
    "\n",
    "    ### Assertion 7:  make sure there is no unexpected updateType \n",
    "    print('=======================================================================================')\n",
    "    print('7. unexpected updateType')\n",
    "    def getTuple(x):\n",
    "        return tuple(i for i in x)\n",
    "\n",
    "    checkLog = orderLog[~((orderLog['updateType'] == 4) & (orderLog.groupby(['order'])['updateType'].shift(-1) == 4))]\n",
    "    checkLog = checkLog.groupby(['order'])['updateType'].apply(lambda x: getTuple(x)).reset_index()\n",
    "    checkLog['status'] = np.where(checkLog['updateType'].isin([(0, 2, 4), (0, 2, 1, 4), (0, 2, 1, 2, 4), (0, 2, 4, 1, 4), (0, 4), (0, 1, 4), (0, 4, 1, 4), (0, 2, 2, 4), (0, 4, 2, 4), (0, 2, 2, 1, 4), (0, 2, 2, 4, 1, 4)]),0,\n",
    "                         np.where(checkLog['updateType'].isin([(0, 2, 4, 1, 3), (0, 2, 4, 1, 4, 3), (0, 2, 1, 4, 3), (0, 4, 1, 3), (0, 1, 4, 3),\n",
    "                                                                   (0, 2, 2, 4, 1, 3), (0, 2, 2, 4, 1, 4, 3), (0, 2, 2, 1, 4, 3), (0, 4, 2, 4, 1, 3),\n",
    "                                                                   (0, 4, 2, 1, 3), (0, 4, 1, 4, 3), (0, 4, 1)]), 1,\n",
    "                         np.where(checkLog['updateType'].isin([(0, 2, 1, 3), (0, 2, 2, 1, 3), (0, 2, 3), (0, 3), (0, 1, 3), (0, ), (0, 2), (0, 2, 1), (0, 2, 2)]), 2, 3)))\n",
    "\n",
    "    orderLog = pd.merge(orderLog, checkLog[['order', 'status']], how='left', on=['order'], validate='many_to_one')\n",
    "    orderLog = orderLog[orderLog['status'].isin([0, 1, 2])].reset_index(drop=True)\n",
    "\n",
    "    ### Assertion 8:  make sure status==0 got all traded\n",
    "    print('=======================================================================================')\n",
    "    print('8. status == 0: all traded')\n",
    "    a = orderLog[orderLog['status'] == 0]\n",
    "    a = a.groupby(['order'])[['absOrderSizeCumFilled', 'absOrderSize']].max().reset_index()\n",
    "    a.columns = ['order', 'filled', 'total']\n",
    "    print('in total trade, any fill != total cases')\n",
    "    display(a[a['filled'] != a['total']])\n",
    "    if a[a['filled'] != a['total']].shape[0] > 0:\n",
    "        removeOrderLs = a[a['filled'] != a['total']]['order'].unique()\n",
    "        orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "    ### Assertion 9:  make sure status==1 got partial traded\n",
    "    print('=======================================================================================')\n",
    "    print('9. status == 1: partial traded')\n",
    "    a = orderLog[orderLog['status'] == 1]\n",
    "    a = a.groupby(['order'])[['absOrderSizeCumFilled', 'absOrderSize']].max().reset_index()\n",
    "    a.columns = ['order', 'filled', 'total']\n",
    "    print('in partial trade, any fill >= total or fill is 0 cases for updateType 4')\n",
    "    display(a[(a['filled'] >= a['total']) | (a['filled'] == 0)])\n",
    "    if a[(a['filled'] >= a['total']) | (a['filled'] == 0)].shape[0] > 0:\n",
    "        removeOrderLs = a[(a['filled'] >= a['total']) | (a['filled'] == 0)]['order'].unique()\n",
    "        orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "    ### Assertion 10: make sure no cancellation within 1 sec\n",
    "    print('=======================================================================================')\n",
    "    print('10. no cancellation within 1 sec')\n",
    "    a = orderLog[(orderLog['updateType'] == 1) & (orderLog['duration'] < 1e6)]\n",
    "    print('any cancellation within 1 sec')\n",
    "    display(a)\n",
    "    if a.shape[0] > 0:\n",
    "        removeOrderLs = a['order'].unique()\n",
    "        orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "\n",
    "\n",
    "    ### Assertion 11: make sure no order has shares > 80w or notional > 800w\n",
    "    print('=======================================================================================')\n",
    "    print('11. Orders with size > 80w or notional > 800w')\n",
    "    orderLog['orderNtl'] = orderLog['absOrderSize'] * orderLog['orderPrice']\n",
    "    if orderLog[orderLog['absOrderSize'] > 800000].shape[0] > 0:\n",
    "        print('some order quantity are > 80w')\n",
    "        print(orderLog[orderLog['absOrderSize'] > 800000].groupby(['colo', 'accCode'])['order'].nunique())\n",
    "        display(orderLog[orderLog['absOrderSize'] > 800000][['date', 'accCode', 'secid', 'vai', 'absOrderSize', 'orderPrice',\n",
    "                                                             'orderNtl', 'orderDirection', 'clock', 'order']])\n",
    "\n",
    "    if orderLog[orderLog['orderNtl'] > 8000000].shape[0] > 0:\n",
    "        print('some order ntl are > 800w')\n",
    "        print(orderLog[orderLog['orderNtl'] > 8000000].groupby(['colo', 'accCode'])['order'].nunique())\n",
    "        display(orderLog[orderLog['orderNtl'] > 8000000][['date', 'accCode', 'secid', 'vai', 'absOrderSize', 'orderPrice',\n",
    "                                                          'orderNtl', 'orderDirection', 'clock', 'order', \"updateType\", \n",
    "                                                          \"tradePrice\", \"absOrderSizeCumFilled\", \"absFilledThisUpdate\"]])\n",
    "\n",
    "    removeOrderLs = list(set(orderLog[orderLog['absOrderSize'] > 800000]['order'].unique()) | set(orderLog[orderLog['orderNtl'] > 8000000]['order'].unique()))\n",
    "    orderLog = orderLog[~(orderLog['order'].isin(removeOrderLs))]\n",
    "    orderLog[\"mrsb90\"] = orderLog.groupby(['order'])['mrsb90'].transform('first')\n",
    "    orderLog[\"mrss90\"] = orderLog.groupby(['order'])['mrss90'].transform('first')\n",
    "    orderLog[\"aaa\"] = orderLog.groupby(['order'])['aaa'].transform('first')\n",
    "\n",
    "    orderLog['m1'] = orderLog['mrstaat'].apply(lambda x: x - (x // 10000) * 10000)\n",
    "    orderLog['m2'] = orderLog['mrstauc'].apply(lambda x: x - (x // 10000) * 10000)\n",
    "    if orderLog[orderLog['mrsb90'] == '-'].shape[0] != 0:\n",
    "        display(orderLog[orderLog['mrsb90'] == '-'])\n",
    "    orderLog = orderLog[orderLog['mrsb90'] != '-']\n",
    "    orderLog['mrsb90'] = orderLog['mrsb90'].astype(float)\n",
    "    if orderLog[orderLog['aaa'] == '-'].shape[0] != 0:\n",
    "        display(orderLog[orderLog['aaa'] == '-'])\n",
    "    orderLog = orderLog[orderLog['aaa'] != '-']\n",
    "    orderLog['aaa'] = orderLog['aaa'].astype(float)\n",
    "    orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'mrstauc'] = \\\n",
    "    orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'm2']\n",
    "\n",
    "    orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'mrstaat'] = \\\n",
    "    orderLog.loc[(orderLog['orderDirection'] >= 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrsb90']) < 1e-12), 'm1']\n",
    "\n",
    "    orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'mrstauc'] = \\\n",
    "    orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'm2']\n",
    "\n",
    "    orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'mrstaat'] = \\\n",
    "    orderLog.loc[(orderLog['orderDirection'] < 1) &\\\n",
    "             (orderLog['mrstaat'].isin([11000, 13000])) & (abs(orderLog['aaa'] - orderLog['mrss90']) < 1e-12), 'm1']  \n",
    "\n",
    "    orderLog = orderLog.sort_values(by=['date', 'secid', 'vai', 'accCode', 'clockAtArrival']).reset_index(drop=True)\n",
    "\n",
    "    orderLog['exchange'] = np.where(orderLog['secid'] >= 2000000, 'SZE', 'SSE')\n",
    "    orderLog['orderNtl'] = orderLog['orderPrice'] * orderLog['absOrderSize']\n",
    "    orderLog['tradeNtl'] = np.where(orderLog['updateType'] == 4, orderLog['tradePrice']*orderLog['absFilledThisUpdate'], 0)\n",
    "    orderLog = orderLog[orderLog['secid'] >= 2000000].reset_index(drop=True)\n",
    "\n",
    "    # 1. market orders\n",
    "    readPath = 'L:\\\\orderLog\\\\mdData'\n",
    "    rawMsgDataSZ = pd.read_pickle(os.path.join(readPath, 'mdLog_msg_%s.pkl'%thisDate))\n",
    "    orderDataSZ = rawMsgDataSZ[rawMsgDataSZ['ExecType'] == '2'][['SecurityID', 'ApplSeqNum', 'clockAtArrival', 'sequenceNo', 'Side', 'OrderQty', 'Price', 'cum_volume', \"agg\", \"TransactTime\"]].reset_index(drop=True)\n",
    "    orderDataSZ['updateType'] = 0\n",
    "    tradeDataSZ = pd.concat([rawMsgDataSZ[rawMsgDataSZ['ExecType'] == 'F'][['SecurityID', 'BidApplSeqNum', 'clockAtArrival', 'sequenceNo', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]],\n",
    "                             rawMsgDataSZ[rawMsgDataSZ['ExecType'] == 'F'][['SecurityID', 'OfferApplSeqNum', 'clockAtArrival', 'sequenceNo', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]]], sort=False)\n",
    "    tradeDataSZ['ApplSeqNum'] = np.where(tradeDataSZ['BidApplSeqNum'].isnull(), tradeDataSZ['OfferApplSeqNum'], tradeDataSZ['BidApplSeqNum'])\n",
    "    tradeDataSZ['Side'] = np.where(tradeDataSZ['BidApplSeqNum'].isnull(), 2, 1)\n",
    "    tradeDataSZ = tradeDataSZ[['SecurityID', 'ApplSeqNum', 'clockAtArrival', 'sequenceNo', 'Side', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]]\n",
    "    tradeDataSZ['updateType'] = 4\n",
    "    cancelDataSZ = rawMsgDataSZ[rawMsgDataSZ['ExecType'] == '4'][['SecurityID', 'BidApplSeqNum', 'OfferApplSeqNum', 'clockAtArrival', 'sequenceNo', 'TradePrice', 'TradeQty', 'cum_volume', \"TransactTime\"]].reset_index(drop=True)\n",
    "    cancelDataSZ['ApplSeqNum'] = np.where(cancelDataSZ['BidApplSeqNum'] == 0, cancelDataSZ['OfferApplSeqNum'], cancelDataSZ['BidApplSeqNum'])\n",
    "    cancelDataSZ['Side'] = np.where(cancelDataSZ['BidApplSeqNum'] == 0, 2, 1)\n",
    "    cancelDataSZ = cancelDataSZ[['SecurityID', 'ApplSeqNum', 'clockAtArrival', 'sequenceNo', 'Side', 'TradeQty', 'cum_volume', \"TransactTime\"]]\n",
    "    cancelDataSZ['updateType'] = 3\n",
    "\n",
    "    msgDataSZ = pd.concat([orderDataSZ, tradeDataSZ, cancelDataSZ], sort=False)\n",
    "    del orderDataSZ\n",
    "    del tradeDataSZ\n",
    "    del cancelDataSZ\n",
    "    msgDataSZ = msgDataSZ.sort_values(by=['SecurityID', 'ApplSeqNum', 'sequenceNo']).reset_index(drop=True)\n",
    "    msgDataSZ['TradePrice'] = np.where(msgDataSZ['updateType'] == 4, msgDataSZ['TradePrice'], 0)\n",
    "    msgDataSZ['TradePrice'] = msgDataSZ['TradePrice'].astype('int64')\n",
    "    msgDataSZ['TradeQty'] = np.where(msgDataSZ['updateType'] == 4, msgDataSZ['TradeQty'], 0)\n",
    "    msgDataSZ['TradeQty'] = msgDataSZ['TradeQty'].astype('int64')\n",
    "    msgDataSZ['secid'] = msgDataSZ['SecurityID'] + 2000000\n",
    "    assert(msgDataSZ['ApplSeqNum'].max() < 1e8)\n",
    "    msgDataSZ['StockSeqNum'] = msgDataSZ['SecurityID']*1e8 + msgDataSZ['ApplSeqNum']\n",
    "    msgDataSZ['date'] = int(thisDate) \n",
    "    print('finish market orders')\n",
    "\n",
    "\n",
    "    # 2. orderLog\n",
    "    infoData = orderLog[(orderLog['date'] == int(thisDate)) & (orderLog[\"isMsg\"] == 1) \n",
    "                        & (orderLog['updateType'].isin([0, 3, 4]))].reset_index(drop=True)\n",
    "    del orderLog\n",
    "    infoData['Price'] = infoData['orderPrice'].apply(lambda x: round(x*100, 0))\n",
    "    infoData['Price'] = infoData['Price'].astype('int64')*100\n",
    "    infoData['OrderQty'] = infoData['absOrderSize']\n",
    "    infoData['Side'] = np.where(infoData['orderDirection1'] == 1, 1, 2)\n",
    "    infoData['TradePrice'] = np.where(infoData['updateType'] == 4, round(infoData['tradePrice']*100, 0), 0)\n",
    "    infoData['TradePrice'] = infoData['TradePrice'].astype('int64')*100\n",
    "    statusInfo = infoData.groupby(['order'])['updateType'].apply(lambda x: tuple(x)).reset_index()\n",
    "    statusInfo.columns = ['order', 'statusLs']\n",
    "    tradePriceInfo = infoData.groupby(['order'])['TradePrice'].apply(lambda x: tuple(x)).reset_index()\n",
    "    tradePriceInfo.columns = ['order', 'TradePriceLs']\n",
    "    tradeQtyInfo = infoData.groupby(['order'])['absFilledThisUpdate'].apply(lambda x: tuple(x)).reset_index()\n",
    "    tradeQtyInfo.columns = ['order', 'TradeQtyLs']\n",
    "    infoData = infoData[infoData['updateType'] == 0]\n",
    "    infoData = pd.merge(infoData, statusInfo, how='left', on=['order'], validate='one_to_one')\n",
    "    infoData = pd.merge(infoData, tradePriceInfo, how='left', on=['order'], validate='one_to_one')\n",
    "    infoData = pd.merge(infoData, tradeQtyInfo, how='left', on=['order'], validate='one_to_one')\n",
    "    infoData['brokerNum'] = infoData.groupby(['date', 'secid', 'vai', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'])['colo_account'].transform('count')\n",
    "    display(infoData[infoData['brokerNum'] >= 2].groupby(['colo', 'accCode'])['date'].count())\n",
    "    display('%.2f%%'%(infoData[infoData['brokerNum'] >= 2].shape[0] / infoData.shape[0]*100))\n",
    "    display(infoData[infoData['brokerNum'] >= 2].shape[0])\n",
    "    display(infoData.shape[0])\n",
    "    infoData = infoData[infoData['brokerNum'] == 1]\n",
    "    infoData = infoData[['date', 'secid', 'vai', 'ars', \"mrstaat\", \"mrstauc\", 'exchange', 'group', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum', 'order', 'colo', 'accCode']]   \n",
    "\n",
    "    print('finish our orders')\n",
    "\n",
    "    # 3. find the position of our orders\n",
    "    checkLog = msgDataSZ[msgDataSZ['updateType'] == 0]\n",
    "    checkLog = pd.merge(checkLog, infoData.drop_duplicates(subset=['secid', 'ApplSeqNum'])[['secid', 'ApplSeqNum', 'ars']], \n",
    "                    on=['secid', 'ApplSeqNum'], how='left')\n",
    "    checkLog['ApplSeqNum'] = np.where(~checkLog['ars'].isnull(), checkLog['ApplSeqNum'], checkLog['ars'])\n",
    "    checkLog['start_time'] = np.where(~checkLog['ars'].isnull(), checkLog['clockAtArrival'], checkLog['ars'])\n",
    "    checkLog.drop([\"ars\"],axis=1,inplace=True)\n",
    "    checkLog['ApplSeqNum'] = checkLog.groupby(['secid'])['ApplSeqNum'].ffill()\n",
    "    checkLog['start_time'] = checkLog.groupby(['secid'])['start_time'].ffill()\n",
    "    checkLog['end_time'] = checkLog['start_time'] + 100*1e3\n",
    "    checkLog = checkLog[(~checkLog['ApplSeqNum'].isnull()) & (checkLog['clockAtArrival'] > checkLog['start_time']) & \n",
    "                       (checkLog['clockAtArrival'] < checkLog['end_time'])]\n",
    "    msgDataSZ = msgDataSZ[msgDataSZ['StockSeqNum'].isin(checkLog['StockSeqNum'].values)]\n",
    "    print('finish get the interval')\n",
    "\n",
    "    statusInfo = msgDataSZ.groupby(['StockSeqNum'])['updateType'].apply(lambda x: getTuple(x)).reset_index()\n",
    "    statusInfo.columns = ['StockSeqNum', 'statusLs']\n",
    "    tradePriceInfo = msgDataSZ.groupby(['StockSeqNum'])['TradePrice'].apply(lambda x: tuple(x)).reset_index()\n",
    "    tradePriceInfo.columns = ['StockSeqNum', 'TradePriceLs']\n",
    "    tradeQtyInfo = msgDataSZ.groupby(['StockSeqNum'])['TradeQty'].apply(lambda x: tuple(x)).reset_index()\n",
    "    tradeQtyInfo.columns = ['StockSeqNum', 'TradeQtyLs']\n",
    "    del msgDataSZ\n",
    "    checkLog = pd.merge(checkLog, statusInfo, how='left', on=['StockSeqNum'], validate='one_to_one')\n",
    "    checkLog = pd.merge(checkLog, tradePriceInfo, how='left', on=['StockSeqNum'], validate='one_to_one')\n",
    "    checkLog = pd.merge(checkLog, tradeQtyInfo, how='left', on=['StockSeqNum'], validate='one_to_one')  \n",
    "    del statusInfo\n",
    "    del tradePriceInfo\n",
    "    del tradeQtyInfo\n",
    "\n",
    "\n",
    "    try:\n",
    "        checkLog = pd.merge(checkLog, infoData, how='left', on=['date', 'secid', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'], validate='many_to_one')\n",
    "    except:\n",
    "        print('There are orders with same pattern but different vai, same ApplSeqNum')\n",
    "        display([infoData[infoData.duplicated(['date', 'secid', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'], keep=False)]])\n",
    "        checkLog = pd.merge(checkLog, infoData, how='left', on=['date', 'secid', 'Price', 'OrderQty', 'Side', 'statusLs', 'TradePriceLs', 'TradeQtyLs', 'ApplSeqNum'])\n",
    "    del infoData\n",
    "    savePath = 'L:\\\\orderLog\\\\result\\\\marketPos'\n",
    "    checkLog.reset_index(drop=True).to_pickle(os.path.join(savePath, 'SZspeed_%s.pkl'%thisDate))\n",
    "    del checkLog \n",
    "    print('finished')\n",
    "\n",
    "        \n",
    "        \n",
    "from twisted.internet import task, reactor\n",
    "import schedule\n",
    "\n",
    "\n",
    "def sleeptime(hour,min,sec):\n",
    "    return hour*3600 + min*60 + sec\n",
    "\n",
    "\n",
    "import tarfile\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import shutil\n",
    "import datetime\n",
    "def un_gz(file_name, dirs):\n",
    "    t = tarfile.open(file_name)\n",
    "    dirs = dirs + '\\\\' + os.path.basename(file_name).split('.')[0]\n",
    "    os.mkdir(dirs)\n",
    "    t.extractall(path=dirs)\n",
    "    da_te = os.path.basename(file_name).split('_')[1]\n",
    "    readPath = dirs + '\\\\md***' + da_te + '***'\n",
    "    dataPathLs = np.array(glob.glob(readPath))\n",
    "    readPath = dirs + '\\\\***'\n",
    "    dataPathLs1 = np.array(glob.glob(readPath))\n",
    "    for i in list(set(dataPathLs1) - set(dataPathLs)):\n",
    "        if os.path.exists(i):\n",
    "            if os.path.isfile(i):\n",
    "                os.remove(i)\n",
    "            if os.path.isdir(i):\n",
    "                shutil.rmtree(i) \n",
    "    for i in dataPathLs:\n",
    "        if os.path.getsize(i) < 1000000000:\n",
    "            os.remove(i)\n",
    "    print('finish ' + da_te)\n",
    "    \n",
    "\n",
    "class Test(object):\n",
    "    def __init__(self):\n",
    "        self.status = True        \n",
    "    def test(self):\n",
    "        while self.status == True:\n",
    "            try:\n",
    "                print(datetime.datetime.now())\n",
    "                date = (datetime.datetime.today()).strftime('%Y%m%d')\n",
    "                readPath = '\\\\\\\\192.168.10.28\\\\equityTradeLogs'\n",
    "                dataPathLs = np.array(glob.glob(os.path.join(readPath, 'speedCompare***.csv')))\n",
    "                dateLs = np.array([int(os.path.basename(i).split('_')[1].split('.')[0]) for i in dataPathLs])\n",
    "                assert(np.max(dateLs) == int(date))\n",
    "#                 readPath = '\\\\\\\\192.168.10.34\\\\random_backup\\\\Kevin_zhenyu\\\\rawData\\\\***'\n",
    "#                 dataPathLs = np.array(glob.glob(readPath))\n",
    "#                 dateLs = np.array([int(os.path.basename(i).split('_')[1]) for i in dataPathLs])\n",
    "#                 assert(np.max(dateLs) == int(date))\n",
    "                readPath = '\\\\\\\\192.168.10.34\\\\trading\\\\dailyRawData\\\\' + date + '\\\\***zs_92_01***'\n",
    "                dataPathLs = np.array(glob.glob(readPath))\n",
    "                assert(len(dataPathLs) == 1)\n",
    "                un_gz(dataPathLs[0], '\\\\\\\\192.168.10.34\\\\random_backup\\\\Kevin_zhenyu\\\\rawData')\n",
    "                print('We start to generate data now')  \n",
    "                download_data()\n",
    "                self.status = False\n",
    "            except:\n",
    "                print('still wait for data coming')\n",
    "                second = sleeptime(0,5,0)\n",
    "                time.sleep(second)\n",
    "\n",
    "test1 = Test()\n",
    "schedule.every().day.at(\"21:00\").do(test1.test)\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new loop from here\n",
      "2020-11-27 15:06:47.369107\n",
      "1606460807.369107\n",
      "1\n",
      "2020-11-27 15:06:52.369689\n",
      "1606460812.3696892\n",
      "1\n",
      "new loop from here\n",
      "2020-11-27 15:07:12.379315\n",
      "1606460832.3793154\n",
      "1\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-03f83cabf9c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\schedule\\__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[1;34m()\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mdefault_scheduler\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \"\"\"\n\u001b[1;32m--> 563\u001b[1;33m     \u001b[0mdefault_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\schedule\\__init__.py\u001b[0m in \u001b[0;36mrun_pending\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mrunnable_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjobs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunnable_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelay_seconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\schedule\\__init__.py\u001b[0m in \u001b[0;36m_run_job\u001b[1;34m(self, job)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCancelJob\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mCancelJob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcancel_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\schedule\\__init__.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    464\u001b[0m         \"\"\"\n\u001b[0;32m    465\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Running job %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjob_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_schedule_next_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-03f83cabf9c2>\u001b[0m in \u001b[0;36mloop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoopingCall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mreactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseconds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1282\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1283\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self, installSignalHandlers)\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \"\"\"\n\u001b[0;32m   1261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_installSignalHandlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstallSignalHandlers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m         \u001b[0mReactorBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\twisted\\internet\\base.py\u001b[0m in \u001b[0;36mstartRunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorAlreadyRunning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_startedBefore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReactorNotRestartable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stopped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from twisted.internet import task, reactor\n",
    "import schedule\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(datetime.datetime.now())\n",
    "    print(time.time())\n",
    "    try:\n",
    "        assert(time.time() < 1606460781)\n",
    "    except:\n",
    "        print(1)\n",
    "        if reactor.running:\n",
    "            reactor.stop()\n",
    "\n",
    "def loop():\n",
    "    print('new loop from here')\n",
    "    timeout = 5.0\n",
    "    l = task.LoopingCall(test)\n",
    "    l.start(timeout)\n",
    "    reactor.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new loop from here\n",
      "2020-11-27 15:20:01.268657\n",
      "1606461601.268657\n",
      "1\n",
      "False\n",
      "2020-11-27 15:20:06.269057\n",
      "1606461606.2690573\n",
      "1\n",
      "True\n",
      "2020-11-27 15:20:11.269439\n",
      "1606461611.2694392\n",
      "1\n",
      "True\n",
      "2020-11-27 15:20:16.269151\n",
      "1606461616.2691512\n",
      "1\n",
      "True\n",
      "2020-11-27 15:20:21.269245\n",
      "1606461621.2692451\n",
      "1\n",
      "True\n",
      "2020-11-27 15:20:26.269235\n",
      "1606461626.2692351\n",
      "1\n",
      "True\n",
      "2020-11-27 15:20:31.269356\n",
      "1606461631.269356\n",
      "1\n",
      "True\n",
      "2020-11-27 15:20:36.269400\n",
      "1606461636.2694006\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from twisted.internet import task, reactor\n",
    "import schedule\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "class Test(object):\n",
    "    def __init__(self):\n",
    "        self.l = None        \n",
    "    def test(self):\n",
    "        print(datetime.datetime.now())\n",
    "        print(time.time())\n",
    "        try:\n",
    "            assert(time.time() < 1606460781)\n",
    "        except:\n",
    "            print(1)\n",
    "#             print(reactor.running)\n",
    "#             self.l.stop()\n",
    "            print(reactor.running)\n",
    "            return()\n",
    "\n",
    "def loop():\n",
    "    print('new loop from here')\n",
    "    timeout = 5.0\n",
    "    Test1 = Test()\n",
    "    l = task.LoopingCall(Test1.test)\n",
    "    Test1.l = l\n",
    "    l.start(timeout)\n",
    "    reactor.run()\n",
    "\n",
    "schedule.every(20).seconds.do(loop)\n",
    "if False:\n",
    "    print('here')\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
